{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7e4d34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from helper_functions.shared_functions import check_if_s3_key_exists, run_sql_in_db, get_db_engine\n",
    "from helper_functions.viz_classes import database\n",
    "import os\n",
    "import codecs\n",
    "import csv\n",
    "import boto3\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from io import StringIO\n",
    "from sqlalchemy.exc import DataError\n",
    "from sqlalchemy.types import Text\n",
    "\n",
    "fim_version = \"4.3.11.0\"\n",
    "fim_folder = f\"fim_{fim_version.replace('.', '_')}\"\n",
    "hand_datasets = f\"{fim_folder}/hand_datasets\"\n",
    "qa_datasets = f\"{fim_folder}/qa_datasets\"\n",
    "fim_bucket = \"hydrovis-ti-deployment-us-east-1\"\n",
    "fim_crosswalk = os.path.join(hand_datasets, \"crosswalk_table.csv\")\n",
    "pipeline_arn = \"arn:aws:states:us-east-1:526904826677:stateMachine:viz_pipeline_ti\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cc9de0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['VIZ_DB_USERNAME'] = \"viz_proc_admin_rw_user\"\n",
    "os.environ['VIZ_DB_PASSWORD'] = \"QLQW31ZxL0CuE78Rm1lvjYTWg\"\n",
    "ti_access_key = 'ASIAXVLPZTM26UZOVROI'\n",
    "ti_secret_key = 'hVr2v49KQhCYadKkXOOwt4pw0TVmo94vAvvKEwgd'\n",
    "ti_token = 'IQoJb3JpZ2luX2VjEKP//////////wEaCXVzLWVhc3QtMSJIMEYCIQDVBcAj/S65mtNVEbK+u8kbolaftbOiN/+LCDvPd+VSZAIhAPTBnROm2yJJjcAp75xI57JxARtDqVdByNbn2HPC2TuxKp4DCNv//////////wEQARoMNTI2OTA0ODI2Njc3IgynJtdd2nhqIEE7ZC0q8gKzbJvmq4cLiM1Rss62YNKSI9HOK7C8O3tpngtsw92ktXh2DtDPTF10HqmiUOucvqZ1obsBOUbbRGRh5tDB+3aB9cqz9ZGuxrhTz+aQCIQ/XM560CRH9Ck4l7640hNLlgiQAMp+0/ZU/EEo4h+1160Crihnn8O7LmIH+AxFXaeQjkzPzmc/Nw+tRiyi83dmiQn/ANH7jWnDKU74WKo2/8sQcAKQN0E1CnEeo9XEf818Jq+YdupaaFa/aTC2dQqaD0mHFGy8MJrKRvVi7O0+a1Qs0n936dIbYl0kQDTa3cBG9OJh8sOQCxu28beOYUXXSm8Kneqy+GOCqOwP1usr9tNpIr+g9DwsUK6cCHFzXc5BvhFEJv5jrTTY8o8krqgVpZoLs7KV0At9DJwjs2OEbD6kI1jQ0N6Xo9h9SziSp8X5TUegGKRt1w9fBp5fJ5dxIr3x51NjfDtCC66G4hzjnz+3iHZKjuZvnkCDoM4EIrvjn7KjMMvh6KMGOqUBxR+a1TEla3GVbj0wok9u3K+gqTRKHFTGZJf/6z7mLkB5q5O7bIkGnMptx8lTimut9KgqGuT/NosvFnuZbafKk939ERhHJYvg1amOj1XFdDq8bwfYTIll8dllpmmkizfJJsh1qLf29nrbtkZ9thWuYU2A6Orkq94P2MxOCb+PPUIGLA0xBJBeSteA8DARl69H2RDZQSEhnLQ8Baa90EhjRyGHF3+N'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "472c99c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/boto3/compat.py:88: PythonDeprecationWarning: Boto3 will no longer support Python 3.6 starting May 30, 2022. To continue receiving service updates, bug fixes, and security updates please upgrade to Python 3.7 or later. More information can be found here: https://aws.amazon.com/blogs/developer/python-support-policy-updates-for-aws-sdks-and-tools/\n",
      "  warnings.warn(warning, PythonDeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "s3_client = boto3.client(\"s3\")\n",
    "client = boto3.client('stepfunctions')\n",
    "viz_engine = get_db_engine('viz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dabd2f",
   "metadata": {},
   "source": [
    "<h2>1 - UPDATE VLAB REPO WITH NEW FIM VERSION</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48423332",
   "metadata": {},
   "source": [
    "<h2>2 - UPLOAD FIM4 HYDRO ID/FEATURE ID CROSSWALK</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcecccf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting column name from fim_4_3_11_0/hand_datasets/crosswalk_table.csv\n",
      "***> Established db connection to: hydrovis-ti-viz-processing.c4vzypepnkx3.us-east-1.rds.amazonaws.com from <module>()\n",
      "Deleting/Creating derived.fim4_featureid_crosswalk using columns (HydroID integer,feature_id integer,huc8 TEXT,LakeID integer,BranchID bigint)\n",
      "Importing fim_4_3_11_0/hand_datasets/crosswalk_table.csv to derived.fim4_featureid_crosswalk\n",
      "Adding fim_version column to derived.fim4_featureid_crosswalk\n",
      "Renaming columns in derived.fim4_featureid_crosswalk\n",
      "Adding feature id index to derived.fim4_featureid_crosswalk\n",
      "Adding hydro id index to derived.fim4_featureid_crosswalk\n",
      "Successully updated derived.fim4_featureid_crosswalk\n"
     ]
    }
   ],
   "source": [
    "print(f\"Getting column name from {fim_crosswalk}\")\n",
    "data = s3_client.get_object(Bucket=fim_bucket, Key=fim_crosswalk)\n",
    "d_reader = csv.DictReader(codecs.getreader(\"utf-8\")(data[\"Body\"]))\n",
    "headers = d_reader.fieldnames\n",
    "\n",
    "header_str = \"(\"\n",
    "for header in headers:\n",
    "    header_str += header\n",
    "    if header in ['HydroID', 'LakeID', 'feature_id']:\n",
    "        header_str += ' integer,'\n",
    "    elif header in ['BranchID']:\n",
    "        header_str += ' bigint,'\n",
    "    else:\n",
    "        header_str += ' TEXT,'\n",
    "header_str = header_str[:-1] + \")\"\n",
    "\n",
    "\n",
    "db = database(db_type=\"viz\")\n",
    "with db.get_db_connection() as conn, conn.cursor() as cur:\n",
    "    print(f\"Deleting/Creating derived.fim4_featureid_crosswalk using columns {header_str}\")\n",
    "    sql = f\"DROP TABLE IF EXISTS derived.fim4_featureid_crosswalk; CREATE TABLE derived.fim4_featureid_crosswalk {header_str};\"\n",
    "    cur.execute(sql)\n",
    "    conn.commit()\n",
    "\n",
    "    print(f\"Importing {fim_crosswalk} to derived.fim4_featureid_crosswalk\")\n",
    "    sql = f\"\"\"\n",
    "        SELECT aws_s3.table_import_from_s3(\n",
    "           'derived.fim4_featureid_crosswalk',\n",
    "           '', \n",
    "           '(format csv, HEADER true)',\n",
    "           (SELECT aws_commons.create_s3_uri(\n",
    "               '{fim_bucket}',\n",
    "               '{fim_crosswalk}',\n",
    "               'us-east-1'\n",
    "                ) AS s3_uri\n",
    "            ),\n",
    "            aws_commons.create_aws_credentials('{ti_access_key}', '{ti_secret_key}', '{ti_token}')\n",
    "           );\n",
    "        \"\"\"\n",
    "    cur.execute(sql)\n",
    "    conn.commit()\n",
    "\n",
    "    print(f\"Adding fim_version column to derived.fim4_featureid_crosswalk\")\n",
    "    sql = f\"ALTER TABLE derived.fim4_featureid_crosswalk ADD COLUMN fim_version text DEFAULT '{fim_version}';\"\n",
    "    cur.execute(sql)\n",
    "    conn.commit()\n",
    "\n",
    "    print(f\"Renaming columns in derived.fim4_featureid_crosswalk\")\n",
    "    sql = f\"\"\"\n",
    "        ALTER TABLE derived.fim4_featureid_crosswalk RENAME COLUMN HydroID TO hydro_id;\n",
    "        ALTER TABLE derived.fim4_featureid_crosswalk RENAME COLUMN LakeID TO lake_id;\n",
    "        ALTER TABLE derived.fim4_featureid_crosswalk RENAME COLUMN BranchID  TO branch_id;\n",
    "    \"\"\"\n",
    "    cur.execute(sql)\n",
    "    conn.commit()\n",
    "\n",
    "    print(f\"Adding feature id index to derived.fim4_featureid_crosswalk\")\n",
    "    sql = f\"CREATE INDEX fim4_crosswalk_feature_id ON derived.fim4_featureid_crosswalk USING btree (feature_id)\"\n",
    "    cur.execute(sql)\n",
    "    conn.commit()\n",
    "\n",
    "    print(f\"Adding hydro id index to derived.fim4_featureid_crosswalk\")\n",
    "    sql = f\"CREATE INDEX fim4_crosswalk_hydro_id ON derived.fim4_featureid_crosswalk USING btree (hydro_id)\"\n",
    "    cur.execute(sql)\n",
    "    conn.commit()\n",
    "\n",
    "print(f\"Successully updated derived.fim4_featureid_crosswalk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92ad73b",
   "metadata": {},
   "source": [
    "<h2>3 - UPDATE FIM HAND PROCESSING LAMBDA WITH NEW FIM VERSION</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85ba44c",
   "metadata": {},
   "source": [
    "<h2>4 - UPDATE FIM DATA PREP LAMBDA WITH NEW FIM VERSION</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60c2897",
   "metadata": {},
   "source": [
    "<h2>5 - UPDATE INITIALIZE PIPELINE LAMBDA TO RUN ONLY AEP FIM SERVICE. UNCOMMENT 2 FIM CONFIGS AT A TIME TO RUN THE AEP CONFIGURATION. CHECK FOR STEP FUNCTION FINISHING BEFORE STARTING NEW ONE.</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d16d550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'executionArn': 'arn:aws:states:us-east-1:526904826677:execution:viz_pipeline_ti:sagemaker_aep_2_5_20230531T1951',\n",
       " 'startDate': datetime.datetime(2023, 5, 31, 19, 51, 29, 633000, tzinfo=tzlocal()),\n",
       " 'ResponseMetadata': {'RequestId': '675cc466-637d-4eba-8d81-6192b703a07a',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '675cc466-637d-4eba-8d81-6192b703a07a',\n",
       "   'date': 'Wed, 31 May 2023 19:51:29 GMT',\n",
       "   'content-type': 'application/x-amz-json-1.0',\n",
       "   'content-length': '143'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_input = {\n",
    "  \"configuration\": \"reference\",\n",
    "  \"job_type\": \"auto\",\n",
    "  \"data_type\": \"channel\",\n",
    "  \"keep_raw\": False,\n",
    "  \"reference_time\": datetime.now().strftime('%Y-%m-%d 00:00:00'),\n",
    "  \"configuration_data_flow\": {\n",
    "    \"db_max_flows\": [],\n",
    "    \"db_ingest_groups\": [],\n",
    "    \"lambda_max_flows\": []\n",
    "  },\n",
    "  \"pipeline_products\": [\n",
    "    {\n",
    "      \"product\": \"static_nwm_aep_inundation_extent_library\",\n",
    "      \"configuration\": \"reference\",\n",
    "      \"product_type\": \"fim\",\n",
    "      \"run\": True,\n",
    "      \"fim_configs\": [\n",
    "        {\n",
    "          \"name\": \"rf_2_inundation\",\n",
    "          \"target_table\": \"aep_fim.rf_2_inundation\",\n",
    "          \"fim_type\": \"hand\",\n",
    "          \"sql_file\": \"rf_2_inundation\"\n",
    "        }\n",
    "      ],\n",
    "      \"services\": [\n",
    "        \"static_nwm_aep_inundation_extent_library_noaa\"\n",
    "      ],\n",
    "      \"raster_outputs\": {\n",
    "        \"output_bucket\": \"\",\n",
    "        \"output_raster_workspaces\": []\n",
    "      },\n",
    "      \"postprocess_sql\": [],\n",
    "      \"product_summaries\": [],\n",
    "      \"lambda_max_flow_dependent\": False\n",
    "    }\n",
    "  ],\n",
    "  \"sql_rename_dict\": {}\n",
    "}\n",
    "\n",
    "client.start_execution(\n",
    "    stateMachineArn = pipeline_arn,\n",
    "    name = f\"sagemaker_aep_2_{datetime.now().strftime('%Y%m%dT%H%M')}\",\n",
    "    input= json.dumps(pipeline_input)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "198d6520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'executionArn': 'arn:aws:states:us-east-1:526904826677:execution:viz_pipeline_ti:sagemaker_aep_5_20230601T1823',\n",
       " 'startDate': datetime.datetime(2023, 6, 1, 18, 23, 46, 533000, tzinfo=tzlocal()),\n",
       " 'ResponseMetadata': {'RequestId': '8111a6bf-62b6-40f1-9e90-7be505b7dcd0',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '8111a6bf-62b6-40f1-9e90-7be505b7dcd0',\n",
       "   'date': 'Thu, 01 Jun 2023 18:23:46 GMT',\n",
       "   'content-type': 'application/x-amz-json-1.0',\n",
       "   'content-length': '141'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_input = {\n",
    "  \"configuration\": \"reference\",\n",
    "  \"job_type\": \"auto\",\n",
    "  \"data_type\": \"channel\",\n",
    "  \"keep_raw\": False,\n",
    "  \"reference_time\": datetime.now().strftime('%Y-%m-%d 00:00:00'),\n",
    "  \"configuration_data_flow\": {\n",
    "    \"db_max_flows\": [],\n",
    "    \"db_ingest_groups\": [],\n",
    "    \"lambda_max_flows\": []\n",
    "  },\n",
    "  \"pipeline_products\": [\n",
    "    {\n",
    "      \"product\": \"static_nwm_aep_inundation_extent_library\",\n",
    "      \"configuration\": \"reference\",\n",
    "      \"product_type\": \"fim\",\n",
    "      \"run\": True,\n",
    "      \"fim_configs\": [\n",
    "        {\n",
    "          \"name\": \"rf_5_inundation\",\n",
    "          \"target_table\": \"aep_fim.rf_5_inundation\",\n",
    "          \"fim_type\": \"hand\",\n",
    "          \"sql_file\": \"rf_5_inundation\"\n",
    "        }\n",
    "      ],\n",
    "      \"services\": [\n",
    "        \"static_nwm_aep_inundation_extent_library_noaa\"\n",
    "      ],\n",
    "      \"raster_outputs\": {\n",
    "        \"output_bucket\": \"\",\n",
    "        \"output_raster_workspaces\": []\n",
    "      },\n",
    "      \"postprocess_sql\": [],\n",
    "      \"product_summaries\": [],\n",
    "      \"lambda_max_flow_dependent\": False\n",
    "    }\n",
    "  ],\n",
    "  \"sql_rename_dict\": {}\n",
    "}\n",
    "\n",
    "client.start_execution(\n",
    "    stateMachineArn = pipeline_arn,\n",
    "    name = f\"sagemaker_aep_5_{datetime.now().strftime('%Y%m%dT%H%M')}\",\n",
    "    input= json.dumps(pipeline_input)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c71d99e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'executionArn': 'arn:aws:states:us-east-1:526904826677:execution:viz_pipeline_ti:sagemaker_aep_10_20230601T1958',\n",
       " 'startDate': datetime.datetime(2023, 6, 1, 19, 58, 2, 218000, tzinfo=tzlocal()),\n",
       " 'ResponseMetadata': {'RequestId': 'd33a004f-f67a-4bd9-b401-107741ee527b',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'd33a004f-f67a-4bd9-b401-107741ee527b',\n",
       "   'date': 'Thu, 01 Jun 2023 19:58:02 GMT',\n",
       "   'content-type': 'application/x-amz-json-1.0',\n",
       "   'content-length': '142'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_input = {\n",
    "  \"configuration\": \"reference\",\n",
    "  \"job_type\": \"auto\",\n",
    "  \"data_type\": \"channel\",\n",
    "  \"keep_raw\": False,\n",
    "  \"reference_time\": datetime.now().strftime('%Y-%m-%d 00:00:00'),\n",
    "  \"configuration_data_flow\": {\n",
    "    \"db_max_flows\": [],\n",
    "    \"db_ingest_groups\": [],\n",
    "    \"lambda_max_flows\": []\n",
    "  },\n",
    "  \"pipeline_products\": [\n",
    "    {\n",
    "      \"product\": \"static_nwm_aep_inundation_extent_library\",\n",
    "      \"configuration\": \"reference\",\n",
    "      \"product_type\": \"fim\",\n",
    "      \"run\": True,\n",
    "      \"fim_configs\": [\n",
    "        {\n",
    "          \"name\": \"rf_10_inundation\",\n",
    "          \"target_table\": \"aep_fim.rf_10_inundation\",\n",
    "          \"fim_type\": \"hand\",\n",
    "          \"sql_file\": \"rf_10_inundation\"\n",
    "        }\n",
    "      ],\n",
    "      \"services\": [\n",
    "        \"static_nwm_aep_inundation_extent_library_noaa\"\n",
    "      ],\n",
    "      \"raster_outputs\": {\n",
    "        \"output_bucket\": \"\",\n",
    "        \"output_raster_workspaces\": []\n",
    "      },\n",
    "      \"postprocess_sql\": [],\n",
    "      \"product_summaries\": [],\n",
    "      \"lambda_max_flow_dependent\": False\n",
    "    }\n",
    "  ],\n",
    "  \"sql_rename_dict\": {}\n",
    "}\n",
    "\n",
    "client.start_execution(\n",
    "    stateMachineArn = pipeline_arn,\n",
    "    name = f\"sagemaker_aep_10_{datetime.now().strftime('%Y%m%dT%H%M')}\",\n",
    "    input= json.dumps(pipeline_input)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bba5449f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'executionArn': 'arn:aws:states:us-east-1:526904826677:execution:viz_pipeline_ti:sagemaker_aep_25_20230602T1353',\n",
       " 'startDate': datetime.datetime(2023, 6, 2, 13, 53, 26, 498000, tzinfo=tzlocal()),\n",
       " 'ResponseMetadata': {'RequestId': '031d9603-f79e-46f2-a6ee-7ff81da8644b',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '031d9603-f79e-46f2-a6ee-7ff81da8644b',\n",
       "   'date': 'Fri, 02 Jun 2023 13:53:26 GMT',\n",
       "   'content-type': 'application/x-amz-json-1.0',\n",
       "   'content-length': '142'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_input = {\n",
    "  \"configuration\": \"reference\",\n",
    "  \"job_type\": \"auto\",\n",
    "  \"data_type\": \"channel\",\n",
    "  \"keep_raw\": False,\n",
    "  \"reference_time\": datetime.now().strftime('%Y-%m-%d 00:00:00'),\n",
    "  \"configuration_data_flow\": {\n",
    "    \"db_max_flows\": [],\n",
    "    \"db_ingest_groups\": [],\n",
    "    \"lambda_max_flows\": []\n",
    "  },\n",
    "  \"pipeline_products\": [\n",
    "    {\n",
    "      \"product\": \"static_nwm_aep_inundation_extent_library\",\n",
    "      \"configuration\": \"reference\",\n",
    "      \"product_type\": \"fim\",\n",
    "      \"run\": True,\n",
    "      \"fim_configs\": [\n",
    "        {\n",
    "          \"name\": \"rf_25_inundation\",\n",
    "          \"target_table\": \"aep_fim.rf_25_inundation\",\n",
    "          \"fim_type\": \"hand\",\n",
    "          \"sql_file\": \"rf_25_inundation\"\n",
    "        }\n",
    "      ],\n",
    "      \"services\": [\n",
    "        \"static_nwm_aep_inundation_extent_library_noaa\"\n",
    "      ],\n",
    "      \"raster_outputs\": {\n",
    "        \"output_bucket\": \"\",\n",
    "        \"output_raster_workspaces\": []\n",
    "      },\n",
    "      \"postprocess_sql\": [],\n",
    "      \"product_summaries\": [],\n",
    "      \"lambda_max_flow_dependent\": False\n",
    "    }\n",
    "  ],\n",
    "  \"sql_rename_dict\": {}\n",
    "}\n",
    "\n",
    "client.start_execution(\n",
    "    stateMachineArn = pipeline_arn,\n",
    "    name = f\"sagemaker_aep_25_{datetime.now().strftime('%Y%m%dT%H%M')}\",\n",
    "    input= json.dumps(pipeline_input)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1df14fd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'executionArn': 'arn:aws:states:us-east-1:526904826677:execution:viz_pipeline_ti:sagemaker_aep_50_20230603T0336',\n",
       " 'startDate': datetime.datetime(2023, 6, 3, 3, 36, 17, 263000, tzinfo=tzlocal()),\n",
       " 'ResponseMetadata': {'RequestId': '51f03d44-52c4-4f60-bf36-87b6ffd90c20',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '51f03d44-52c4-4f60-bf36-87b6ffd90c20',\n",
       "   'date': 'Sat, 03 Jun 2023 03:36:17 GMT',\n",
       "   'content-type': 'application/x-amz-json-1.0',\n",
       "   'content-length': '142'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_input = {\n",
    "  \"configuration\": \"reference\",\n",
    "  \"job_type\": \"auto\",\n",
    "  \"data_type\": \"channel\",\n",
    "  \"keep_raw\": False,\n",
    "  \"reference_time\": datetime.now().strftime('%Y-%m-%d 00:00:00'),\n",
    "  \"configuration_data_flow\": {\n",
    "    \"db_max_flows\": [],\n",
    "    \"db_ingest_groups\": [],\n",
    "    \"lambda_max_flows\": []\n",
    "  },\n",
    "  \"pipeline_products\": [\n",
    "    {\n",
    "      \"product\": \"static_nwm_aep_inundation_extent_library\",\n",
    "      \"configuration\": \"reference\",\n",
    "      \"product_type\": \"fim\",\n",
    "      \"run\": True,\n",
    "      \"fim_configs\": [\n",
    "        {\n",
    "          \"name\": \"rf_50_inundation\",\n",
    "          \"target_table\": \"aep_fim.rf_50_inundation\",\n",
    "          \"fim_type\": \"hand\",\n",
    "          \"sql_file\": \"rf_50_inundation\"\n",
    "        }\n",
    "      ],\n",
    "      \"services\": [\n",
    "        \"static_nwm_aep_inundation_extent_library_noaa\"\n",
    "      ],\n",
    "      \"raster_outputs\": {\n",
    "        \"output_bucket\": \"\",\n",
    "        \"output_raster_workspaces\": []\n",
    "      },\n",
    "      \"postprocess_sql\": [],\n",
    "      \"product_summaries\": [],\n",
    "      \"lambda_max_flow_dependent\": False\n",
    "    }\n",
    "  ],\n",
    "  \"sql_rename_dict\": {}\n",
    "}\n",
    "\n",
    "client.start_execution(\n",
    "    stateMachineArn = pipeline_arn,\n",
    "    name = f\"sagemaker_aep_50_{datetime.now().strftime('%Y%m%dT%H%M')}\",\n",
    "    input= json.dumps(pipeline_input)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a716365c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'executionArn': 'arn:aws:states:us-east-1:526904826677:execution:viz_pipeline_ti:sagemaker_aep_hw_20230603T0302',\n",
       " 'startDate': datetime.datetime(2023, 6, 3, 3, 2, 39, 221000, tzinfo=tzlocal()),\n",
       " 'ResponseMetadata': {'RequestId': '5ef7384d-7aa1-44ed-9a9a-c8dc332bd3f5',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '5ef7384d-7aa1-44ed-9a9a-c8dc332bd3f5',\n",
       "   'date': 'Sat, 03 Jun 2023 03:02:39 GMT',\n",
       "   'content-type': 'application/x-amz-json-1.0',\n",
       "   'content-length': '142'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_input = {\n",
    "  \"configuration\": \"reference\",\n",
    "  \"job_type\": \"auto\",\n",
    "  \"data_type\": \"channel\",\n",
    "  \"keep_raw\": False,\n",
    "  \"reference_time\": datetime.now().strftime('%Y-%m-%d 00:00:00'),\n",
    "  \"configuration_data_flow\": {\n",
    "    \"db_max_flows\": [],\n",
    "    \"db_ingest_groups\": [],\n",
    "    \"lambda_max_flows\": []\n",
    "  },\n",
    "  \"pipeline_products\": [\n",
    "    {\n",
    "      \"product\": \"static_nwm_aep_inundation_extent_library\",\n",
    "      \"configuration\": \"reference\",\n",
    "      \"product_type\": \"fim\",\n",
    "      \"run\": True,\n",
    "      \"fim_configs\": [\n",
    "        {\n",
    "          \"name\": \"rf_high_water_inundation\",\n",
    "          \"target_table\": \"aep_fim.rf_high_water_inundation\",\n",
    "          \"fim_type\": \"hand\",\n",
    "          \"sql_file\": \"rf_high_water_inundation\"\n",
    "        }\n",
    "      ],\n",
    "      \"services\": [\n",
    "        \"static_nwm_aep_inundation_extent_library_noaa\"\n",
    "      ],\n",
    "      \"raster_outputs\": {\n",
    "        \"output_bucket\": \"\",\n",
    "        \"output_raster_workspaces\": []\n",
    "      },\n",
    "      \"postprocess_sql\": [],\n",
    "      \"product_summaries\": [],\n",
    "      \"lambda_max_flow_dependent\": False\n",
    "    }\n",
    "  ],\n",
    "  \"sql_rename_dict\": {}\n",
    "}\n",
    "\n",
    "client.start_execution(\n",
    "    stateMachineArn = pipeline_arn,\n",
    "    name = f\"sagemaker_aep_hw_{datetime.now().strftime('%Y%m%dT%H%M')}\",\n",
    "    input= json.dumps(pipeline_input)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f38492",
   "metadata": {},
   "source": [
    "<h2>6 - RUN CATCHMENT WORKFLOWS 2 CONFIGS AT A TIME. CHECK FOR STEP FUNCTION FINISHING BEFORE STARTING NEW ONE</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553cd908",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_input = {\n",
    "  \"configuration\": \"reference\",\n",
    "  \"job_type\": \"auto\",\n",
    "  \"data_type\": \"channel\",\n",
    "  \"keep_raw\": False,\n",
    "  \"reference_time\": datetime.now().strftime('%Y-%m-%d 00:00:00'),\n",
    "  \"configuration_data_flow\": {\n",
    "    \"db_max_flows\": [],\n",
    "    \"db_ingest_groups\": [],\n",
    "    \"lambda_max_flows\": []\n",
    "  },\n",
    "  \"pipeline_products\": [\n",
    "    {\n",
    "      \"product\": \"static_hand_catchments_0_branches\",\n",
    "      \"configuration\": \"reference\",\n",
    "      \"product_type\": \"fim\",\n",
    "      \"run\": True,\n",
    "      \"fim_configs\": [\n",
    "        {\n",
    "          \"name\": \"branch_0_catchments\",\n",
    "          \"target_table\": \"fim_catchments.branch_0_catchments\",\n",
    "          \"fim_type\": \"hand\",\n",
    "          \"sql_file\": \"branch_0_catchments\"\n",
    "        }\n",
    "      ],\n",
    "      \"services\": [\n",
    "        \"static_hand_catchments_0_branches_noaa\"\n",
    "      ],\n",
    "      \"raster_outputs\": {\n",
    "        \"output_bucket\": \"\",\n",
    "        \"output_raster_workspaces\": []\n",
    "      },\n",
    "      \"postprocess_sql\": [],\n",
    "      \"product_summaries\": [],\n",
    "      \"lambda_max_flow_dependent\": False\n",
    "    },\n",
    "    {\n",
    "      \"product\": \"static_hand_catchments_0_branches_hi\",\n",
    "      \"configuration\": \"reference\",\n",
    "      \"product_type\": \"fim\",\n",
    "      \"run\": True,\n",
    "      \"fim_configs\": [\n",
    "        {\n",
    "          \"name\": \"branch_0_catchments_hi\",\n",
    "          \"target_table\": \"fim_catchments.branch_0_catchments_hi\",\n",
    "          \"fim_type\": \"hand\",\n",
    "          \"sql_file\": \"branch_0_catchments_hi\"\n",
    "        }\n",
    "      ],\n",
    "      \"services\": [\n",
    "        \"static_hand_catchments_0_branches_hi_noaa\"\n",
    "      ],\n",
    "      \"raster_outputs\": {\n",
    "        \"output_bucket\": \"\",\n",
    "        \"output_raster_workspaces\": []\n",
    "      },\n",
    "      \"postprocess_sql\": [],\n",
    "      \"product_summaries\": [],\n",
    "      \"lambda_max_flow_dependent\": False\n",
    "    },\n",
    "    {\n",
    "      \"product\": \"static_hand_catchments_0_branches_prvi\",\n",
    "      \"configuration\": \"reference\",\n",
    "      \"product_type\": \"fim\",\n",
    "      \"run\": True,\n",
    "      \"fim_configs\": [\n",
    "        {\n",
    "          \"name\": \"branch_0_catchments_prvi\",\n",
    "          \"target_table\": \"fim_catchments.branch_0_catchments_prvi\",\n",
    "          \"fim_type\": \"hand\",\n",
    "          \"sql_file\": \"branch_0_catchments_prvi\"\n",
    "        }\n",
    "      ],\n",
    "      \"services\": [\n",
    "        \"static_hand_catchments_0_branches_prvi_noaa\"\n",
    "      ],\n",
    "      \"raster_outputs\": {\n",
    "        \"output_bucket\": \"\",\n",
    "        \"output_raster_workspaces\": []\n",
    "      },\n",
    "      \"postprocess_sql\": [],\n",
    "      \"product_summaries\": [],\n",
    "      \"lambda_max_flow_dependent\": False\n",
    "    }\n",
    "  ],\n",
    "  \"sql_rename_dict\": {}\n",
    "}\n",
    "\n",
    "client.start_execution(\n",
    "    stateMachineArn = pipeline_arn,\n",
    "    name = f\"sagemaker_0_catchments_{datetime.now().strftime('%Y%m%dT%H%M')}\",\n",
    "    input= json.dumps(pipeline_input)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4b7bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_input = {\n",
    "  \"configuration\": \"reference\",\n",
    "  \"job_type\": \"auto\",\n",
    "  \"data_type\": \"channel\",\n",
    "  \"keep_raw\": False,\n",
    "  \"reference_time\": datetime.now().strftime('%Y-%m-%d 00:00:00'),\n",
    "  \"configuration_data_flow\": {\n",
    "    \"db_max_flows\": [],\n",
    "    \"db_ingest_groups\": [],\n",
    "    \"lambda_max_flows\": []\n",
    "  },\n",
    "  \"pipeline_products\": [\n",
    "    {\n",
    "      \"product\": \"static_hand_catchments_gms_branches\",\n",
    "      \"configuration\": \"reference\",\n",
    "      \"product_type\": \"fim\",\n",
    "      \"run\": True,\n",
    "      \"fim_configs\": [\n",
    "        {\n",
    "          \"name\": \"branch_gms_catchments\",\n",
    "          \"target_table\": \"fim_catchments.branch_gms_catchments\",\n",
    "          \"fim_type\": \"hand\",\n",
    "          \"sql_file\": \"branch_gms_catchments\"\n",
    "        }\n",
    "      ],\n",
    "      \"services\": [\n",
    "        \"static_hand_catchments_gms_branches_noaa\"\n",
    "      ],\n",
    "      \"raster_outputs\": {\n",
    "        \"output_bucket\": \"\",\n",
    "        \"output_raster_workspaces\": []\n",
    "      },\n",
    "      \"postprocess_sql\": [],\n",
    "      \"product_summaries\": [],\n",
    "      \"lambda_max_flow_dependent\": False\n",
    "    },\n",
    "    {\n",
    "      \"product\": \"static_hand_catchments_gms_branches_hi\",\n",
    "      \"configuration\": \"reference\",\n",
    "      \"product_type\": \"fim\",\n",
    "      \"run\": True,\n",
    "      \"fim_configs\": [\n",
    "        {\n",
    "          \"name\": \"branch_gms_catchments_hi\",\n",
    "          \"target_table\": \"fim_catchments.branch_gms_catchments_hi\",\n",
    "          \"fim_type\": \"hand\",\n",
    "          \"sql_file\": \"branch_gms_catchments_hi\"\n",
    "        }\n",
    "      ],\n",
    "      \"services\": [\n",
    "        \"static_hand_catchments_gms_branches_hi_noaa\"\n",
    "      ],\n",
    "      \"raster_outputs\": {\n",
    "        \"output_bucket\": \"\",\n",
    "        \"output_raster_workspaces\": []\n",
    "      },\n",
    "      \"postprocess_sql\": [],\n",
    "      \"product_summaries\": [],\n",
    "      \"lambda_max_flow_dependent\": False\n",
    "    },\n",
    "    {\n",
    "      \"product\": \"static_hand_catchments_gms_branches_prvi\",\n",
    "      \"configuration\": \"reference\",\n",
    "      \"product_type\": \"fim\",\n",
    "      \"run\": True,\n",
    "      \"fim_configs\": [\n",
    "        {\n",
    "          \"name\": \"branch_gms_catchments_prvi\",\n",
    "          \"target_table\": \"fim_catchments.branch_gms_catchments_prvi\",\n",
    "          \"fim_type\": \"hand\",\n",
    "          \"sql_file\": \"branch_gms_catchments_prvi\"\n",
    "        }\n",
    "      ],\n",
    "      \"services\": [\n",
    "        \"static_hand_catchments_gms_branches_prvi_noaa\"\n",
    "      ],\n",
    "      \"raster_outputs\": {\n",
    "        \"output_bucket\": \"\",\n",
    "        \"output_raster_workspaces\": []\n",
    "      },\n",
    "      \"postprocess_sql\": [],\n",
    "      \"product_summaries\": [],\n",
    "      \"lambda_max_flow_dependent\": False\n",
    "    }\n",
    "  ],\n",
    "  \"sql_rename_dict\": {}\n",
    "}\n",
    "\n",
    "client.start_execution(\n",
    "    stateMachineArn = pipeline_arn,\n",
    "    name = f\"sagemaker_gms_catchments_{datetime.now().strftime('%Y%m%dT%H%M')}\",\n",
    "    input= json.dumps(pipeline_input)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c28156a",
   "metadata": {},
   "source": [
    "<h2>6 - UPDATE RATING CURVES IN DB</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "90234057",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1 of 1000 on page 1\n",
      "Processing 2 of 1000 on page 1\n",
      "Processing 3 of 1000 on page 1\n",
      "Processing 4 of 1000 on page 1\n",
      "Processing 5 of 1000 on page 1\n",
      "Processing 6 of 1000 on page 1\n",
      "Processing 7 of 1000 on page 1\n",
      "Processing 8 of 1000 on page 1\n",
      "Processing 9 of 1000 on page 1\n",
      "Processing 10 of 1000 on page 1\n",
      "Processing 11 of 1000 on page 1\n",
      "Processing 12 of 1000 on page 1\n",
      "Processing 13 of 1000 on page 1\n",
      "Processing 14 of 1000 on page 1\n",
      "Processing 15 of 1000 on page 1\n",
      "Processing 16 of 1000 on page 1\n",
      "Processing 17 of 1000 on page 1\n",
      "Processing 18 of 1000 on page 1\n",
      "Processing 19 of 1000 on page 1\n",
      "Processing 20 of 1000 on page 1\n",
      "Processing 21 of 1000 on page 1\n",
      "Processing 22 of 1000 on page 1\n",
      "Processing 23 of 1000 on page 1\n",
      "Processing 24 of 1000 on page 1\n",
      "Processing 25 of 1000 on page 1\n",
      "Processing 26 of 1000 on page 1\n",
      "Processing 27 of 1000 on page 1\n",
      "Processing 28 of 1000 on page 1\n",
      "Processing 29 of 1000 on page 1\n",
      "Processing 30 of 1000 on page 1\n",
      "Processing 31 of 1000 on page 1\n",
      "Processing 32 of 1000 on page 1\n",
      "Processing 33 of 1000 on page 1\n",
      "Processing 34 of 1000 on page 1\n",
      "Processing 35 of 1000 on page 1\n",
      "Processing 36 of 1000 on page 1\n",
      "Processing 37 of 1000 on page 1\n",
      "Processing 38 of 1000 on page 1\n",
      "Processing 39 of 1000 on page 1\n",
      "Processing 40 of 1000 on page 1\n",
      "Processing 41 of 1000 on page 1\n",
      "Processing 42 of 1000 on page 1\n",
      "Processing 43 of 1000 on page 1\n",
      "Processing 44 of 1000 on page 1\n",
      "Processing 45 of 1000 on page 1\n",
      "Processing 46 of 1000 on page 1\n",
      "Processing 47 of 1000 on page 1\n",
      "Processing 48 of 1000 on page 1\n",
      "Processing 49 of 1000 on page 1\n",
      "Processing 50 of 1000 on page 1\n",
      "Processing 51 of 1000 on page 1\n",
      "Processing 52 of 1000 on page 1\n",
      "Processing 53 of 1000 on page 1\n",
      "Processing 54 of 1000 on page 1\n",
      "Processing 55 of 1000 on page 1\n",
      "Processing 56 of 1000 on page 1\n",
      "Processing 57 of 1000 on page 1\n",
      "Processing 58 of 1000 on page 1\n",
      "Processing 59 of 1000 on page 1\n",
      "Processing 60 of 1000 on page 1\n",
      "Processing 61 of 1000 on page 1\n",
      "Processing 62 of 1000 on page 1\n",
      "Processing 63 of 1000 on page 1\n",
      "Processing 64 of 1000 on page 1\n",
      "Processing 65 of 1000 on page 1\n",
      "Processing 66 of 1000 on page 1\n",
      "Processing 67 of 1000 on page 1\n",
      "Processing 68 of 1000 on page 1\n",
      "Processing 69 of 1000 on page 1\n",
      "Processing 70 of 1000 on page 1\n",
      "Processing 71 of 1000 on page 1\n",
      "Processing 72 of 1000 on page 1\n",
      "Processing 73 of 1000 on page 1\n",
      "Processing 74 of 1000 on page 1\n",
      "Processing 75 of 1000 on page 1\n",
      "Processing 76 of 1000 on page 1\n",
      "Processing 77 of 1000 on page 1\n",
      "Processing 78 of 1000 on page 1\n",
      "Processing 79 of 1000 on page 1\n",
      "Processing 80 of 1000 on page 1\n",
      "Processing 81 of 1000 on page 1\n",
      "Processing 82 of 1000 on page 1\n",
      "Processing 83 of 1000 on page 1\n",
      "Processing 84 of 1000 on page 1\n",
      "Processing 85 of 1000 on page 1\n",
      "Processing 86 of 1000 on page 1\n",
      "Processing 87 of 1000 on page 1\n",
      "Processing 88 of 1000 on page 1\n",
      "Processing 89 of 1000 on page 1\n",
      "Processing 90 of 1000 on page 1\n",
      "Processing 91 of 1000 on page 1\n",
      "Processing 92 of 1000 on page 1\n",
      "Processing 93 of 1000 on page 1\n",
      "Processing 94 of 1000 on page 1\n",
      "Processing 95 of 1000 on page 1\n",
      "Processing 96 of 1000 on page 1\n",
      "Processing 97 of 1000 on page 1\n",
      "Processing 98 of 1000 on page 1\n",
      "Processing 99 of 1000 on page 1\n",
      "Processing 100 of 1000 on page 1\n",
      "Processing 101 of 1000 on page 1\n",
      "Processing 102 of 1000 on page 1\n",
      "Processing 103 of 1000 on page 1\n",
      "Processing 104 of 1000 on page 1\n",
      "Processing 105 of 1000 on page 1\n",
      "Processing 106 of 1000 on page 1\n",
      "Processing 107 of 1000 on page 1\n",
      "Processing 108 of 1000 on page 1\n",
      "Processing 109 of 1000 on page 1\n",
      "Processing 110 of 1000 on page 1\n",
      "Processing 111 of 1000 on page 1\n",
      "Processing 112 of 1000 on page 1\n",
      "Processing 113 of 1000 on page 1\n",
      "Processing 114 of 1000 on page 1\n",
      "Processing 115 of 1000 on page 1\n",
      "Processing 116 of 1000 on page 1\n",
      "Processing 117 of 1000 on page 1\n",
      "Processing 118 of 1000 on page 1\n",
      "Processing 119 of 1000 on page 1\n",
      "Processing 120 of 1000 on page 1\n",
      "Processing 121 of 1000 on page 1\n",
      "Processing 122 of 1000 on page 1\n",
      "Processing 123 of 1000 on page 1\n",
      "Processing 124 of 1000 on page 1\n",
      "Processing 125 of 1000 on page 1\n",
      "Processing 126 of 1000 on page 1\n",
      "Processing 127 of 1000 on page 1\n",
      "Processing 128 of 1000 on page 1\n",
      "Processing 129 of 1000 on page 1\n",
      "Processing 130 of 1000 on page 1\n",
      "Processing 131 of 1000 on page 1\n",
      "Processing 132 of 1000 on page 1\n",
      "Processing 133 of 1000 on page 1\n",
      "Processing 134 of 1000 on page 1\n",
      "Processing 135 of 1000 on page 1\n",
      "Processing 136 of 1000 on page 1\n",
      "Processing 137 of 1000 on page 1\n",
      "Processing 138 of 1000 on page 1\n",
      "Processing 139 of 1000 on page 1\n",
      "Processing 140 of 1000 on page 1\n",
      "Processing 141 of 1000 on page 1\n",
      "Processing 142 of 1000 on page 1\n",
      "Processing 143 of 1000 on page 1\n",
      "Processing 144 of 1000 on page 1\n",
      "Processing 145 of 1000 on page 1\n",
      "Processing 146 of 1000 on page 1\n",
      "Processing 147 of 1000 on page 1\n",
      "Processing 148 of 1000 on page 1\n",
      "Processing 149 of 1000 on page 1\n",
      "Processing 150 of 1000 on page 1\n",
      "Processing 151 of 1000 on page 1\n",
      "Processing 152 of 1000 on page 1\n",
      "Processing 153 of 1000 on page 1\n",
      "Processing 154 of 1000 on page 1\n",
      "Processing 155 of 1000 on page 1\n",
      "Processing 156 of 1000 on page 1\n",
      "Processing 157 of 1000 on page 1\n",
      "Processing 158 of 1000 on page 1\n",
      "Processing 159 of 1000 on page 1\n",
      "Processing 160 of 1000 on page 1\n",
      "Processing 161 of 1000 on page 1\n",
      "Processing 162 of 1000 on page 1\n",
      "Processing 163 of 1000 on page 1\n",
      "Processing 164 of 1000 on page 1\n",
      "Processing 165 of 1000 on page 1\n",
      "Processing 166 of 1000 on page 1\n",
      "Processing 167 of 1000 on page 1\n",
      "Processing 168 of 1000 on page 1\n",
      "Processing 169 of 1000 on page 1\n",
      "Processing 170 of 1000 on page 1\n",
      "Processing 171 of 1000 on page 1\n",
      "Processing 172 of 1000 on page 1\n",
      "Processing 173 of 1000 on page 1\n",
      "Processing 174 of 1000 on page 1\n",
      "Processing 175 of 1000 on page 1\n",
      "Processing 176 of 1000 on page 1\n",
      "Processing 177 of 1000 on page 1\n",
      "Processing 178 of 1000 on page 1\n",
      "Processing 179 of 1000 on page 1\n",
      "Processing 180 of 1000 on page 1\n",
      "Processing 181 of 1000 on page 1\n",
      "Processing 182 of 1000 on page 1\n",
      "Processing 183 of 1000 on page 1\n",
      "Processing 184 of 1000 on page 1\n",
      "******************************************\n",
      "Error encountered on fim_4_3_11_0/hand_datasets/03040205/branches/0/usgs_elev_table.csv\n",
      "(psycopg2.DataError) invalid input syntax for type bigint: \"DD62E384\"\n",
      "LINE 1: ...', 33.8339,  -80.1315, 'NAD83', 'South Carolina', 'DD62E384'...\n",
      "                                                             ^\n",
      "\n",
      "[SQL: INSERT INTO derived.usgs_elev_table (wrds_timestamp, nrldb_timestamp, nwis_timestamp, metadata_sources, nws_lid, location_id, feature_id, goes_id, env_can_gage_id, nws_data_name, nws_data_wfo, nws_data_rfc, nws_data_geo_rfc, nws_data_latitude, nws_data_longitude, nws_data_map_link, nws_data_horizontal_datum_name, nws_data_state, nws_data_county, nws_data_county_code, nws_data_huc, nws_data_hsa, nws_data_zero_datum, nws_data_vertical_datum_name, nws_data_rfc_forecast_point, nws_data_rfc_defined_fcst_point, nws_data_riverpoint, usgs_data_name, usgs_data_geo_rfc, usgs_data_latitude, usgs_data_longitude, usgs_data_map_link, usgs_data_coord_accuracy_code, usgs_data_latlon_datum_name, usgs_data_coord_method_code, usgs_data_state, usgs_data_huc, usgs_data_site_type, usgs_data_altitude, usgs_data_alt_accuracy_code, usgs_data_alt_datum_code, usgs_data_alt_method_code, usgs_data_drainage_area, usgs_data_drainage_area_units, usgs_data_contrib_drainage_area, usgs_data_active, usgs_data_gages_ii_reference, nwm_feature_data_downstream_feature_id, nwm_feature_data_latitude, nwm_feature_data_longitude, nwm_feature_data_altitude, nwm_feature_data_stream_length, nwm_feature_data_stream_order, nwm_feature_data_mannings_roughness, nwm_feature_data_slope, nwm_feature_data_channel_side_slope, nwm_feature_data_nhd_waterbody_comid, env_can_gage_data_name, env_can_gage_data_latitude, env_can_gage_data_longitude, env_can_gage_data_map_link, env_can_gage_data_drainage_area, env_can_gage_data_contrib_drainage_area, env_can_gage_data_water_course, nws_preferred_name, nws_preferred_latitude, nws_preferred_longitude, nws_preferred_latlon_datum_name, nws_preferred_state, nws_preferred_huc, usgs_preferred_name, usgs_preferred_latitude, usgs_preferred_longitude, usgs_preferred_latlon_datum_name, usgs_preferred_state, usgs_preferred_huc, crosswalk_datasets_location_nwm_crosswalk_dataset_location_nwm_crosswalk_dataset_id, crosswalk_datasets_location_nwm_crosswalk_dataset_name, crosswalk_datasets_location_nwm_crosswalk_dataset_description, crosswalk_datasets_nws_usgs_crosswalk_dataset_nws_usgs_crosswalk_dataset_id, crosswalk_datasets_nws_usgs_crosswalk_dataset_name, crosswalk_datasets_nws_usgs_crosswalk_dataset_description, assigned_crs, \"HUC8\", name, states, curve, mainstem, acceptable_codes, acceptable_alt_error, levpa_id, order_, geometry, index_right, \"HydroID\", \"LakeID\", geometry_ln, geometry_snapped, snap_distance, dem_elevation, dem_adj_elevation, fim_version) VALUES (%(wrds_timestamp)s, %(nrldb_timestamp)s, %(nwis_timestamp)s, %(metadata_sources)s, %(nws_lid)s, %(location_id)s, %(feature_id)s, %(goes_id)s, %(env_can_gage_id)s, %(nws_data_name)s, %(nws_data_wfo)s, %(nws_data_rfc)s, %(nws_data_geo_rfc)s, %(nws_data_latitude)s, %(nws_data_longitude)s, %(nws_data_map_link)s, %(nws_data_horizontal_datum_name)s, %(nws_data_state)s, %(nws_data_county)s, %(nws_data_county_code)s, %(nws_data_huc)s, %(nws_data_hsa)s, %(nws_data_zero_datum)s, %(nws_data_vertical_datum_name)s, %(nws_data_rfc_forecast_point)s, %(nws_data_rfc_defined_fcst_point)s, %(nws_data_riverpoint)s, %(usgs_data_name)s, %(usgs_data_geo_rfc)s, %(usgs_data_latitude)s, %(usgs_data_longitude)s, %(usgs_data_map_link)s, %(usgs_data_coord_accuracy_code)s, %(usgs_data_latlon_datum_name)s, %(usgs_data_coord_method_code)s, %(usgs_data_state)s, %(usgs_data_huc)s, %(usgs_data_site_type)s, %(usgs_data_altitude)s, %(usgs_data_alt_accuracy_code)s, %(usgs_data_alt_datum_code)s, %(usgs_data_alt_method_code)s, %(usgs_data_drainage_area)s, %(usgs_data_drainage_area_units)s, %(usgs_data_contrib_drainage_area)s, %(usgs_data_active)s, %(usgs_data_gages_ii_reference)s, %(nwm_feature_data_downstream_feature_id)s, %(nwm_feature_data_latitude)s, %(nwm_feature_data_longitude)s, %(nwm_feature_data_altitude)s, %(nwm_feature_data_stream_length)s, %(nwm_feature_data_stream_order)s, %(nwm_feature_data_mannings_roughness)s, %(nwm_feature_data_slope)s, %(nwm_feature_data_channel_side_slope)s, %(nwm_feature_data_nhd_waterbody_comid)s, %(env_can_gage_data_name)s, %(env_can_gage_data_latitude)s, %(env_can_gage_data_longitude)s, %(env_can_gage_data_map_link)s, %(env_can_gage_data_drainage_area)s, %(env_can_gage_data_contrib_drainage_area)s, %(env_can_gage_data_water_course)s, %(nws_preferred_name)s, %(nws_preferred_latitude)s, %(nws_preferred_longitude)s, %(nws_preferred_latlon_datum_name)s, %(nws_preferred_state)s, %(nws_preferred_huc)s, %(usgs_preferred_name)s, %(usgs_preferred_latitude)s, %(usgs_preferred_longitude)s, %(usgs_preferred_latlon_datum_name)s, %(usgs_preferred_state)s, %(usgs_preferred_huc)s, %(crosswalk_datasets_location_nwm_crosswalk_dataset_location_nwm_crosswalk_dataset_id)s, %(crosswalk_datasets_location_nwm_crosswalk_dataset_name)s, %(crosswalk_datasets_location_nwm_crosswalk_dataset_description)s, %(crosswalk_datasets_nws_usgs_crosswalk_dataset_nws_usgs_crosswalk_dataset_id)s, %(crosswalk_datasets_nws_usgs_crosswalk_dataset_name)s, %(crosswalk_datasets_nws_usgs_crosswalk_dataset_description)s, %(assigned_crs)s, %(HUC8)s, %(name)s, %(states)s, %(curve)s, %(mainstem)s, %(acceptable_codes)s, %(acceptable_alt_error)s, %(levpa_id)s, %(order_)s, %(geometry)s, %(index_right)s, %(HydroID)s, %(LakeID)s, %(geometry_ln)s, %(geometry_snapped)s, %(snap_distance)s, %(dem_elevation)s, %(dem_adj_elevation)s, %(fim_version)s)]\n",
      "[parameters: ({'wrds_timestamp': 'USGS data: USGS NWIS - Last updated: 2022-11-28 22:16:23 UTC', 'nrldb_timestamp': 'NWS data: NRLDB - Last updated: 2021-11-15 22:57:38 UTC', 'nwis_timestamp': 'USGS data: USGS NWIS - Last updated: 2022-11-28 22:16:23 UTC', 'metadata_sources': \"['NWS data: NRLDB - Last updated: 2021-11-15 22:57:38 UTC', 'USACE data: USACE - Last updated: 2021-09-24 17:19:28 UTC', 'USGS data: USGS NWIS - Last updated: 2022-11-28 22:16:23 UTC']\", 'nws_lid': 'KINS1', 'location_id': 2136000, 'feature_id': 9090136, 'goes_id': 'DD823066', 'env_can_gage_id': None, 'nws_data_name': 'Kingstree', 'nws_data_wfo': 'ILM', 'nws_data_rfc': 'SERFC', 'nws_data_geo_rfc': 'SERFC', 'nws_data_latitude': 33.661111111111005, 'nws_data_longitude': -79.83611111111098, 'nws_data_map_link': 'https://maps.google.com/maps?t=k&q=loc:33.661111111111+-79.836111111111', 'nws_data_horizontal_datum_name': 'UNK', 'nws_data_state': 'South Carolina', 'nws_data_county': 'Williamsburg', 'nws_data_county_code': 45089, 'nws_data_huc': '03040205', 'nws_data_hsa': 'ILM', 'nws_data_zero_datum': 25.66, 'nws_data_vertical_datum_name': None, 'nws_data_rfc_forecast_point': 1, 'nws_data_rfc_defined_fcst_point': 1, 'nws_data_riverpoint': 1, 'usgs_data_name': 'BLACK RIVER AT KINGSTREE, SC', 'usgs_data_geo_rfc': 'SERFC', 'usgs_data_latitude': 33.66127545, 'usgs_data_longitude': -79.83590448, 'usgs_data_map_link': 'https://maps.google.com/maps?t=k&q=loc:33.66127545+-79.83590448', 'usgs_data_coord_accuracy_code': 'U', 'usgs_data_latlon_datum_name': 'NAD83', 'usgs_data_coord_method_code': 'M', 'usgs_data_state': 'South Carolina', 'usgs_data_huc': 3040205, 'usgs_data_site_type': 'ST', 'usgs_data_altitude': 24.48, 'usgs_data_alt_accuracy_code': 0.18, 'usgs_data_alt_datum_code': 'NAVD88', 'usgs_data_alt_method_code': 'X', 'usgs_data_drainage_area': 1252.0, 'usgs_data_drainage_area_units': 'square miles', 'usgs_data_contrib_drainage_area': None, 'usgs_data_active': 1, 'usgs_data_gages_ii_reference': 0, 'nwm_feature_data_downstream_feature_id': 9090262.0, 'nwm_feature_data_latitude': 33.662459999999996, 'nwm_feature_data_longitude': -79.83708, 'nwm_feature_data_altitude': 10.16, 'nwm_feature_data_stream_length': 137.0, 'nwm_feature_data_stream_order': 6.0, 'nwm_feature_data_mannings_roughness': 0.05, 'nwm_feature_data_slope': 1e-05, 'nwm_feature_data_channel_side_slope': 0.23449299999999998, 'nwm_feature_data_nhd_waterbody_comid': None, 'env_can_gage_data_name': None, 'env_can_gage_data_latitude': None, 'env_can_gage_data_longitude': None, 'env_can_gage_data_map_link': None, 'env_can_gage_data_drainage_area': None, 'env_can_gage_data_contrib_drainage_area': None, 'env_can_gage_data_water_course': None, 'nws_preferred_name': 'Kingstree', 'nws_preferred_latitude': 33.661111111111005, 'nws_preferred_longitude': -79.836111111111, 'nws_preferred_latlon_datum_name': 'UNK', 'nws_preferred_state': 'South Carolina', 'nws_preferred_huc': '03040205', 'usgs_preferred_name': 'BLACK RIVER AT KINGSTREE, SC', 'usgs_preferred_latitude': 33.66127545, 'usgs_preferred_longitude': -79.83590448, 'usgs_preferred_latlon_datum_name': 'NAD83', 'usgs_preferred_state': 'South Carolina', 'usgs_preferred_huc': 3040205, 'crosswalk_datasets_location_nwm_crosswalk_dataset_location_nwm_crosswalk_dataset_id': 1.2, 'crosswalk_datasets_location_nwm_crosswalk_dataset_name': 'Location NWM Crosswalk v1.2', 'crosswalk_datasets_location_nwm_crosswalk_dataset_description': 'Created 20220831.  Source:\\n1) NWM Routelink File v2.2-corrected\\n2) NHDPlus v2.1+\\n3) GID', 'crosswalk_datasets_nws_usgs_crosswalk_dataset_nws_usgs_crosswalk_dataset_id': 2.0, 'crosswalk_datasets_nws_usgs_crosswalk_dataset_name': 'NWS Station to USGS Gages 2.0', 'crosswalk_datasets_nws_usgs_crosswalk_dataset_description': 'Created 20210809.  Authoritative 2.0 dataset mapping NWS Stations to USGS Gages.  Source 1) AHPS CMS Report  2) HADS dataset  3) GID', 'assigned_crs': 'EPSG:4269', 'HUC8': 3040205, 'name': 'Black', 'states': 'SC', 'curve': 'yes', 'mainstem': 'yes', 'acceptable_codes': 0, 'acceptable_alt_error': 1, 'levpa_id': 0, 'order_': 6, 'geometry': 'POINT (1481418.4335078613 1303420.6111954711)', 'index_right': 2288, 'HydroID': 11840108, 'LakeID': -999.0, 'geometry_ln': 'LINESTRING (1481334.409435884 1303469.2824222166, 1481334.409435884 1303459.2824222166, 1481344.409435884 1303449.2824222166, 1481344.409435884 13034 ... (4501 characters truncated) ... 81624.409435884 1302249.2824222166, 1481634.409435884 1302249.2824222166, 1481644.409435884 1302239.2824222166, 1481654.409435884 1302229.2824222166)', 'geometry_snapped': 'POINT (1481354.409435884 1303389.2824222166)', 'snap_distance': 71.2781440988793, 'dem_elevation': 10.397894, 'dem_adj_elevation': 10.182439, 'fim_version': '4.3.11.0'}, {'wrds_timestamp': 'USGS data: USGS NWIS - Last updated: 2022-11-28 22:16:23 UTC', 'nrldb_timestamp': 'NWS data: NRLDB - Last updated: 2021-11-15 22:57:38 UTC', 'nwis_timestamp': 'USGS data: USGS NWIS - Last updated: 2022-11-28 22:16:23 UTC', 'metadata_sources': \"['NWS data: NRLDB - Last updated: 2021-11-15 22:57:38 UTC', 'USACE data: USACE - Last updated: 2021-09-24 17:19:28 UTC', 'USGS data: USGS NWIS - Last updated: 2022-11-28 22:16:23 UTC']\", 'nws_lid': 'ADRS1', 'location_id': 2136030, 'feature_id': 9097804, 'goes_id': None, 'env_can_gage_id': None, 'nws_data_name': 'Andrews', 'nws_data_wfo': 'ILM', 'nws_data_rfc': 'SERFC', 'nws_data_geo_rfc': 'SERFC', 'nws_data_latitude': 33.490278, 'nws_data_longitude': -79.544833, 'nws_data_map_link': 'https://maps.google.com/maps?t=k&q=loc:33.490278+-79.544833', 'nws_data_horizontal_datum_name': None, 'nws_data_state': 'South Carolina', 'nws_data_county': 'Georgetown', 'nws_data_county_code': 45043, 'nws_data_huc': None, 'nws_data_hsa': 'ILM', 'nws_data_zero_datum': None, 'nws_data_vertical_datum_name': None, 'nws_data_rfc_forecast_point': 0, 'nws_data_rfc_defined_fcst_point': 0, 'nws_data_riverpoint': 1, 'usgs_data_name': 'BLACK RIVER NEAR ANDREWS, SC', 'usgs_data_geo_rfc': 'SERFC', 'usgs_data_latitude': 33.49044467, 'usgs_data_longitude': -79.54561819, 'usgs_data_map_link': 'https://maps.google.com/maps?t=k&q=loc:33.49044467+-79.54561819', 'usgs_data_coord_accuracy_code': 'S', 'usgs_data_latlon_datum_name': 'NAD83', 'usgs_data_coord_method_code': 'M', 'usgs_data_state': 'South Carolina', 'usgs_data_huc': 3040205, 'usgs_data_site_type': 'ST', 'usgs_data_altitude': -17.23, 'usgs_data_alt_accuracy_code': 0.01, 'usgs_data_alt_datum_code': 'NAVD88', 'usgs_data_alt_method_code': 'X', 'usgs_data_drainage_area': 1560.0, 'usgs_data_drainage_area_units': 'square miles', 'usgs_data_contrib_drainage_area': None, 'usgs_data_active': 1, 'usgs_data_gages_ii_reference': 0, 'nwm_feature_data_downstream_feature_id': 9097802.0, 'nwm_feature_data_latitude': 33.486270000000005, 'nwm_feature_data_longitude': -79.54307, 'nwm_feature_data_altitude': 5.49, 'nwm_feature_data_stream_length': 1638.0, 'nwm_feature_data_stream_order': 6.0, 'nwm_feature_data_mannings_roughness': 0.05, 'nwm_feature_data_slope': 1e-05, 'nwm_feature_data_channel_side_slope': 0.22676770000000002, 'nwm_feature_data_nhd_waterbody_comid': None, 'env_can_gage_data_name': None, 'env_can_gage_data_latitude': None, 'env_can_gage_data_longitude': None, 'env_can_gage_data_map_link': None, 'env_can_gage_data_drainage_area': None, 'env_can_gage_data_contrib_drainage_area': None, 'env_can_gage_data_water_course': None, 'nws_preferred_name': 'Andrews', 'nws_preferred_latitude': 33.490278, 'nws_preferred_longitude': -79.544833, 'nws_preferred_latlon_datum_name': 'NAD83', 'nws_preferred_state': 'South Carolina', 'nws_preferred_huc': '03040205', 'usgs_preferred_name': 'BLACK RIVER NEAR ANDREWS, SC', 'usgs_preferred_latitude': 33.49044467, 'usgs_preferred_longitude': -79.54561819, 'usgs_preferred_latlon_datum_name': 'NAD83', 'usgs_preferred_state': 'South Carolina', 'usgs_preferred_huc': 3040205, 'crosswalk_datasets_location_nwm_crosswalk_dataset_location_nwm_crosswalk_dataset_id': 1.2, 'crosswalk_datasets_location_nwm_crosswalk_dataset_name': 'Location NWM Crosswalk v1.2', 'crosswalk_datasets_location_nwm_crosswalk_dataset_description': 'Created 20220831.  Source:\\n1) NWM Routelink File v2.2-corrected\\n2) NHDPlus v2.1+\\n3) GID', 'crosswalk_datasets_nws_usgs_crosswalk_dataset_nws_usgs_crosswalk_dataset_id': 2.0, 'crosswalk_datasets_nws_usgs_crosswalk_dataset_name': 'NWS Station to USGS Gages 2.0', 'crosswalk_datasets_nws_usgs_crosswalk_dataset_description': 'Created 20210809.  Authoritative 2.0 dataset mapping NWS Stations to USGS Gages.  Source 1) AHPS CMS Report  2) HADS dataset  3) GID', 'assigned_crs': 'EPSG:4269', 'HUC8': 3040205, 'name': 'Black', 'states': 'SC', 'curve': 'yes', 'mainstem': 'yes', 'acceptable_codes': 1, 'acceptable_alt_error': 1, 'levpa_id': 0, 'order_': 6, 'geometry': 'POINT (1511046.0622315782 1289190.2372099361)', 'index_right': 2869, 'HydroID': 11840066, 'LakeID': -999.0, 'geometry_ln': 'LINESTRING (1510714.409435884 1289949.2824222166, 1510714.409435884 1289949.2824222166, 1510714.409435884 1289949.2824222166, 1510714.409435884 12899 ... (7693 characters truncated) ... 11114.409435884 1288949.2824222166, 1511124.409435884 1288939.2824222166, 1511134.409435884 1288929.2824222166, 1511134.409435884 1288919.2824222166)', 'geometry_snapped': 'POINT (1511004.409435884 1289190.2372099361)', 'snap_distance': 41.65279569406994, 'dem_elevation': 1.1076399, 'dem_adj_elevation': 1.1076399, 'fim_version': '4.3.11.0'}, {'wrds_timestamp': 'USGS data: USGS NWIS - Last updated: 2022-11-28 22:16:23 UTC', 'nrldb_timestamp': 'NWS data: NRLDB - Last updated: 2021-11-15 22:57:38 UTC', 'nwis_timestamp': 'USGS data: USGS NWIS - Last updated: 2022-11-28 22:16:23 UTC', 'metadata_sources': \"['NWS data: NRLDB - Last updated: 2021-11-15 22:57:38 UTC', 'USACE data: USACE - Last updated: 2021-09-24 17:19:28 UTC', 'USGS data: USGS NWIS - Last updated: 2022-11-28 22:16:23 UTC']\", 'nws_lid': 'BLIS1', 'location_id': 2135501, 'feature_id': 9093360, 'goes_id': None, 'env_can_gage_id': None, 'nws_data_name': 'Black River at I-95 near Manning', 'nws_data_wfo': 'CAE', 'nws_data_rfc': 'SERFC', 'nws_data_geo_rfc': 'SERFC', 'nws_data_latitude': 33.8339, 'nws_data_longitude': -80.1315, 'nws_data_map_link': 'https://maps.google.com/maps?t=k&q=loc:33.8339+-80.1315', 'nws_data_horizontal_datum_name': 'NAD83', 'nws_data_state': 'South Carolina', 'nws_data_county': 'Clarendon', 'nws_data_county_code': 45027, 'nws_data_huc': 'DD62E384', 'nws_data_hsa': 'CAE', 'nws_data_zero_datum': None, 'nws_data_vertical_datum_name': '68.04', 'nws_data_rfc_forecast_point': 0, 'nws_data_rfc_defined_fcst_point': 0, 'nws_data_riverpoint': 1, 'usgs_data_name': 'BLACK RIVER AT I-95 NEAR MANNING, SC', 'usgs_data_geo_rfc': 'SERFC', 'usgs_data_latitude': 33.83388889, 'usgs_data_longitude': -80.1315833, 'usgs_data_map_link': 'https://maps.google.com/maps?t=k&q=loc:33.83388889+-80.1315833', 'usgs_data_coord_accuracy_code': 'S', 'usgs_data_latlon_datum_name': 'NAD83', 'usgs_data_coord_method_code': 'M', 'usgs_data_state': 'South Carolina', 'usgs_data_huc': 3040205, 'usgs_data_site_type': 'ST', 'usgs_data_altitude': 68.04, 'usgs_data_alt_accuracy_code': 0.04, 'usgs_data_alt_datum_code': 'NAVD88', 'usgs_data_alt_method_code': 'X', 'usgs_data_drainage_area': 415.0, 'usgs_data_drainage_area_units': 'square miles', 'usgs_data_contrib_drainage_area': None, 'usgs_data_active': 1, 'usgs_data_gages_ii_reference': 0, 'nwm_feature_data_downstream_feature_id': None, 'nwm_feature_data_latitude': None, 'nwm_feature_data_longitude': None, 'nwm_feature_data_altitude': None, 'nwm_feature_data_stream_length': None, 'nwm_feature_data_stream_order': None, 'nwm_feature_data_mannings_roughness': None, 'nwm_feature_data_slope': None, 'nwm_feature_data_channel_side_slope': None, 'nwm_feature_data_nhd_waterbody_comid': None, 'env_can_gage_data_name': None, 'env_can_gage_data_latitude': None, 'env_can_gage_data_longitude': None, 'env_can_gage_data_map_link': None, 'env_can_gage_data_drainage_area': None, 'env_can_gage_data_contrib_drainage_area': None, 'env_can_gage_data_water_course': None, 'nws_preferred_name': 'Black River at I-95 near Manning', 'nws_preferred_latitude': 33.8339, 'nws_preferred_longitude': -80.1315, 'nws_preferred_latlon_datum_name': 'NAD83', 'nws_preferred_state': 'South Carolina', 'nws_preferred_huc': 'DD62E384', 'usgs_preferred_name': 'BLACK RIVER AT I-95 NEAR MANNING, SC', 'usgs_preferred_latitude': 33.83388889, 'usgs_preferred_longitude': -80.1315833, 'usgs_preferred_latlon_datum_name': 'NAD83', 'usgs_preferred_state': 'South Carolina', 'usgs_preferred_huc': 3040205, 'crosswalk_datasets_location_nwm_crosswalk_dataset_location_nwm_crosswalk_dataset_id': 1.2, 'crosswalk_datasets_location_nwm_crosswalk_dataset_name': 'Location NWM Crosswalk v1.2', 'crosswalk_datasets_location_nwm_crosswalk_dataset_description': 'Created 20220831.  Source:\\n1) NWM Routelink File v2.2-corrected\\n2) NHDPlus v2.1+\\n3) GID', 'crosswalk_datasets_nws_usgs_crosswalk_dataset_nws_usgs_crosswalk_dataset_id': 2.0, 'crosswalk_datasets_nws_usgs_crosswalk_dataset_name': 'NWS Station to USGS Gages 2.0', 'crosswalk_datasets_nws_usgs_crosswalk_dataset_description': 'Created 20210809.  Authoritative 2.0 dataset mapping NWS Stations to USGS Gages.  Source 1) AHPS CMS Report  2) HADS dataset  3) GID', 'assigned_crs': 'EPSG:4269', 'HUC8': 3040205, 'name': 'Black', 'states': 'SC', 'curve': 'no', 'mainstem': 'no', 'acceptable_codes': 1, 'acceptable_alt_error': 1, 'levpa_id': 0, 'order_': 5, 'geometry': 'POINT (1451369.3963914735 1317869.6014176193)', 'index_right': 1369, 'HydroID': 11842255, 'LakeID': -999.0, 'geometry_ln': 'LINESTRING (1450594.409435884 1317979.2824222166, 1450594.409435884 1317979.2824222166, 1450594.409435884 1317979.2824222166, 1450594.409435884 13179 ... (7465 characters truncated) ... 51024.409435884 1317149.2824222166, 1451014.409435884 1317139.2824222166, 1451014.409435884 1317129.2824222166, 1451004.409435884 1317119.2824222166)', 'geometry_snapped': 'POINT (1451104.409435884 1317379.2824222166)', 'snap_distance': 557.3426270125075, 'dem_elevation': 25.27583, 'dem_adj_elevation': 25.211945999999998, 'fim_version': '4.3.11.0'}, {'wrds_timestamp': 'USGS data: USGS NWIS - Last updated: 2022-11-28 22:16:23 UTC', 'nrldb_timestamp': 'NWS data: NRLDB - Last updated: 2021-11-15 22:57:38 UTC', 'nwis_timestamp': 'USGS data: USGS NWIS - Last updated: 2022-11-28 22:16:23 UTC', 'metadata_sources': \"['NWS data: NRLDB - Last updated: 2021-11-15 22:57:38 UTC', 'USACE data: USACE - Last updated: 2021-09-24 17:19:28 UTC', 'USGS data: USGS NWIS - Last updated: 2022-11-28 22:16:23 UTC']\", 'nws_lid': 'POCS1', 'location_id': 2135615, 'feature_id': 9094292, 'goes_id': None, 'env_can_gage_id': None, 'nws_data_name': 'Pocotaligo River at I-95, above Manning, SC', 'nws_data_wfo': 'CAE', 'nws_data_rfc': 'SERFC', 'nws_data_geo_rfc': 'SERFC', 'nws_data_latitude': 33.7294, 'nws_data_longitude': -80.22, 'nws_data_map_link': 'https://maps.google.com/maps?t=k&q=loc:33.7294+-80.22', 'nws_data_horizontal_datum_name': 'NAD83', 'nws_data_state': 'South Carolina', 'nws_data_county': 'Clarendon', 'nws_data_county_code': 45027, 'nws_data_huc': '03040205', 'nws_data_hsa': 'CAE', 'nws_data_zero_datum': 73.5, 'nws_data_vertical_datum_name': 'NAVD88', 'nws_data_rfc_forecast_point': 0, 'nws_data_rfc_defined_fcst_point': 0, 'nws_data_riverpoint': 1, 'usgs_data_name': 'POCOTALIGO RIVER AT I-95 ABOVE MANNING, SC', 'usgs_data_geo_rfc': 'SERFC', 'usgs_data_latitude': 33.72963889, 'usgs_data_longitude': -80.2201944, 'usgs_data_map_link': 'https://maps.google.com/maps?t=k&q=loc:33.72963889+-80.2201944', 'usgs_data_coord_accuracy_code': 'S', 'usgs_data_latlon_datum_name': 'NAD83', 'usgs_data_coord_method_code': 'M', 'usgs_data_state': 'South Carolina', 'usgs_data_huc': 3040205, 'usgs_data_site_type': 'ST', 'usgs_data_altitude': 73.5, 'usgs_data_alt_accuracy_code': 0.04, 'usgs_data_alt_datum_code': 'NAVD88', 'usgs_data_alt_method_code': 'X', 'usgs_data_drainage_area': 197.0, 'usgs_data_drainage_area_units': 'square miles', 'usgs_data_contrib_drainage_area': None, 'usgs_data_active': 1, 'usgs_data_gages_ii_reference': 0, 'nwm_feature_data_downstream_feature_id': 9094304.0, 'nwm_feature_data_latitude': 33.72963, 'nwm_feature_data_longitude': -80.22047000000002, 'nwm_feature_data_altitude': 25.02, 'nwm_feature_data_stream_length': 186.0, 'nwm_feature_data_stream_order': 5.0, 'nwm_feature_data_mannings_roughness': 0.05, 'nwm_feature_data_slope': 0.0011300000000000001, 'nwm_feature_data_channel_side_slope': 0.29287959999999996, 'nwm_feature_data_nhd_waterbody_comid': None, 'env_can_gage_data_name': None, 'env_can_gage_data_latitude': None, 'env_can_gage_data_longitude': None, 'env_can_gage_data_map_link': None, 'env_can_gage_data_drainage_area': None, 'env_can_gage_data_contrib_drainage_area': None, 'env_can_gage_data_water_course': None, 'nws_preferred_name': 'Pocotaligo River at I-95, above Manning, SC', 'nws_preferred_latitude': 33.7294, 'nws_preferred_longitude': -80.22, 'nws_preferred_latlon_datum_name': 'NAD83', 'nws_preferred_state': 'South Carolina', 'nws_preferred_huc': '03040205', 'usgs_preferred_name': 'POCOTALIGO RIVER AT I-95 ABOVE MANNING, SC', 'usgs_preferred_latitude': 33.72963889, 'usgs_preferred_longitude': -80.2201944, 'usgs_preferred_latlon_datum_name': 'NAD83', 'usgs_preferred_state': 'South Carolina', 'usgs_preferred_huc': 3040205, 'crosswalk_datasets_location_nwm_crosswalk_dataset_location_nwm_crosswalk_dataset_id': 1.2, 'crosswalk_datasets_location_nwm_crosswalk_dataset_name': 'Location NWM Crosswalk v1.2', 'crosswalk_datasets_location_nwm_crosswalk_dataset_description': 'Created 20220831.  Source:\\n1) NWM Routelink File v2.2-corrected\\n2) NHDPlus v2.1+\\n3) GID', 'crosswalk_datasets_nws_usgs_crosswalk_dataset_nws_usgs_crosswalk_dataset_id': 2.0, 'crosswalk_datasets_nws_usgs_crosswalk_dataset_name': 'NWS Station to USGS Gages 2.0', 'crosswalk_datasets_nws_usgs_crosswalk_dataset_description': 'Created 20210809.  Authoritative 2.0 dataset mapping NWS Stations to USGS Gages.  Source 1) AHPS CMS Report  2) HADS dataset  3) GID', 'assigned_crs': 'EPSG:4269', 'HUC8': 3040205, 'name': 'Black', 'states': 'SC', 'curve': 'no', 'mainstem': 'no', 'acceptable_codes': 1, 'acceptable_alt_error': 1, 'levpa_id': 0, 'order_': 5, 'geometry': 'POINT (1445264.7481682934 1305032.7617393306)', 'index_right': 2161, 'HydroID': 11840273, 'LakeID': -999.0, 'geometry_ln': 'LINESTRING (1445194.409435884 1305039.2824222166, 1445194.409435884 1305039.2824222166, 1445194.409435884 1305039.2824222166, 1445194.409435884 13050 ... (8453 characters truncated) ... 46264.409435884 1304479.2824222166, 1446274.409435884 1304479.2824222166, 1446284.409435884 1304479.2824222166, 1446294.409435884 1304469.2824222166)', 'geometry_snapped': 'POINT (1445264.409435884 1305019.2824222166)', 'snap_distance': 13.483572579424449, 'dem_elevation': 24.648386, 'dem_adj_elevation': 24.552267, 'fim_version': '4.3.11.0'}, {'wrds_timestamp': 'USGS data: USGS NWIS - Last updated: 2022-11-28 22:16:23 UTC', 'nrldb_timestamp': 'NWS data: NRLDB - Last updated: 2021-11-15 22:57:38 UTC', 'nwis_timestamp': 'USGS data: USGS NWIS - Last updated: 2022-11-28 22:16:23 UTC', 'metadata_sources': \"['NWS data: NRLDB - Last updated: 2021-11-15 22:57:38 UTC', 'USACE data: USACE - Last updated: 2021-09-24 17:19:28 UTC', 'USGS data: USGS NWIS - Last updated: 2022-11-28 22:16:23 UTC']\", 'nws_lid': 'TBGS1', 'location_id': 21355015, 'feature_id': 9095360, 'goes_id': None, 'env_can_gage_id': None, 'nws_data_name': 'Tearcoat Branch at I-95 near Manning, SC', 'nws_data_wfo': 'CAE', 'nws_data_rfc': 'SERFC', 'nws_data_geo_rfc': 'SERFC', 'nws_data_latitude': 33.813, 'nws_data_longitude': -80.1516, 'nws_data_map_link': 'https://maps.google.com/maps?t=k&q=loc:33.813+-80.1516', 'nws_data_horizontal_datum_name': 'NAD83', 'nws_data_state': 'South Carolina', 'nws_data_county': 'Clarendon', 'nws_data_county_code': 45027, 'nws_data_huc': '03040205', 'nws_data_hsa': 'CAE', 'nws_data_zero_datum': 68.2, 'nws_data_vertical_datum_name': 'NAVD88', 'nws_data_rfc_forecast_point': 0, 'nws_data_rfc_defined_fcst_point': 0, 'nws_data_riverpoint': 1, 'usgs_data_name': 'TEARCOAT BRANCH AT I-95 NEAR MANNING, SC', 'usgs_data_geo_rfc': 'SERFC', 'usgs_data_latitude': 33.81302778, 'usgs_data_longitude': -80.1516111, 'usgs_data_map_link': 'https://maps.google.com/maps?t=k&q=loc:33.81302778+-80.1516111', 'usgs_data_coord_accuracy_code': 'S', 'usgs_data_latlon_datum_name': 'NAD83', 'usgs_data_coord_method_code': 'M', 'usgs_data_state': 'South Carolina', 'usgs_data_huc': 3040205, 'usgs_data_site_type': 'ST', 'usgs_data_altitude': 68.2, 'usgs_data_alt_accuracy_code': 0.04, 'usgs_data_alt_datum_code': 'NAVD88', 'usgs_data_alt_method_code': 'X', 'usgs_data_drainage_area': 27.1, 'usgs_data_drainage_area_units': 'square miles', 'usgs_data_contrib_drainage_area': None, 'usgs_data_active': 1, 'usgs_data_gages_ii_reference': 0, 'nwm_feature_data_downstream_feature_id': 9093512.0, 'nwm_feature_data_latitude': 33.809059999999995, 'nwm_feature_data_longitude': -80.13763, 'nwm_feature_data_altitude': 25.94, 'nwm_feature_data_stream_length': 3999.0, 'nwm_feature_data_stream_order': 3.0, 'nwm_feature_data_mannings_roughness': 0.055, 'nwm_feature_data_slope': 0.0005099999999999999, 'nwm_feature_data_channel_side_slope': 0.4095362, 'nwm_feature_data_nhd_waterbody_comid': None, 'env_can_gage_data_name': None, 'env_can_gage_data_latitude': None, 'env_can_gage_data_longitude': None, 'env_can_gage_data_map_link': None, 'env_can_gage_data_drainage_area': None, 'env_can_gage_data_contrib_drainage_area': None, 'env_can_gage_data_water_course': None, 'nws_preferred_name': 'Tearcoat Branch at I-95 near Manning, SC', 'nws_preferred_latitude': 33.813, 'nws_preferred_longitude': -80.1516, 'nws_preferred_latlon_datum_name': 'NAD83', 'nws_preferred_state': 'South Carolina', 'nws_preferred_huc': '03040205', 'usgs_preferred_name': 'TEARCOAT BRANCH AT I-95 NEAR MANNING, SC', 'usgs_preferred_latitude': 33.81302778, 'usgs_preferred_longitude': -80.1516111, 'usgs_preferred_latlon_datum_name': 'NAD83', 'usgs_preferred_state': 'South Carolina', 'usgs_preferred_huc': 3040205, 'crosswalk_datasets_location_nwm_crosswalk_dataset_location_nwm_crosswalk_dataset_id': 1.2, 'crosswalk_datasets_location_nwm_crosswalk_dataset_name': 'Location NWM Crosswalk v1.2', 'crosswalk_datasets_location_nwm_crosswalk_dataset_description': 'Created 20220831.  Source:\\n1) NWM Routelink File v2.2-corrected\\n2) NHDPlus v2.1+\\n3) GID', 'crosswalk_datasets_nws_usgs_crosswalk_dataset_nws_usgs_crosswalk_dataset_id': 2.0, 'crosswalk_datasets_nws_usgs_crosswalk_dataset_name': 'NWS Station to USGS Gages 2.0', 'crosswalk_datasets_nws_usgs_crosswalk_dataset_description': 'Created 20210809.  Authoritative 2.0 dataset mapping NWS Stations to USGS Gages.  Source 1) AHPS CMS Report  2) HADS dataset  3) GID', 'assigned_crs': 'EPSG:4269', 'HUC8': 3040205, 'name': 'Black', 'states': 'SC', 'curve': 'no', 'mainstem': 'no', 'acceptable_codes': 1, 'acceptable_alt_error': 1, 'levpa_id': 0, 'order_': 3, 'geometry': 'POINT (1449941.534551052 1315265.392210678)', 'index_right': 1628, 'HydroID': 11840525, 'LakeID': -999.0, 'geometry_ln': 'LINESTRING (1449444.409435884 1315179.2824222166, 1449454.409435884 1315179.2824222166, 1449464.409435884 1315179.2824222166, 1449474.409435884 13151 ... (4767 characters truncated) ... 50734.409435884 1315309.2824222166, 1450744.409435884 1315309.2824222166, 1450754.409435884 1315309.2824222166, 1450764.409435884 1315309.2824222166)', 'geometry_snapped': 'POINT (1449941.534551052 1315259.2824222166)', 'snap_distance': 6.1097884613554925, 'dem_elevation': 25.317926, 'dem_adj_elevation': 25.279405999999998, 'fim_version': '4.3.11.0'})]\n",
      "(Background on this error at: http://sqlalche.me/e/13/9h9h)\n",
      "******************************************\n",
      "Processing 185 of 1000 on page 1\n",
      "Processing 186 of 1000 on page 1\n",
      "Processing 187 of 1000 on page 1\n",
      "Processing 188 of 1000 on page 1\n",
      "Processing 189 of 1000 on page 1\n",
      "Processing 190 of 1000 on page 1\n",
      "Processing 191 of 1000 on page 1\n",
      "Processing 192 of 1000 on page 1\n",
      "Processing 193 of 1000 on page 1\n",
      "Processing 194 of 1000 on page 1\n",
      "Processing 195 of 1000 on page 1\n",
      "Processing 196 of 1000 on page 1\n",
      "Processing 197 of 1000 on page 1\n",
      "Processing 198 of 1000 on page 1\n",
      "Processing 199 of 1000 on page 1\n",
      "Processing 200 of 1000 on page 1\n",
      "Processing 201 of 1000 on page 1\n",
      "Processing 202 of 1000 on page 1\n",
      "Processing 203 of 1000 on page 1\n",
      "Processing 204 of 1000 on page 1\n",
      "Processing 205 of 1000 on page 1\n",
      "Processing 206 of 1000 on page 1\n",
      "Processing 207 of 1000 on page 1\n",
      "Processing 208 of 1000 on page 1\n",
      "Processing 209 of 1000 on page 1\n",
      "Processing 210 of 1000 on page 1\n",
      "Processing 211 of 1000 on page 1\n",
      "Processing 212 of 1000 on page 1\n",
      "Processing 213 of 1000 on page 1\n",
      "Processing 214 of 1000 on page 1\n",
      "Processing 215 of 1000 on page 1\n",
      "Processing 216 of 1000 on page 1\n",
      "Processing 217 of 1000 on page 1\n",
      "Processing 218 of 1000 on page 1\n",
      "Processing 219 of 1000 on page 1\n",
      "Processing 220 of 1000 on page 1\n",
      "Processing 221 of 1000 on page 1\n",
      "Processing 222 of 1000 on page 1\n",
      "Processing 223 of 1000 on page 1\n",
      "Processing 224 of 1000 on page 1\n",
      "Processing 225 of 1000 on page 1\n",
      "Processing 226 of 1000 on page 1\n",
      "Processing 227 of 1000 on page 1\n",
      "Processing 228 of 1000 on page 1\n",
      "Processing 229 of 1000 on page 1\n",
      "Processing 230 of 1000 on page 1\n",
      "Processing 231 of 1000 on page 1\n",
      "Processing 232 of 1000 on page 1\n",
      "Processing 233 of 1000 on page 1\n",
      "Processing 234 of 1000 on page 1\n",
      "Processing 235 of 1000 on page 1\n",
      "Processing 236 of 1000 on page 1\n",
      "Processing 237 of 1000 on page 1\n",
      "Processing 238 of 1000 on page 1\n",
      "Processing 239 of 1000 on page 1\n",
      "Processing 240 of 1000 on page 1\n",
      "Processing 241 of 1000 on page 1\n",
      "Processing 242 of 1000 on page 1\n",
      "Processing 243 of 1000 on page 1\n",
      "Processing 244 of 1000 on page 1\n",
      "Processing 245 of 1000 on page 1\n",
      "Processing 246 of 1000 on page 1\n",
      "Processing 247 of 1000 on page 1\n",
      "Processing 248 of 1000 on page 1\n",
      "Processing 249 of 1000 on page 1\n",
      "Processing 250 of 1000 on page 1\n",
      "Processing 251 of 1000 on page 1\n",
      "Processing 252 of 1000 on page 1\n",
      "Processing 253 of 1000 on page 1\n",
      "Processing 254 of 1000 on page 1\n",
      "Processing 255 of 1000 on page 1\n",
      "Processing 256 of 1000 on page 1\n",
      "Processing 257 of 1000 on page 1\n",
      "Processing 258 of 1000 on page 1\n",
      "Processing 259 of 1000 on page 1\n",
      "Processing 260 of 1000 on page 1\n",
      "Processing 261 of 1000 on page 1\n",
      "Processing 262 of 1000 on page 1\n",
      "Processing 263 of 1000 on page 1\n",
      "Processing 264 of 1000 on page 1\n",
      "Processing 265 of 1000 on page 1\n",
      "Processing 266 of 1000 on page 1\n",
      "Processing 267 of 1000 on page 1\n",
      "Processing 268 of 1000 on page 1\n",
      "Processing 269 of 1000 on page 1\n",
      "Processing 270 of 1000 on page 1\n",
      "Processing 271 of 1000 on page 1\n",
      "Processing 272 of 1000 on page 1\n",
      "Processing 273 of 1000 on page 1\n",
      "Processing 274 of 1000 on page 1\n",
      "Processing 275 of 1000 on page 1\n",
      "Processing 276 of 1000 on page 1\n",
      "Processing 277 of 1000 on page 1\n",
      "Processing 278 of 1000 on page 1\n",
      "Processing 279 of 1000 on page 1\n",
      "Processing 280 of 1000 on page 1\n",
      "Processing 281 of 1000 on page 1\n",
      "Processing 282 of 1000 on page 1\n",
      "Processing 283 of 1000 on page 1\n",
      "Processing 284 of 1000 on page 1\n",
      "Processing 285 of 1000 on page 1\n",
      "Processing 286 of 1000 on page 1\n",
      "Processing 287 of 1000 on page 1\n",
      "Processing 288 of 1000 on page 1\n",
      "Processing 289 of 1000 on page 1\n",
      "Processing 290 of 1000 on page 1\n",
      "Processing 291 of 1000 on page 1\n",
      "Processing 292 of 1000 on page 1\n",
      "Processing 293 of 1000 on page 1\n",
      "Processing 294 of 1000 on page 1\n",
      "Processing 295 of 1000 on page 1\n",
      "Processing 296 of 1000 on page 1\n",
      "Processing 297 of 1000 on page 1\n",
      "Processing 298 of 1000 on page 1\n",
      "Processing 299 of 1000 on page 1\n",
      "Processing 300 of 1000 on page 1\n",
      "Processing 301 of 1000 on page 1\n",
      "Processing 302 of 1000 on page 1\n",
      "Processing 303 of 1000 on page 1\n",
      "Processing 304 of 1000 on page 1\n",
      "Processing 305 of 1000 on page 1\n",
      "Processing 306 of 1000 on page 1\n",
      "Processing 307 of 1000 on page 1\n",
      "Processing 308 of 1000 on page 1\n",
      "Processing 309 of 1000 on page 1\n",
      "Processing 310 of 1000 on page 1\n",
      "Processing 311 of 1000 on page 1\n",
      "Processing 312 of 1000 on page 1\n",
      "Processing 313 of 1000 on page 1\n",
      "Processing 314 of 1000 on page 1\n",
      "Processing 315 of 1000 on page 1\n",
      "Processing 316 of 1000 on page 1\n",
      "Processing 317 of 1000 on page 1\n",
      "Processing 318 of 1000 on page 1\n",
      "Processing 319 of 1000 on page 1\n",
      "Processing 320 of 1000 on page 1\n",
      "Processing 321 of 1000 on page 1\n",
      "Processing 322 of 1000 on page 1\n",
      "Processing 323 of 1000 on page 1\n",
      "Processing 324 of 1000 on page 1\n",
      "Processing 325 of 1000 on page 1\n",
      "Processing 326 of 1000 on page 1\n",
      "Processing 327 of 1000 on page 1\n",
      "Processing 328 of 1000 on page 1\n",
      "Processing 329 of 1000 on page 1\n",
      "Processing 330 of 1000 on page 1\n",
      "Processing 331 of 1000 on page 1\n",
      "Processing 332 of 1000 on page 1\n",
      "Processing 333 of 1000 on page 1\n",
      "Processing 334 of 1000 on page 1\n",
      "Processing 335 of 1000 on page 1\n",
      "Processing 336 of 1000 on page 1\n",
      "Processing 337 of 1000 on page 1\n",
      "Processing 338 of 1000 on page 1\n",
      "Processing 339 of 1000 on page 1\n",
      "Processing 340 of 1000 on page 1\n",
      "Processing 341 of 1000 on page 1\n",
      "Processing 342 of 1000 on page 1\n",
      "Processing 343 of 1000 on page 1\n",
      "Processing 344 of 1000 on page 1\n",
      "Processing 345 of 1000 on page 1\n",
      "Processing 346 of 1000 on page 1\n",
      "Processing 347 of 1000 on page 1\n",
      "Processing 348 of 1000 on page 1\n",
      "Processing 349 of 1000 on page 1\n",
      "Processing 350 of 1000 on page 1\n",
      "Processing 351 of 1000 on page 1\n",
      "Processing 352 of 1000 on page 1\n",
      "Processing 353 of 1000 on page 1\n",
      "Processing 354 of 1000 on page 1\n",
      "Processing 355 of 1000 on page 1\n",
      "Processing 356 of 1000 on page 1\n",
      "Processing 357 of 1000 on page 1\n",
      "Processing 358 of 1000 on page 1\n",
      "Processing 359 of 1000 on page 1\n",
      "Processing 360 of 1000 on page 1\n",
      "Processing 361 of 1000 on page 1\n",
      "Processing 362 of 1000 on page 1\n",
      "Processing 363 of 1000 on page 1\n",
      "Processing 364 of 1000 on page 1\n",
      "Processing 365 of 1000 on page 1\n",
      "Processing 366 of 1000 on page 1\n",
      "Processing 367 of 1000 on page 1\n",
      "Processing 368 of 1000 on page 1\n",
      "Processing 369 of 1000 on page 1\n",
      "Processing 370 of 1000 on page 1\n",
      "Processing 371 of 1000 on page 1\n",
      "Processing 372 of 1000 on page 1\n",
      "******************************************\n",
      "Error encountered on fim_4_3_11_0/hand_datasets/04030110/branches/0/usgs_elev_table.csv\n",
      "(psycopg2.DataError) invalid input syntax for type bigint: \"NONE\"\n",
      "LINE 1: ...11111111,  -87.21361111111099, 'UNK', 'Michigan', 'NONE', 'E...\n",
      "                                                             ^\n",
      "\n",
      "[SQL: INSERT INTO derived.usgs_elev_table (wrds_timestamp, nrldb_timestamp, nwis_timestamp, metadata_sources, nws_lid, location_id, feature_id, goes_id, env_can_gage_id, nws_data_name, nws_data_wfo, nws_data_rfc, nws_data_geo_rfc, nws_data_latitude, nws_data_longitude, nws_data_map_link, nws_data_horizontal_datum_name, nws_data_state, nws_data_county, nws_data_county_code, nws_data_huc, nws_data_hsa, nws_data_zero_datum, nws_data_vertical_datum_name, nws_data_rfc_forecast_point, nws_data_rfc_defined_fcst_point, nws_data_riverpoint, usgs_data_name, usgs_data_geo_rfc, usgs_data_latitude, usgs_data_longitude, usgs_data_map_link, usgs_data_coord_accuracy_code, usgs_data_latlon_datum_name, usgs_data_coord_method_code, usgs_data_state, usgs_data_huc, usgs_data_site_type, usgs_data_altitude, usgs_data_alt_accuracy_code, usgs_data_alt_datum_code, usgs_data_alt_method_code, usgs_data_drainage_area, usgs_data_drainage_area_units, usgs_data_contrib_drainage_area, usgs_data_active, usgs_data_gages_ii_reference, nwm_feature_data_downstream_feature_id, nwm_feature_data_latitude, nwm_feature_data_longitude, nwm_feature_data_altitude, nwm_feature_data_stream_length, nwm_feature_data_stream_order, nwm_feature_data_mannings_roughness, nwm_feature_data_slope, nwm_feature_data_channel_side_slope, nwm_feature_data_nhd_waterbody_comid, env_can_gage_data_name, env_can_gage_data_latitude, env_can_gage_data_longitude, env_can_gage_data_map_link, env_can_gage_data_drainage_area, env_can_gage_data_contrib_drainage_area, env_can_gage_data_water_course, nws_preferred_name, nws_preferred_latitude, nws_preferred_longitude, nws_preferred_latlon_datum_name, nws_preferred_state, nws_preferred_huc, usgs_preferred_name, usgs_preferred_latitude, usgs_preferred_longitude, usgs_preferred_latlon_datum_name, usgs_preferred_state, usgs_preferred_huc, crosswalk_datasets_location_nwm_crosswalk_dataset_location_nwm_crosswalk_dataset_id, crosswalk_datasets_location_nwm_crosswalk_dataset_name, crosswalk_datasets_location_nwm_crosswalk_dataset_description, crosswalk_datasets_nws_usgs_crosswalk_dataset_nws_usgs_crosswalk_dataset_id, crosswalk_datasets_nws_usgs_crosswalk_dataset_name, crosswalk_datasets_nws_usgs_crosswalk_dataset_description, assigned_crs, \"HUC8\", name, states, curve, mainstem, acceptable_codes, acceptable_alt_error, levpa_id, order_, geometry, index_right, \"HydroID\", \"LakeID\", geometry_ln, geometry_snapped, snap_distance, dem_elevation, dem_adj_elevation, fim_version) VALUES (%(wrds_timestamp)s, %(nrldb_timestamp)s, %(nwis_timestamp)s, %(metadata_sources)s, %(nws_lid)s, %(location_id)s, %(feature_id)s, %(goes_id)s, %(env_can_gage_id)s, %(nws_data_name)s, %(nws_data_wfo)s, %(nws_data_rfc)s, %(nws_data_geo_rfc)s, %(nws_data_latitude)s, %(nws_data_longitude)s, %(nws_data_map_link)s, %(nws_data_horizontal_datum_name)s, %(nws_data_state)s, %(nws_data_county)s, %(nws_data_county_code)s, %(nws_data_huc)s, %(nws_data_hsa)s, %(nws_data_zero_datum)s, %(nws_data_vertical_datum_name)s, %(nws_data_rfc_forecast_point)s, %(nws_data_rfc_defined_fcst_point)s, %(nws_data_riverpoint)s, %(usgs_data_name)s, %(usgs_data_geo_rfc)s, %(usgs_data_latitude)s, %(usgs_data_longitude)s, %(usgs_data_map_link)s, %(usgs_data_coord_accuracy_code)s, %(usgs_data_latlon_datum_name)s, %(usgs_data_coord_method_code)s, %(usgs_data_state)s, %(usgs_data_huc)s, %(usgs_data_site_type)s, %(usgs_data_altitude)s, %(usgs_data_alt_accuracy_code)s, %(usgs_data_alt_datum_code)s, %(usgs_data_alt_method_code)s, %(usgs_data_drainage_area)s, %(usgs_data_drainage_area_units)s, %(usgs_data_contrib_drainage_area)s, %(usgs_data_active)s, %(usgs_data_gages_ii_reference)s, %(nwm_feature_data_downstream_feature_id)s, %(nwm_feature_data_latitude)s, %(nwm_feature_data_longitude)s, %(nwm_feature_data_altitude)s, %(nwm_feature_data_stream_length)s, %(nwm_feature_data_stream_order)s, %(nwm_feature_data_mannings_roughness)s, %(nwm_feature_data_slope)s, %(nwm_feature_data_channel_side_slope)s, %(nwm_feature_data_nhd_waterbody_comid)s, %(env_can_gage_data_name)s, %(env_can_gage_data_latitude)s, %(env_can_gage_data_longitude)s, %(env_can_gage_data_map_link)s, %(env_can_gage_data_drainage_area)s, %(env_can_gage_data_contrib_drainage_area)s, %(env_can_gage_data_water_course)s, %(nws_preferred_name)s, %(nws_preferred_latitude)s, %(nws_preferred_longitude)s, %(nws_preferred_latlon_datum_name)s, %(nws_preferred_state)s, %(nws_preferred_huc)s, %(usgs_preferred_name)s, %(usgs_preferred_latitude)s, %(usgs_preferred_longitude)s, %(usgs_preferred_latlon_datum_name)s, %(usgs_preferred_state)s, %(usgs_preferred_huc)s, %(crosswalk_datasets_location_nwm_crosswalk_dataset_location_nwm_crosswalk_dataset_id)s, %(crosswalk_datasets_location_nwm_crosswalk_dataset_name)s, %(crosswalk_datasets_location_nwm_crosswalk_dataset_description)s, %(crosswalk_datasets_nws_usgs_crosswalk_dataset_nws_usgs_crosswalk_dataset_id)s, %(crosswalk_datasets_nws_usgs_crosswalk_dataset_name)s, %(crosswalk_datasets_nws_usgs_crosswalk_dataset_description)s, %(assigned_crs)s, %(HUC8)s, %(name)s, %(states)s, %(curve)s, %(mainstem)s, %(acceptable_codes)s, %(acceptable_alt_error)s, %(levpa_id)s, %(order_)s, %(geometry)s, %(index_right)s, %(HydroID)s, %(LakeID)s, %(geometry_ln)s, %(geometry_snapped)s, %(snap_distance)s, %(dem_elevation)s, %(dem_adj_elevation)s, %(fim_version)s)]\n",
      "[parameters: ({'wrds_timestamp': 'USGS data: USGS NWIS - Last updated: 2022-11-28 22:16:23 UTC', 'nrldb_timestamp': 'NWS data: NRLDB - Last updated: 2021-11-15 22:57:38 UTC', 'nwis_timestamp': 'USGS data: USGS NWIS - Last updated: 2022-11-28 22:16:23 UTC', 'metadata_sources': \"['NWS data: NRLDB - Last updated: 2021-11-15 22:57:38 UTC', 'USACE data: USACE - Last updated: 2021-09-24 17:19:28 UTC', 'USGS data: USGS NWIS - Last updated: 2022-11-28 22:16:23 UTC']\", 'nws_lid': 'HUMM4', 'location_id': '04057800', 'feature_id': 11959338, 'goes_id': '176F143C', 'env_can_gage_id': None, 'nws_data_name': 'Humboldt', 'nws_data_wfo': 'MQT', 'nws_data_rfc': 'NCRFC', 'nws_data_geo_rfc': 'NCRFC', 'nws_data_latitude': 46.499166666667, 'nws_data_longitude': -87.886388888889, 'nws_data_map_link': 'https://maps.google.com/maps?t=k&q=loc:46.499166666667+-87.886388888889', 'nws_data_horizontal_datum_name': 'NAD 1927', 'nws_data_state': 'Michigan', 'nws_data_county': 'Marquette', 'nws_data_county_code': 26103.0, 'nws_data_huc': None, 'nws_data_hsa': 'MQT', 'nws_data_zero_datum': 1521.37, 'nws_data_vertical_datum_name': 'NAVD88', 'nws_data_rfc_forecast_point': 1.0, 'nws_data_rfc_defined_fcst_point': 1.0, 'nws_data_riverpoint': 1.0, 'usgs_data_name': 'MIDDLE BRANCH ESCANABA RIVER AT HUMBOLDT, MI', 'usgs_data_geo_rfc': 'NCRFC', 'usgs_data_latitude': 46.49910267, 'usgs_data_longitude': -87.8865238, 'usgs_data_map_link': 'https://maps.google.com/maps?t=k&q=loc:46.49910267+-87.8865238', 'usgs_data_coord_accuracy_code': 'S', 'usgs_data_latlon_datum_name': 'NAD83', 'usgs_data_coord_method_code': 'M', 'usgs_data_state': 'Michigan', 'usgs_data_huc': 4030110.0, 'usgs_data_site_type': 'ST', 'usgs_data_altitude': 1521.37, 'usgs_data_alt_accuracy_code': 0.01, 'usgs_data_alt_datum_code': 'NAVD88', 'usgs_data_alt_method_code': 'L', 'usgs_data_drainage_area': 45.7, 'usgs_data_drainage_area_units': 'square miles', 'usgs_data_contrib_drainage_area': None, 'usgs_data_active': 1.0, 'usgs_data_gages_ii_reference': 1.0, 'nwm_feature_data_downstream_feature_id': 11959342.0, 'nwm_feature_data_latitude': 46.495940000000004, 'nwm_feature_data_longitude': -87.86917, 'nwm_feature_data_altitude': 467.98, 'nwm_feature_data_stream_length': 3747.0, 'nwm_feature_data_stream_order': 3.0, 'nwm_feature_data_mannings_roughness': 0.055, 'nwm_feature_data_slope': 0.00123, 'nwm_feature_data_channel_side_slope': 0.38151090000000004, 'nwm_feature_data_nhd_waterbody_comid': None, 'env_can_gage_data_name': None, 'env_can_gage_data_latitude': None, 'env_can_gage_data_longitude': None, 'env_can_gage_data_map_link': None, 'env_can_gage_data_drainage_area': None, 'env_can_gage_data_contrib_drainage_area': None, 'env_can_gage_data_water_course': None, 'nws_preferred_name': 'Humboldt', 'nws_preferred_latitude': 46.499166666667, 'nws_preferred_longitude': -87.886388888889, 'nws_preferred_latlon_datum_name': 'NAD 1927', 'nws_preferred_state': 'Michigan', 'nws_preferred_huc': '04030110', 'usgs_preferred_name': 'MIDDLE BRANCH ESCANABA RIVER AT HUMBOLDT, MI', 'usgs_preferred_latitude': 46.49910267, 'usgs_preferred_longitude': -87.8865238, 'usgs_preferred_latlon_datum_name': 'NAD83', 'usgs_preferred_state': 'Michigan', 'usgs_preferred_huc': 4030110.0, 'crosswalk_datasets_location_nwm_crosswalk_dataset_location_nwm_crosswalk_dataset_id': 1.2, 'crosswalk_datasets_location_nwm_crosswalk_dataset_name': 'Location NWM Crosswalk v1.2', 'crosswalk_datasets_location_nwm_crosswalk_dataset_description': 'Created 20220831.  Source:\\n1) NWM Routelink File v2.2-corrected\\n2) NHDPlus v2.1+\\n3) GID', 'crosswalk_datasets_nws_usgs_crosswalk_dataset_nws_usgs_crosswalk_dataset_id': 2.0, 'crosswalk_datasets_nws_usgs_crosswalk_dataset_name': 'NWS Station to USGS Gages 2.0', 'crosswalk_datasets_nws_usgs_crosswalk_dataset_description': 'Created 20210809.  Authoritative 2.0 dataset mapping NWS Stations to USGS Gages.  Source 1) AHPS CMS Report  2) HADS dataset  3) GID', 'assigned_crs': 'EPSG:4269', 'HUC8': 4030110, 'name': 'Escanaba', 'states': 'MI', 'curve': 'yes', 'mainstem': 'yes', 'acceptable_codes': 1.0, 'acceptable_alt_error': 1.0, 'levpa_id': 0, 'order_': 3.0, 'geometry': 'POINT (623907.6149865735 2638867.044777158)', 'index_right': 79, 'HydroID': 13720230, 'LakeID': -999.0, 'geometry_ln': 'LINESTRING (623744.4094358841 2638999.2824222166, 623744.4094358841 2638999.2824222166, 623744.4094358841 2638999.2824222166, 623744.4094358841 26389 ... (8377 characters truncated) ... 4754.4094358841 2638749.2824222166, 624754.4094358841 2638759.2824222166, 624764.4094358841 2638769.2824222166, 624764.4094358841 2638779.2824222166)', 'geometry_snapped': 'POINT (623902.131033758 2638861.5608243425)', 'snap_distance': 7.755480447133856, 'dem_elevation': 466.24637, 'dem_adj_elevation': 464.69757000000004, 'fim_version': '4.3.11.0'}, {'wrds_timestamp': 'USGS data: USGS NWIS - Last updated: 2022-11-28 22:16:23 UTC', 'nrldb_timestamp': 'NWS data: NRLDB - Last updated: 2021-11-15 22:57:38 UTC', 'nwis_timestamp': 'USGS data: USGS NWIS - Last updated: 2022-11-28 22:16:23 UTC', 'metadata_sources': \"['NWS data: NRLDB - Last updated: 2021-11-15 22:57:38 UTC', 'USACE data: USACE - Last updated: 2021-09-24 17:19:28 UTC', 'USGS data: USGS NWIS - Last updated: 2022-11-28 22:16:23 UTC']\", 'nws_lid': 'GREM4', 'location_id': '04057811', 'feature_id': 11959926, 'goes_id': 'DD1E61A0', 'env_can_gage_id': None, 'nws_data_name': 'Greenwood Reservoir Near Greenwood', 'nws_data_wfo': 'MQT', 'nws_data_rfc': 'NCRFC', 'nws_data_geo_rfc': 'NCRFC', 'nws_data_latitude': 46.44222222222201, 'nws_data_longitude': -87.80055555555599, 'nws_data_map_link': 'https://maps.google.com/maps?t=k&q=loc:46.442222222222+-87.800555555556', 'nws_data_horizontal_datum_name': None, 'nws_data_state': 'Michigan', 'nws_data_county': 'Marquette', 'nws_data_county_code': 26103.0, 'nws_data_huc': None, 'nws_data_hsa': 'MQT', 'nws_data_zero_datum': None, 'nws_data_vertical_datum_name': 'NGVD 1929', 'nws_data_rfc_forecast_point': 0.0, 'nws_data_rfc_defined_fcst_point': 0.0, 'nws_data_riverpoint': 1.0, 'usgs_data_name': 'GREENWOOD RESERVOIR NEAR GREENWOOD, MI', 'usgs_data_geo_rfc': 'NCRFC', 'usgs_data_latitude': 46.4421599, 'usgs_data_longitude': -87.8006905, 'usgs_data_map_link': 'https://maps.google.com/maps?t=k&q=loc:46.4421599+-87.8006905', 'usgs_data_coord_accuracy_code': 'S', 'usgs_data_latlon_datum_name': 'NAD83', 'usgs_data_coord_method_code': 'M', 'usgs_data_state': 'Michigan', 'usgs_data_huc': 4030110.0, 'usgs_data_site_type': 'LK', 'usgs_data_altitude': 1400.14, 'usgs_data_alt_accuracy_code': 0.01, 'usgs_data_alt_datum_code': 'NAVD88', 'usgs_data_alt_method_code': 'R', 'usgs_data_drainage_area': 67.4, 'usgs_data_drainage_area_units': 'square miles', 'usgs_data_contrib_drainage_area': None, 'usgs_data_active': 1.0, 'usgs_data_gages_ii_reference': 0.0, 'nwm_feature_data_downstream_feature_id': 11959398.0, 'nwm_feature_data_latitude': 46.440090000000005, 'nwm_feature_data_longitude': -87.79165999999998, 'nwm_feature_data_altitude': 456.15, 'nwm_feature_data_stream_length': 1705.0, 'nwm_feature_data_stream_order': 3.0, 'nwm_feature_data_mannings_roughness': 0.055, 'nwm_feature_data_slope': 0.00713, 'nwm_feature_data_channel_side_slope': 0.36363779999999996, 'nwm_feature_data_nhd_waterbody_comid': None, 'env_can_gage_data_name': None, 'env_can_gage_data_latitude': None, 'env_can_gage_data_longitude': None, 'env_can_gage_data_map_link': None, 'env_can_gage_data_drainage_area': None, 'env_can_gage_data_contrib_drainage_area': None, 'env_can_gage_data_water_course': None, 'nws_preferred_name': 'Greenwood Reservoir Near Greenwood', 'nws_preferred_latitude': 46.44222222222201, 'nws_preferred_longitude': -87.80055555555599, 'nws_preferred_latlon_datum_name': 'NAD83', 'nws_preferred_state': 'Michigan', 'nws_preferred_huc': '04030110', 'usgs_preferred_name': 'GREENWOOD RESERVOIR NEAR GREENWOOD, MI', 'usgs_preferred_latitude': 46.4421599, 'usgs_preferred_longitude': -87.8006905, 'usgs_preferred_latlon_datum_name': 'NAD83', 'usgs_preferred_state': 'Michigan', 'usgs_preferred_huc': 4030110.0, 'crosswalk_datasets_location_nwm_crosswalk_dataset_location_nwm_crosswalk_dataset_id': 1.2, 'crosswalk_datasets_location_nwm_crosswalk_dataset_name': 'Location NWM Crosswalk v1.2', 'crosswalk_datasets_location_nwm_crosswalk_dataset_description': 'Created 20220831.  Source:\\n1) NWM Routelink File v2.2-corrected\\n2) NHDPlus v2.1+\\n3) GID', 'crosswalk_datasets_nws_usgs_crosswalk_dataset_nws_usgs_crosswalk_dataset_id': 2.0, 'crosswalk_datasets_nws_usgs_crosswalk_dataset_name': 'NWS Station to USGS Gages 2.0', 'crosswalk_datasets_nws_usgs_crosswalk_dataset_description': 'Created 20210809.  Authoritative 2.0 dataset mapping NWS Stations to USGS Gages.  Source 1) AHPS CMS Report  2) HADS dataset  3) GID', 'assigned_crs': 'EPSG:4269', 'HUC8': 4030110, 'name': 'Escanaba', 'states': 'MI', 'curve': 'no', 'mainstem': 'yes', 'acceptable_codes': 0.0, 'acceptable_alt_error': 1.0, 'levpa_id': 0, 'order_': 3.0, 'geometry': 'POINT (631035.5831988874 2633145.3364276816)', 'index_right': 157, 'HydroID': 13720182, 'LakeID': -999.0, 'geometry_ln': 'LINESTRING (630984.4094358841 2633169.2824222166, 630984.4094358841 2633169.2824222166, 630984.4094358841 2633169.2824222166, 630984.4094358841 26331 ... (8605 characters truncated) ... 2094.4094358841 2632859.2824222166, 632094.4094358841 2632849.2824222166, 632094.4094358841 2632839.2824222166, 632094.4094358841 2632829.2824222166)', 'geometry_snapped': 'POINT (631035.5831988874 2633159.2824222166)', 'snap_distance': 13.945994534995409, 'dem_elevation': 451.7148, 'dem_adj_elevation': 451.0766, 'fim_version': '4.3.11.0'}, {'wrds_timestamp': 'USGS data: USGS NWIS - Last updated: 2022-11-28 22:16:23 UTC', 'nrldb_timestamp': 'NWS data: NRLDB - Last updated: 2021-11-15 22:57:38 UTC', 'nwis_timestamp': 'USGS data: USGS NWIS - Last updated: 2022-11-28 22:16:23 UTC', 'metadata_sources': \"['NWS data: NRLDB - Last updated: 2021-11-15 22:57:38 UTC', 'USACE data: USACE - Last updated: 2021-09-24 17:19:28 UTC', 'USGS data: USGS NWIS - Last updated: 2022-11-28 22:16:23 UTC']\", 'nws_lid': 'GWDM4', 'location_id': '04057813', 'feature_id': 11959766, 'goes_id': 'DD42F11C', 'env_can_gage_id': None, 'nws_data_name': 'Greenwood Diversion Near Greenwood', 'nws_data_wfo': 'MQT', 'nws_data_rfc': 'NCRFC', 'nws_data_geo_rfc': 'NCRFC', 'nws_data_latitude': 46.434444444444, 'nws_data_longitude': -87.769444444444, 'nws_data_map_link': 'https://maps.google.com/maps?t=k&q=loc:46.434444444444+-87.769444444444', 'nws_data_horizontal_datum_name': None, 'nws_data_state': 'Michigan', 'nws_data_county': 'Marquette', 'nws_data_county_code': 26103.0, 'nws_data_huc': None, 'nws_data_hsa': 'MQT', 'nws_data_zero_datum': 1454.72, 'nws_data_vertical_datum_name': 'NAVD88', 'nws_data_rfc_forecast_point': 0.0, 'nws_data_rfc_defined_fcst_point': 0.0, 'nws_data_riverpoint': 1.0, 'usgs_data_name': 'GREENWOOD DIVERSION NEAR GREENWOOD, MI', 'usgs_data_geo_rfc': 'NCRFC', 'usgs_data_latitude': 46.43438238, 'usgs_data_longitude': -87.7695795, 'usgs_data_map_link': 'https://maps.google.com/maps?t=k&q=loc:46.43438238+-87.7695795', 'usgs_data_coord_accuracy_code': 'S', 'usgs_data_latlon_datum_name': 'NAD83', 'usgs_data_coord_method_code': 'M', 'usgs_data_state': 'Michigan', 'usgs_data_huc': 4030110.0, 'usgs_data_site_type': 'ST', 'usgs_data_altitude': 1454.72, 'usgs_data_alt_accuracy_code': 0.01, 'usgs_data_alt_datum_code': 'NAVD88', 'usgs_data_alt_method_code': 'R', 'usgs_data_drainage_area': None, 'usgs_data_drainage_area_units': 'square miles', 'usgs_data_contrib_drainage_area': None, 'usgs_data_active': 1.0, 'usgs_data_gages_ii_reference': 0.0, 'nwm_feature_data_downstream_feature_id': 11959388.0, 'nwm_feature_data_latitude': 46.43407, 'nwm_feature_data_longitude': -87.76618999999998, 'nwm_feature_data_altitude': 457.42, 'nwm_feature_data_stream_length': 3197.0, 'nwm_feature_data_stream_order': 1.0, 'nwm_feature_data_mannings_roughness': 0.06, 'nwm_feature_data_slope': 0.01095, 'nwm_feature_data_channel_side_slope': 0.581558, 'nwm_feature_data_nhd_waterbody_comid': None, 'env_can_gage_data_name': None, 'env_can_gage_data_latitude': None, 'env_can_gage_data_longitude': None, 'env_can_gage_data_map_link': None, 'env_can_gage_data_drainage_area': None, 'env_can_gage_data_contrib_drainage_area': None, 'env_can_gage_data_water_course': None, 'nws_preferred_name': 'Greenwood Diversion Near Greenwood', 'nws_preferred_latitude': 46.434444444444, 'nws_preferred_longitude': -87.769444444444, 'nws_preferred_latlon_datum_name': 'NAD83', 'nws_preferred_state': 'Michigan', 'nws_preferred_huc': '04030110', 'usgs_preferred_name': 'GREENWOOD DIVERSION NEAR GREENWOOD, MI', 'usgs_preferred_latitude': 46.43438238, 'usgs_preferred_longitude': -87.7695795, 'usgs_preferred_latlon_datum_name': 'NAD83', 'usgs_preferred_state': 'Michigan', 'usgs_preferred_huc': 4030110.0, 'crosswalk_datasets_location_nwm_crosswalk_dataset_location_nwm_crosswalk_dataset_id': 1.2, 'crosswalk_datasets_location_nwm_crosswalk_dataset_name': 'Location NWM Crosswalk v1.2', 'crosswalk_datasets_location_nwm_crosswalk_dataset_description': 'Created 20220831.  Source:\\n1) NWM Routelink File v2.2-corrected\\n2) NHDPlus v2.1+\\n3) GID', 'crosswalk_datasets_nws_usgs_crosswalk_dataset_nws_usgs_crosswalk_dataset_id': 2.0, 'crosswalk_datasets_nws_usgs_crosswalk_dataset_name': 'NWS Station to USGS Gages 2.0', 'crosswalk_datasets_nws_usgs_crosswalk_dataset_description': 'Created 20210809.  Authoritative 2.0 dataset mapping NWS Stations to USGS Gages.  Source 1) AHPS CMS Report  2) HADS dataset  3) GID', 'assigned_crs': 'EPSG:4269', 'HUC8': 4030110, 'name': 'Escanaba', 'states': 'MI', 'curve': 'yes', 'mainstem': 'no', 'acceptable_codes': 1.0, 'acceptable_alt_error': 1.0, 'levpa_id': 0, 'order_': None, 'geometry': 'POINT (633498.5460647431 2632493.3790095733)', 'index_right': 176, 'HydroID': 13720748, 'LakeID': -999.0, 'geometry_ln': 'LINESTRING (633404.4094358841 2632809.2824222166, 633404.4094358841 2632809.2824222166, 633404.4094358841 2632809.2824222166, 633404.4094358841 26328 ... (6857 characters truncated) ... 4084.4094358841 2632189.2824222166, 634094.4094358841 2632189.2824222166, 634104.4094358841 2632179.2824222166, 634114.4094358841 2632169.2824222166)', 'geometry_snapped': 'POINT (633514.4094358841 2632549.2824222166)', 'snap_distance': 58.110567792192214, 'dem_elevation': 438.7065, 'dem_adj_elevation': 438.3779, 'fim_version': '4.3.11.0'}, {'wrds_timestamp': 'USGS data: USGS NWIS - Last updated: 2022-11-28 22:16:23 UTC', 'nrldb_timestamp': 'NWS data: NRLDB - Last updated: 2021-11-15 22:57:38 UTC', 'nwis_timestamp': 'USGS data: USGS NWIS - Last updated: 2022-11-28 22:16:23 UTC', 'metadata_sources': \"['NWS data: NRLDB - Last updated: 2021-11-15 22:57:38 UTC', 'USACE data: USACE - Last updated: 2021-09-24 17:19:28 UTC', 'USGS data: USGS NWIS - Last updated: 2022-11-28 22:16:23 UTC']\", 'nws_lid': 'GWRM4', 'location_id': '04057814', 'feature_id': 11959396, 'goes_id': 'DD1E543A', 'env_can_gage_id': None, 'nws_data_name': 'Greenwood Release Near Greenwood', 'nws_data_wfo': 'MQT', 'nws_data_rfc': 'NCRFC', 'nws_data_geo_rfc': 'NCRFC', 'nws_data_latitude': 46.439444444444, 'nws_data_longitude': -87.79777777777798, 'nws_data_map_link': 'https://maps.google.com/maps?t=k&q=loc:46.439444444444+-87.797777777778', 'nws_data_horizontal_datum_name': None, 'nws_data_state': 'Michigan', 'nws_data_county': 'Marquette', 'nws_data_county_code': 26103.0, 'nws_data_huc': None, 'nws_data_hsa': 'MQT', 'nws_data_zero_datum': 1473.77, 'nws_data_vertical_datum_name': 'NGVD 1929', 'nws_data_rfc_forecast_point': 0.0, 'nws_data_rfc_defined_fcst_point': 0.0, 'nws_data_riverpoint': 1.0, 'usgs_data_name': 'GREENWOOD RELEASE NEAR GREENWOOD, MI', 'usgs_data_geo_rfc': 'NCRFC', 'usgs_data_latitude': 46.439382200000004, 'usgs_data_longitude': -87.7979128, 'usgs_data_map_link': 'https://maps.google.com/maps?t=k&q=loc:46.4393822+-87.7979128', 'usgs_data_coord_accuracy_code': 'S', 'usgs_data_latlon_datum_name': 'NAD83', 'usgs_data_coord_method_code': 'M', 'usgs_data_state': 'Michigan', 'usgs_data_huc': 4030110.0, 'usgs_data_site_type': 'ST', 'usgs_data_altitude': 1473.91, 'usgs_data_alt_accuracy_code': 0.01, 'usgs_data_alt_datum_code': 'NAVD88', 'usgs_data_alt_method_code': 'R', 'usgs_data_drainage_area': 67.4, 'usgs_data_drainage_area_units': 'square miles', 'usgs_data_contrib_drainage_area': None, 'usgs_data_active': 1.0, 'usgs_data_gages_ii_reference': 0.0, 'nwm_feature_data_downstream_feature_id': 11959400.0, 'nwm_feature_data_latitude': 46.43051, 'nwm_feature_data_longitude': -87.81014999999998, 'nwm_feature_data_altitude': 450.91, 'nwm_feature_data_stream_length': 4221.0, 'nwm_feature_data_stream_order': 1.0, 'nwm_feature_data_mannings_roughness': 0.06, 'nwm_feature_data_slope': 0.0046700000000000005, 'nwm_feature_data_channel_side_slope': 0.6708177, 'nwm_feature_data_nhd_waterbody_comid': None, 'env_can_gage_data_name': None, 'env_can_gage_data_latitude': None, 'env_can_gage_data_longitude': None, 'env_can_gage_data_map_link': None, 'env_can_gage_data_drainage_area': None, 'env_can_gage_data_contrib_drainage_area': None, 'env_can_gage_data_water_course': None, 'nws_preferred_name': 'Greenwood Release Near Greenwood', 'nws_preferred_latitude': 46.439444444444, 'nws_preferred_longitude': -87.797777777778, 'nws_preferred_latlon_datum_name': 'NAD83', 'nws_preferred_state': 'Michigan', 'nws_preferred_huc': '04030110', 'usgs_preferred_name': 'GREENWOOD RELEASE NEAR GREENWOOD, MI', 'usgs_preferred_latitude': 46.439382200000004, 'usgs_preferred_longitude': -87.7979128, 'usgs_preferred_latlon_datum_name': 'NAD83', 'usgs_preferred_state': 'Michigan', 'usgs_preferred_huc': 4030110.0, 'crosswalk_datasets_location_nwm_crosswalk_dataset_location_nwm_crosswalk_dataset_id': 1.2, 'crosswalk_datasets_location_nwm_crosswalk_dataset_name': 'Location NWM Crosswalk v1.2', 'crosswalk_datasets_location_nwm_crosswalk_dataset_description': 'Created 20220831.  Source:\\n1) NWM Routelink File v2.2-corrected\\n2) NHDPlus v2.1+\\n3) GID', 'crosswalk_datasets_nws_usgs_crosswalk_dataset_nws_usgs_crosswalk_dataset_id': 2.0, 'crosswalk_datasets_nws_usgs_crosswalk_dataset_name': 'NWS Station to USGS Gages 2.0', 'crosswalk_datasets_nws_usgs_crosswalk_dataset_description': 'Created 20210809.  Authoritative 2.0 dataset mapping NWS Stations to USGS Gages.  Source 1) AHPS CMS Report  2) HADS dataset  3) GID', 'assigned_crs': 'EPSG:4269', 'HUC8': 4030110, 'name': 'Escanaba', 'states': 'MI', 'curve': 'yes', 'mainstem': 'no', 'acceptable_codes': 1.0, 'acceptable_alt_error': 1.0, 'levpa_id': 0, 'order_': None, 'geometry': 'POINT (631275.3720251985 2632857.015854334)', 'index_right': 165, 'HydroID': 13720741, 'LakeID': -999.0, 'geometry_ln': 'LINESTRING (631364.4094358841 2632889.2824222166, 631354.4094358841 2632879.2824222166, 631344.4094358841 2632869.2824222166, 631334.4094358841 26328 ... (4463 characters truncated) ... 0294.4094358841 2632329.2824222166, 630284.4094358841 2632319.2824222166, 630274.4094358841 2632309.2824222166, 630264.4094358841 2632299.2824222166)', 'geometry_snapped': 'POINT (631284.4094358841 2632839.2824222166)', 'snap_distance': 19.90350236921465, 'dem_elevation': 448.17537999999996, 'dem_adj_elevation': 448.04065, 'fim_version': '4.3.11.0'}, {'wrds_timestamp': 'USGS data: USGS NWIS - Last updated: 2022-11-28 22:16:23 UTC', 'nrldb_timestamp': 'NWS data: NRLDB - Last updated: 2021-11-15 22:57:38 UTC', 'nwis_timestamp': 'USGS data: USGS NWIS - Last updated: 2022-11-28 22:16:23 UTC', 'metadata_sources': \"['NWS data: NRLDB - Last updated: 2021-11-15 22:57:38 UTC', 'USACE data: USACE - Last updated: 2021-09-24 17:19:28 UTC', 'USGS data: USGS NWIS - Last updated: 2022-11-28 22:16:23 UTC']\", 'nws_lid': 'PRNM4', 'location_id': '04058100', 'feature_id': 11959788, 'goes_id': 'D11AD25E', 'env_can_gage_id': None, 'nws_data_name': 'Princeton', 'nws_data_wfo': 'MQT', 'nws_data_rfc': 'NCRFC', 'nws_data_geo_rfc': 'NCRFC', 'nws_data_latitude': 46.31722222222201, 'nws_data_longitude': -87.501944444444, 'nws_data_map_link': 'https://maps.google.com/maps?t=k&q=loc:46.317222222222+-87.501944444444', 'nws_data_horizontal_datum_name': None, 'nws_data_state': 'Michigan', 'nws_data_county': 'Marquette', 'nws_data_county_code': 26103.0, 'nws_data_huc': None, 'nws_data_hsa': 'MQT', 'nws_data_zero_datum': 1103.02, 'nws_data_vertical_datum_name': 'NAVD88', 'nws_data_rfc_forecast_point': 0.0, 'nws_data_rfc_defined_fcst_point': 0.0, 'nws_data_riverpoint': 1.0, 'usgs_data_name': 'MIDDLE BRANCH ESCANABA RIVER NR PRINCETON, MI', 'usgs_data_geo_rfc': 'NCRFC', 'usgs_data_latitude': 46.317164899999995, 'usgs_data_longitude': -87.5020817, 'usgs_data_map_link': 'https://maps.google.com/maps?t=k&q=loc:46.3171649+-87.5020817', 'usgs_data_coord_accuracy_code': 'S', 'usgs_data_latlon_datum_name': 'NAD83', 'usgs_data_coord_method_code': 'M', 'usgs_data_state': 'Michigan', 'usgs_data_huc': 4030110.0, 'usgs_data_site_type': 'ST', 'usgs_data_altitude': 1103.02, 'usgs_data_alt_accuracy_code': 0.01, 'usgs_data_alt_datum_code': 'NAVD88', 'usgs_data_alt_method_code': 'L', 'usgs_data_drainage_area': 210.0, 'usgs_data_drainage_area_units': 'square miles', 'usgs_data_contrib_drainage_area': None, 'usgs_data_active': 1.0, 'usgs_data_gages_ii_reference': 0.0, 'nwm_feature_data_downstream_feature_id': 11959560.0, 'nwm_feature_data_latitude': 46.31877, 'nwm_feature_data_longitude': -87.5061, 'nwm_feature_data_altitude': 355.78, 'nwm_feature_data_stream_length': 1832.0, 'nwm_feature_data_stream_order': 4.0, 'nwm_feature_data_mannings_roughness': 0.055, 'nwm_feature_data_slope': 0.01181, 'nwm_feature_data_channel_side_slope': 0.3064173, 'nwm_feature_data_nhd_waterbody_comid': None, 'env_can_gage_data_name': None, 'env_can_gage_data_latitude': None, 'env_can_gage_data_longitude': None, 'env_can_gage_data_map_link': None, 'env_can_gage_data_drainage_area': None, 'env_can_gage_data_contrib_drainage_area': None, 'env_can_gage_data_water_course': None, 'nws_preferred_name': 'Princeton', 'nws_preferred_latitude': 46.31722222222201, 'nws_preferred_longitude': -87.501944444444, 'nws_preferred_latlon_datum_name': 'NAD83', 'nws_preferred_state': 'Michigan', 'nws_preferred_huc': '04030110', 'usgs_preferred_name': 'MIDDLE BRANCH ESCANABA RIVER NR PRINCETON, MI', 'usgs_preferred_latitude': 46.317164899999995, 'usgs_preferred_longitude': -87.5020817, 'usgs_preferred_latlon_datum_name': 'NAD83', 'usgs_preferred_state': 'Michigan', 'usgs_preferred_huc': 4030110.0, 'crosswalk_datasets_location_nwm_crosswalk_dataset_location_nwm_crosswalk_dataset_id': 1.2, 'crosswalk_datasets_location_nwm_crosswalk_dataset_name': 'Location NWM Crosswalk v1.2', 'crosswalk_datasets_location_nwm_crosswalk_dataset_description': 'Created 20220831.  Source:\\n1) NWM Routelink File v2.2-corrected\\n2) NHDPlus v2.1+\\n3) GID', 'crosswalk_datasets_nws_usgs_crosswalk_dataset_nws_usgs_crosswalk_dataset_id': 2.0, 'crosswalk_datasets_nws_usgs_crosswalk_dataset_name': 'NWS Station to USGS Gages 2.0', 'crosswalk_datasets_nws_usgs_crosswalk_dataset_description': 'Created 20210809.  Authoritative 2.0 dataset mapping NWS Stations to USGS Gages.  Source 1) AHPS CMS Report  2) HADS dataset  3) GID', 'assigned_crs': 'EPSG:4269', 'HUC8': 4030110, 'name': 'Escanaba', 'states': 'MI', 'curve': 'yes', 'mainstem': 'yes', 'acceptable_codes': 1.0, 'acceptable_alt_error': 1.0, 'levpa_id': 0, 'order_': 4.0, 'geometry': 'POINT (655194.5015902977 2621361.047419369)', 'index_right': 376, 'HydroID': 13720114, 'LakeID': -999.0, 'geometry_ln': 'LINESTRING (654524.4094358841 2621509.2824222166, 654524.4094358841 2621509.2824222166, 654524.4094358841 2621509.2824222166, 654524.4094358841 26215 ... (8453 characters truncated) ... 5644.4094358841 2621489.2824222166, 655654.4094358841 2621479.2824222166, 655664.4094358841 2621479.2824222166, 655674.4094358841 2621469.2824222166)', 'geometry_snapped': 'POINT (655224.4094358841 2621399.2824222166)', 'snap_distance': 48.542709755029676, 'dem_elevation': 336.67184, 'dem_adj_elevation': 336.67184, 'fim_version': '4.3.11.0'}, {'wrds_timestamp': 'USGS data: USGS NWIS - Last updated: 2022-11-28 22:16:23 UTC', 'nrldb_timestamp': 'NWS data: NRLDB - Last updated: 2021-11-15 22:57:38 UTC', 'nwis_timestamp': 'USGS data: USGS NWIS - Last updated: 2022-11-28 22:16:23 UTC', 'metadata_sources': \"['NWS data: NRLDB - Last updated: 2021-11-15 22:57:38 UTC', 'USACE data: USACE - Last updated: 2021-09-24 17:19:28 UTC', 'USGS data: USGS NWIS - Last updated: 2022-11-28 22:16:23 UTC']\", 'nws_lid': 'SZRM4', 'location_id': '04058190', 'feature_id': 11959934, 'goes_id': '17BD64A2', 'env_can_gage_id': None, 'nws_data_name': 'Schweitzer Reservoir', 'nws_data_wfo': 'MQT', 'nws_data_rfc': 'NCRFC', 'nws_data_geo_rfc': 'NCRFC', 'nws_data_latitude': 46.416666666667005, 'nws_data_longitude': -87.646666666667, 'nws_data_map_link': 'https://maps.google.com/maps?t=k&q=loc:46.416666666667+-87.646666666667', 'nws_data_horizontal_datum_name': None, 'nws_data_state': 'Michigan', 'nws_data_county': 'Marquette', 'nws_data_county_code': 26103.0, 'nws_data_huc': None, 'nws_data_hsa': 'MQT', 'nws_data_zero_datum': 1300.16, 'nws_data_vertical_datum_name': 'NAVD88', 'nws_data_rfc_forecast_point': 0.0, 'nws_data_rfc_defined_fcst_point': 0.0, 'nws_data_riverpoint': 1.0, 'usgs_data_name': 'SCHWEITZER RESERVOIR NEAR PALMER, MI', 'usgs_data_geo_rfc': 'NCRFC', 'usgs_data_latitude': 46.4166052, 'usgs_data_longitude': -87.6468026, 'usgs_data_map_link': 'https://maps.google.com/maps?t=k&q=loc:46.4166052+-87.6468026', 'usgs_data_coord_accuracy_code': 'S', 'usgs_data_latlon_datum_name': 'NAD83', 'usgs_data_coord_method_code': 'M', 'usgs_data_state': 'Michigan', 'usgs_data_huc': 4030110.0, 'usgs_data_site_type': 'LK', 'usgs_data_altitude': 1300.16, 'usgs_data_alt_accuracy_code': 0.01, 'usgs_data_alt_datum_code': 'NAVD88', 'usgs_data_alt_method_code': 'L', 'usgs_data_drainage_area': 23.1, 'usgs_data_drainage_area_units': 'square miles', 'usgs_data_contrib_drainage_area': None, 'usgs_data_active': 1.0, 'usgs_data_gages_ii_reference': 0.0, 'nwm_feature_data_downstream_feature_id': 11959428.0, 'nwm_feature_data_latitude': 46.41669, 'nwm_feature_data_longitude': -87.67867, 'nwm_feature_data_altitude': 406.94, 'nwm_feature_data_stream_length': 5680.0, 'nwm_feature_data_stream_order': 2.0, 'nwm_feature_data_mannings_roughness': 0.06, 'nwm_feature_data_slope': 0.00059, 'nwm_feature_data_channel_side_slope': 0.4271125, 'nwm_feature_data_nhd_waterbody_comid': 11958896.0, 'env_can_gage_data_name': None, 'env_can_gage_data_latitude': None, 'env_can_gage_data_longitude': None, 'env_can_gage_data_map_link': None, 'env_can_gage_data_drainage_area': None, 'env_can_gage_data_contrib_drainage_area': None, 'env_can_gage_data_water_course': None, 'nws_preferred_name': 'Schweitzer Reservoir', 'nws_preferred_latitude': 46.416666666667005, 'nws_preferred_longitude': -87.646666666667, 'nws_preferred_latlon_datum_name': 'NAD83', 'nws_preferred_state': 'Michigan', 'nws_preferred_huc': '04030110', 'usgs_preferred_name': 'SCHWEITZER RESERVOIR NEAR PALMER, MI', 'usgs_preferred_latitude': 46.4166052, 'usgs_preferred_longitude': -87.6468026, 'usgs_preferred_latlon_datum_name': 'NAD83', 'usgs_preferred_state': 'Michigan', 'usgs_preferred_huc': 4030110.0, 'crosswalk_datasets_location_nwm_crosswalk_dataset_location_nwm_crosswalk_dataset_id': 1.2, 'crosswalk_datasets_location_nwm_crosswalk_dataset_name': 'Location NWM Crosswalk v1.2', 'crosswalk_datasets_location_nwm_crosswalk_dataset_description': 'Created 20220831.  Source:\\n1) NWM Routelink File v2.2-corrected\\n2) NHDPlus v2.1+\\n3) GID', 'crosswalk_datasets_nws_usgs_crosswalk_dataset_nws_usgs_crosswalk_dataset_id': 2.0, 'crosswalk_datasets_nws_usgs_crosswalk_dataset_name': 'NWS Station to USGS Gages 2.0', 'crosswalk_datasets_nws_usgs_crosswalk_dataset_description': 'Created 20210809.  Authoritative 2.0 dataset mapping NWS Stations to USGS Gages.  Source 1) AHPS CMS Report  2) HADS dataset  3) GID', 'assigned_crs': 'EPSG:4269', 'HUC8': 4030110, 'name': 'Escanaba', 'states': 'MI', 'curve': 'no', 'mainstem': 'no', 'acceptable_codes': 0.0, 'acceptable_alt_error': 1.0, 'levpa_id': 0, 'order_': None, 'geometry': 'POINT (643097.559228668 2631354.7869091793)', 'index_right': 208, 'HydroID': 13720026, 'LakeID': 11959120.0, 'geometry_ln': 'LINESTRING (641904.4094358841 2631309.2824222166, 641904.4094358841 2631309.2824222166, 641904.4094358841 2631309.2824222166, 641904.4094358841 26313 ... (9593 characters truncated) ... 3174.4094358841 2631099.2824222166, 643184.4094358841 2631099.2824222166, 643194.4094358841 2631099.2824222166, 643197.1288898574 2631099.2824222166)', 'geometry_snapped': 'POINT (642968.2320887947 2631225.4597693062)', 'snap_distance': 182.89619519156497, 'dem_elevation': 406.79828, 'dem_adj_elevation': 406.79828, 'fim_version': '4.3.11.0'}, {'wrds_timestamp': 'USGS data: USGS NWIS - Last updated: 2022-11-28 22:16:23 UTC', 'nrldb_timestamp': 'NWS data: NRLDB - Last updated: 2021-11-15 22:57:38 UTC', 'nwis_timestamp': 'USGS data: USGS NWIS - Last updated: 2022-11-28 22:16:23 UTC', 'metadata_sources': \"['NWS data: NRLDB - Last updated: 2021-11-15 22:57:38 UTC', 'USACE data: USACE - Last updated: 2021-09-24 17:19:28 UTC', 'USGS data: USGS NWIS - Last updated: 2022-11-28 22:16:23 UTC']\", 'nws_lid': 'SZCM4', 'location_id': '04058200', 'feature_id': 11959428, 'goes_id': 'DE21C35C', 'env_can_gage_id': None, 'nws_data_name': 'Palmer', 'nws_data_wfo': 'MQT', 'nws_data_rfc': 'NCRFC', 'nws_data_geo_rfc': 'NCRFC', 'nws_data_latitude': 46.411111111111005, 'nws_data_longitude': -87.62416666666698, 'nws_data_map_link': 'https://maps.google.com/maps?t=k&q=loc:46.411111111111+-87.624166666667', 'nws_data_horizontal_datum_name': None, 'nws_data_state': 'Michigan', 'nws_data_county': 'Marquette', 'nws_data_county_code': 26103.0, 'nws_data_huc': None, 'nws_data_hsa': 'MQT', 'nws_data_zero_datum': 1268.45, 'nws_data_vertical_datum_name': 'NAVD88', 'nws_data_rfc_forecast_point': 0.0, 'nws_data_rfc_defined_fcst_point': 0.0, 'nws_data_riverpoint': 1.0, 'usgs_data_name': 'SCHWEITZER CREEK NEAR PALMER, MI', 'usgs_data_geo_rfc': 'NCRFC', 'usgs_data_latitude': 46.41104987, 'usgs_data_longitude': -87.6243028, 'usgs_data_map_link': 'https://maps.google.com/maps?t=k&q=loc:46.41104987+-87.6243028', 'usgs_data_coord_accuracy_code': 'S', 'usgs_data_latlon_datum_name': 'NAD83', 'usgs_data_coord_method_code': 'M', 'usgs_data_state': 'Michigan', 'usgs_data_huc': 4030110.0, 'usgs_data_site_type': 'ST', 'usgs_data_altitude': 1268.45, 'usgs_data_alt_accuracy_code': 0.01, 'usgs_data_alt_datum_code': 'NAVD88', 'usgs_data_alt_method_code': 'L', 'usgs_data_drainage_area': 23.6, 'usgs_data_drainage_area_units': 'square miles', 'usgs_data_contrib_drainage_area': None, 'usgs_data_active': 1.0, 'usgs_data_gages_ii_reference': 0.0, 'nwm_feature_data_downstream_feature_id': 11959432.0, 'nwm_feature_data_latitude': 46.407140000000005, 'nwm_feature_data_longitude': -87.60683, 'nwm_feature_data_altitude': 403.59, 'nwm_feature_data_stream_length': 6636.0, 'nwm_feature_data_stream_order': 2.0, 'nwm_feature_data_mannings_roughness': 0.06, 'nwm_feature_data_slope': 0.00556, 'nwm_feature_data_channel_side_slope': 0.4189486, 'nwm_feature_data_nhd_waterbody_comid': None, 'env_can_gage_data_name': None, 'env_can_gage_data_latitude': None, 'env_can_gage_data_longitude': None, 'env_can_gage_data_map_link': None, 'env_can_gage_data_drainage_area': None, 'env_can_gage_data_contrib_drainage_area': None, 'env_can_gage_data_water_course': None, 'nws_preferred_name': 'Palmer', 'nws_preferred_latitude': 46.411111111111005, 'nws_preferred_longitude': -87.62416666666701, 'nws_preferred_latlon_datum_name': 'NAD83', 'nws_preferred_state': 'Michigan', 'nws_preferred_huc': '04030110', 'usgs_preferred_name': 'SCHWEITZER CREEK NEAR PALMER, MI', 'usgs_preferred_latitude': 46.41104987, 'usgs_preferred_longitude': -87.6243028, 'usgs_preferred_latlon_datum_name': 'NAD83', 'usgs_preferred_state': 'Michigan', 'usgs_preferred_huc': 4030110.0, 'crosswalk_datasets_location_nwm_crosswalk_dataset_location_nwm_crosswalk_dataset_id': 1.2, 'crosswalk_datasets_location_nwm_crosswalk_dataset_name': 'Location NWM Crosswalk v1.2', 'crosswalk_datasets_location_nwm_crosswalk_dataset_description': 'Created 20220831.  Source:\\n1) NWM Routelink File v2.2-corrected\\n2) NHDPlus v2.1+\\n3) GID', 'crosswalk_datasets_nws_usgs_crosswalk_dataset_nws_usgs_crosswalk_dataset_id': 2.0, 'crosswalk_datasets_nws_usgs_crosswalk_dataset_name': 'NWS Station to USGS Gages 2.0', 'crosswalk_datasets_nws_usgs_crosswalk_dataset_description': 'Created 20210809.  Authoritative 2.0 dataset mapping NWS Stations to USGS Gages.  Source 1) AHPS CMS Report  2) HADS dataset  3) GID', 'assigned_crs': 'EPSG:4269', 'HUC8': 4030110, 'name': 'Escanaba', 'states': 'MI', 'curve': 'yes', 'mainstem': 'no', 'acceptable_codes': 1.0, 'acceptable_alt_error': 1.0, 'levpa_id': 0, 'order_': None, 'geometry': 'POINT (644879.5058609735 2630893.7698675543)', 'index_right': 218, 'HydroID': 13720284, 'LakeID': -999.0, 'geometry_ln': 'LINESTRING (644404.4094358841 2630969.2824222166, 644404.4094358841 2630969.2824222166, 644404.4094358841 2630969.2824222166, 644404.4094358841 26309 ... (9365 characters truncated) ... 5614.4094358841 2630649.2824222166, 645624.4094358841 2630649.2824222166, 645634.4094358841 2630649.2824222166, 645644.4094358841 2630649.2824222166)', 'geometry_snapped': 'POINT (644874.4094358841 2630893.7698675543)', 'snap_distance': 5.0964250894030565, 'dem_elevation': 387.64026, 'dem_adj_elevation': 387.36773999999997, 'fim_version': '4.3.11.0'}, {'wrds_timestamp': 'USGS data: USGS NWIS - Last updated: 2022-11-28 22:16:23 UTC', 'nrldb_timestamp': 'NWS data: NRLDB - Last updated: 2021-11-15 22:57:38 UTC', 'nwis_timestamp': 'USGS data: USGS NWIS - Last updated: 2022-11-28 22:16:23 UTC', 'metadata_sources': \"['NWS data: NRLDB - Last updated: 2021-11-15 22:57:38 UTC', 'USACE data: USACE - Last updated: 2021-09-24 17:19:28 UTC', 'USGS data: USGS NWIS - Last updated: 2022-11-28 22:16:23 UTC']\", 'nws_lid': 'CRNM4', 'location_id': '04059000', 'feature_id': 11962300, 'goes_id': '1629DD40', 'env_can_gage_id': None, 'nws_data_name': 'Cornell', 'nws_data_wfo': 'MQT', 'nws_data_rfc': 'NCRFC', 'nws_data_geo_rfc': 'NCRFC', 'nws_data_latitude': 45.908611111111, 'nws_data_longitude': -87.213611111111, 'nws_data_map_link': 'https://maps.google.com/maps?t=k&q=loc:45.908611111111+-87.213611111111', 'nws_data_horizontal_datum_name': 'UNK', 'nws_data_state': 'Michigan', 'nws_data_county': 'Delta', 'nws_data_county_code': 26041.0, 'nws_data_huc': 'NONE', 'nws_data_hsa': 'MQT', 'nws_data_zero_datum': 749.36, 'nws_data_vertical_datum_name': 'NAVD88', 'nws_data_rfc_forecast_point': 0.0, 'nws_data_rfc_defined_fcst_point': 0.0, 'nws_data_riverpoint': 1.0, 'usgs_data_name': 'ESCANABA RIVER AT CORNELL, MI', 'usgs_data_geo_rfc': 'NCRFC', 'usgs_data_latitude': 45.90891667, 'usgs_data_longitude': -87.2135278, 'usgs_data_map_link': 'https://maps.google.com/maps?t=k&q=loc:45.90891667+-87.2135278', 'usgs_data_coord_accuracy_code': '1', 'usgs_data_latlon_datum_name': 'NAD83', 'usgs_data_coord_method_code': 'D', 'usgs_data_state': 'Michigan', 'usgs_data_huc': 4030110.0, 'usgs_data_site_type': 'ST', 'usgs_data_altitude': 749.36, 'usgs_data_alt_accuracy_code': 0.01, 'usgs_data_alt_datum_code': 'NAVD88', 'usgs_data_alt_method_code': 'R', 'usgs_data_drainage_area': 870.0, 'usgs_data_drainage_area_units': 'square miles', 'usgs_data_contrib_drainage_area': None, 'usgs_data_active': 1.0, 'usgs_data_gages_ii_reference': 0.0, 'nwm_feature_data_downstream_feature_id': 11962302.0, 'nwm_feature_data_latitude': 45.90113, 'nwm_feature_data_longitude': -87.15697, 'nwm_feature_data_altitude': 228.96, 'nwm_feature_data_stream_length': 12142.0, 'nwm_feature_data_stream_order': 5.0, 'nwm_feature_data_mannings_roughness': 0.05, 'nwm_feature_data_slope': 0.0018100000000000002, 'nwm_feature_data_channel_side_slope': 0.24497339999999998, 'nwm_feature_data_nhd_waterbody_comid': None, 'env_can_gage_data_name': None, 'env_can_gage_data_latitude': None, 'env_can_gage_data_longitude': None, 'env_can_gage_data_map_link': None, 'env_can_gage_data_drainage_area': None, 'env_can_gage_data_contrib_drainage_area': None, 'env_can_gage_data_water_course': None, 'nws_preferred_name': 'Cornell', 'nws_preferred_latitude': 45.908611111111, 'nws_preferred_longitude': -87.21361111111099, 'nws_preferred_latlon_datum_name': 'UNK', 'nws_preferred_state': 'Michigan', 'nws_preferred_huc': 'NONE', 'usgs_preferred_name': 'ESCANABA RIVER AT CORNELL, MI', 'usgs_preferred_latitude': 45.90891667, 'usgs_preferred_longitude': -87.2135278, 'usgs_preferred_latlon_datum_name': 'NAD83', 'usgs_preferred_state': 'Michigan', 'usgs_preferred_huc': 4030110.0, 'crosswalk_datasets_location_nwm_crosswalk_dataset_location_nwm_crosswalk_dataset_id': 1.2, 'crosswalk_datasets_location_nwm_crosswalk_dataset_name': 'Location NWM Crosswalk v1.2', 'crosswalk_datasets_location_nwm_crosswalk_dataset_description': 'Created 20220831.  Source:\\n1) NWM Routelink File v2.2-corrected\\n2) NHDPlus v2.1+\\n3) GID', 'crosswalk_datasets_nws_usgs_crosswalk_dataset_nws_usgs_crosswalk_dataset_id': 2.0, 'crosswalk_datasets_nws_usgs_crosswalk_dataset_name': 'NWS Station to USGS Gages 2.0', 'crosswalk_datasets_nws_usgs_crosswalk_dataset_description': 'Created 20210809.  Authoritative 2.0 dataset mapping NWS Stations to USGS Gages.  Source 1) AHPS CMS Report  2) HADS dataset  3) GID', 'assigned_crs': 'EPSG:4269', 'HUC8': 4030110, 'name': 'Escanaba', 'states': 'MI', 'curve': 'yes', 'mainstem': 'yes', 'acceptable_codes': 1.0, 'acceptable_alt_error': 1.0, 'levpa_id': 0, 'order_': 5.0, 'geometry': 'POINT (681561.9614728471 2578279.373438757)', 'index_right': 809, 'HydroID': 13720047, 'LakeID': -999.0, 'geometry_ln': 'LINESTRING (681564.4094358841 2578919.2824222166, 681574.4094358841 2578909.2824222166, 681574.4094358841 2578899.2824222166, 681584.4094358841 25788 ... (4577 characters truncated) ... 1874.4094358841 2577679.2824222166, 681884.4094358841 2577669.2824222166, 681894.4094358841 2577659.2824222166, 681904.4094358841 2577649.2824222166)', 'geometry_snapped': 'POINT (681634.4094358841 2578279.373438757)', 'snap_distance': 72.44796303694602, 'dem_elevation': 228.451, 'dem_adj_elevation': 228.428, 'fim_version': '4.3.11.0'}, {'wrds_timestamp': None, 'nrldb_timestamp': None, 'nwis_timestamp': None, 'metadata_sources': None, 'nws_lid': 'GNNM4', 'location_id': 'MTAO1', 'feature_id': 11959566, 'goes_id': None, 'env_can_gage_id': None, 'nws_data_name': None, 'nws_data_wfo': None, 'nws_data_rfc': None, 'nws_data_geo_rfc': None, 'nws_data_latitude': None, 'nws_data_longitude': None, 'nws_data_map_link': None, 'nws_data_horizontal_datum_name': None, 'nws_data_state': None, 'nws_data_county': None, 'nws_data_county_code': None, 'nws_data_huc': None, 'nws_data_hsa': None, 'nws_data_zero_datum': None, 'nws_data_vertical_datum_name': None, 'nws_data_rfc_forecast_point': None, 'nws_data_rfc_defined_fcst_point': None, 'nws_data_riverpoint': None, 'usgs_data_name': None, 'usgs_data_geo_rfc': None, 'usgs_data_latitude': None, 'usgs_data_longitude': None, 'usgs_data_map_link': None, 'usgs_data_coord_accuracy_code': None, 'usgs_data_latlon_datum_name': None, 'usgs_data_coord_method_code': None, 'usgs_data_state': None, 'usgs_data_huc': None, 'usgs_data_site_type': None, 'usgs_data_altitude': None, 'usgs_data_alt_accuracy_code': None, 'usgs_data_alt_datum_code': None, 'usgs_data_alt_method_code': None, 'usgs_data_drainage_area': None, 'usgs_data_drainage_area_units': None, 'usgs_data_contrib_drainage_area': None, 'usgs_data_active': None, 'usgs_data_gages_ii_reference': None, 'nwm_feature_data_downstream_feature_id': None, 'nwm_feature_data_latitude': None, 'nwm_feature_data_longitude': None, 'nwm_feature_data_altitude': None, 'nwm_feature_data_stream_length': None, 'nwm_feature_data_stream_order': None, 'nwm_feature_data_mannings_roughness': None, 'nwm_feature_data_slope': None, 'nwm_feature_data_channel_side_slope': None, 'nwm_feature_data_nhd_waterbody_comid': None, 'env_can_gage_data_name': None, 'env_can_gage_data_latitude': None, 'env_can_gage_data_longitude': None, 'env_can_gage_data_map_link': None, 'env_can_gage_data_drainage_area': None, 'env_can_gage_data_contrib_drainage_area': None, 'env_can_gage_data_water_course': None, 'nws_preferred_name': None, 'nws_preferred_latitude': None, 'nws_preferred_longitude': None, 'nws_preferred_latlon_datum_name': None, 'nws_preferred_state': None, 'nws_preferred_huc': None, 'usgs_preferred_name': None, 'usgs_preferred_latitude': None, 'usgs_preferred_longitude': None, 'usgs_preferred_latlon_datum_name': None, 'usgs_preferred_state': None, 'usgs_preferred_huc': None, 'crosswalk_datasets_location_nwm_crosswalk_dataset_location_nwm_crosswalk_dataset_id': None, 'crosswalk_datasets_location_nwm_crosswalk_dataset_name': None, 'crosswalk_datasets_location_nwm_crosswalk_dataset_description': None, 'crosswalk_datasets_nws_usgs_crosswalk_dataset_nws_usgs_crosswalk_dataset_id': None, 'crosswalk_datasets_nws_usgs_crosswalk_dataset_name': None, 'crosswalk_datasets_nws_usgs_crosswalk_dataset_description': None, 'assigned_crs': None, 'HUC8': 4030110, 'name': 'Escanaba', 'states': 'MI', 'curve': None, 'mainstem': None, 'acceptable_codes': None, 'acceptable_alt_error': None, 'levpa_id': 0, 'order_': 4.0, 'geometry': 'POINT (660660.238467496 2617467.362296472)', 'index_right': 436, 'HydroID': 13720148, 'LakeID': -999.0, 'geometry_ln': 'LINESTRING (660674.4094358841 2617929.2824222166, 660674.4094358841 2617929.2824222166, 660674.4094358841 2617929.2824222166, 660674.4094358841 26179 ... (7769 characters truncated) ... 0034.4094358841 2617429.2824222166, 660024.4094358841 2617429.2824222166, 660014.4094358841 2617419.2824222166, 660004.4094358841 2617409.2824222166)', 'geometry_snapped': 'POINT (660634.4094358841 2617509.2824222166)', 'snap_distance': 49.23856025988607, 'dem_elevation': 331.7328, 'dem_adj_elevation': 329.4961, 'fim_version': '4.3.11.0'})]\n",
      "(Background on this error at: http://sqlalche.me/e/13/9h9h)\n",
      "******************************************\n",
      "Processing 373 of 1000 on page 1\n",
      "Processing 374 of 1000 on page 1\n",
      "Processing 375 of 1000 on page 1\n",
      "Processing 376 of 1000 on page 1\n",
      "Processing 377 of 1000 on page 1\n",
      "Processing 378 of 1000 on page 1\n",
      "Processing 379 of 1000 on page 1\n",
      "Processing 380 of 1000 on page 1\n",
      "Processing 381 of 1000 on page 1\n",
      "Processing 382 of 1000 on page 1\n",
      "Processing 383 of 1000 on page 1\n",
      "Processing 384 of 1000 on page 1\n",
      "Processing 385 of 1000 on page 1\n",
      "Processing 386 of 1000 on page 1\n",
      "Processing 387 of 1000 on page 1\n",
      "Processing 388 of 1000 on page 1\n",
      "Processing 389 of 1000 on page 1\n",
      "Processing 390 of 1000 on page 1\n",
      "Processing 391 of 1000 on page 1\n",
      "Processing 392 of 1000 on page 1\n",
      "Processing 393 of 1000 on page 1\n",
      "Processing 394 of 1000 on page 1\n",
      "Processing 395 of 1000 on page 1\n",
      "Processing 396 of 1000 on page 1\n",
      "Processing 397 of 1000 on page 1\n",
      "Processing 398 of 1000 on page 1\n",
      "Processing 399 of 1000 on page 1\n",
      "Processing 400 of 1000 on page 1\n",
      "Processing 401 of 1000 on page 1\n",
      "Processing 402 of 1000 on page 1\n",
      "Processing 403 of 1000 on page 1\n",
      "Processing 404 of 1000 on page 1\n",
      "Processing 405 of 1000 on page 1\n",
      "Processing 406 of 1000 on page 1\n",
      "Processing 407 of 1000 on page 1\n",
      "Processing 408 of 1000 on page 1\n",
      "Processing 409 of 1000 on page 1\n",
      "Processing 410 of 1000 on page 1\n",
      "Processing 411 of 1000 on page 1\n",
      "Processing 412 of 1000 on page 1\n",
      "Processing 413 of 1000 on page 1\n",
      "Processing 414 of 1000 on page 1\n",
      "Processing 415 of 1000 on page 1\n",
      "Processing 416 of 1000 on page 1\n",
      "Processing 417 of 1000 on page 1\n",
      "Processing 418 of 1000 on page 1\n",
      "Processing 419 of 1000 on page 1\n",
      "Processing 420 of 1000 on page 1\n",
      "Processing 421 of 1000 on page 1\n",
      "Processing 422 of 1000 on page 1\n",
      "Processing 423 of 1000 on page 1\n",
      "Processing 424 of 1000 on page 1\n",
      "Processing 425 of 1000 on page 1\n",
      "Processing 426 of 1000 on page 1\n",
      "Processing 427 of 1000 on page 1\n",
      "Processing 428 of 1000 on page 1\n",
      "Processing 429 of 1000 on page 1\n",
      "Processing 430 of 1000 on page 1\n",
      "Processing 431 of 1000 on page 1\n",
      "Processing 432 of 1000 on page 1\n",
      "Processing 433 of 1000 on page 1\n",
      "Processing 434 of 1000 on page 1\n",
      "Processing 435 of 1000 on page 1\n",
      "Processing 436 of 1000 on page 1\n",
      "Processing 437 of 1000 on page 1\n",
      "Processing 438 of 1000 on page 1\n",
      "Processing 439 of 1000 on page 1\n",
      "Processing 440 of 1000 on page 1\n",
      "Processing 441 of 1000 on page 1\n",
      "Processing 442 of 1000 on page 1\n",
      "Processing 443 of 1000 on page 1\n",
      "Processing 444 of 1000 on page 1\n",
      "Processing 445 of 1000 on page 1\n",
      "Processing 446 of 1000 on page 1\n",
      "Processing 447 of 1000 on page 1\n",
      "Processing 448 of 1000 on page 1\n",
      "Processing 449 of 1000 on page 1\n",
      "Processing 450 of 1000 on page 1\n",
      "Processing 451 of 1000 on page 1\n",
      "Processing 452 of 1000 on page 1\n",
      "Processing 453 of 1000 on page 1\n",
      "Processing 454 of 1000 on page 1\n",
      "Processing 455 of 1000 on page 1\n",
      "Processing 456 of 1000 on page 1\n",
      "Processing 457 of 1000 on page 1\n",
      "Processing 458 of 1000 on page 1\n",
      "Processing 459 of 1000 on page 1\n",
      "Processing 460 of 1000 on page 1\n",
      "Processing 461 of 1000 on page 1\n",
      "Processing 462 of 1000 on page 1\n",
      "Processing 463 of 1000 on page 1\n",
      "Processing 464 of 1000 on page 1\n",
      "Processing 465 of 1000 on page 1\n",
      "Processing 466 of 1000 on page 1\n",
      "Processing 467 of 1000 on page 1\n",
      "Processing 468 of 1000 on page 1\n",
      "Processing 469 of 1000 on page 1\n",
      "Processing 470 of 1000 on page 1\n",
      "Processing 471 of 1000 on page 1\n",
      "Processing 472 of 1000 on page 1\n",
      "Processing 473 of 1000 on page 1\n",
      "Processing 474 of 1000 on page 1\n",
      "Processing 475 of 1000 on page 1\n",
      "Processing 476 of 1000 on page 1\n",
      "Processing 477 of 1000 on page 1\n",
      "Processing 478 of 1000 on page 1\n",
      "Processing 479 of 1000 on page 1\n",
      "Processing 480 of 1000 on page 1\n",
      "Processing 481 of 1000 on page 1\n",
      "Processing 482 of 1000 on page 1\n",
      "Processing 483 of 1000 on page 1\n",
      "Processing 484 of 1000 on page 1\n",
      "Processing 485 of 1000 on page 1\n",
      "Processing 486 of 1000 on page 1\n",
      "Processing 487 of 1000 on page 1\n",
      "Processing 488 of 1000 on page 1\n",
      "Processing 489 of 1000 on page 1\n",
      "Processing 490 of 1000 on page 1\n",
      "Processing 491 of 1000 on page 1\n",
      "Processing 492 of 1000 on page 1\n",
      "Processing 493 of 1000 on page 1\n",
      "Processing 494 of 1000 on page 1\n",
      "Processing 495 of 1000 on page 1\n",
      "Processing 496 of 1000 on page 1\n",
      "Processing 497 of 1000 on page 1\n",
      "Processing 498 of 1000 on page 1\n",
      "Processing 499 of 1000 on page 1\n",
      "Processing 500 of 1000 on page 1\n",
      "Processing 501 of 1000 on page 1\n",
      "Processing 502 of 1000 on page 1\n",
      "Processing 503 of 1000 on page 1\n",
      "Processing 504 of 1000 on page 1\n",
      "Processing 505 of 1000 on page 1\n",
      "Processing 506 of 1000 on page 1\n",
      "Processing 507 of 1000 on page 1\n",
      "Processing 508 of 1000 on page 1\n",
      "Processing 509 of 1000 on page 1\n",
      "Processing 510 of 1000 on page 1\n",
      "Processing 511 of 1000 on page 1\n",
      "Processing 512 of 1000 on page 1\n",
      "Processing 513 of 1000 on page 1\n",
      "Processing 514 of 1000 on page 1\n",
      "Processing 515 of 1000 on page 1\n",
      "Processing 516 of 1000 on page 1\n",
      "Processing 517 of 1000 on page 1\n",
      "Processing 518 of 1000 on page 1\n",
      "Processing 519 of 1000 on page 1\n",
      "Processing 520 of 1000 on page 1\n",
      "Processing 521 of 1000 on page 1\n",
      "Processing 522 of 1000 on page 1\n",
      "Processing 523 of 1000 on page 1\n",
      "Processing 524 of 1000 on page 1\n",
      "Processing 525 of 1000 on page 1\n",
      "Processing 526 of 1000 on page 1\n",
      "Processing 527 of 1000 on page 1\n",
      "Processing 528 of 1000 on page 1\n",
      "Processing 529 of 1000 on page 1\n",
      "Processing 530 of 1000 on page 1\n",
      "Processing 531 of 1000 on page 1\n",
      "Processing 532 of 1000 on page 1\n",
      "Processing 533 of 1000 on page 1\n",
      "Processing 534 of 1000 on page 1\n",
      "Processing 535 of 1000 on page 1\n",
      "Processing 536 of 1000 on page 1\n",
      "Processing 537 of 1000 on page 1\n",
      "Processing 538 of 1000 on page 1\n",
      "Processing 539 of 1000 on page 1\n",
      "Processing 540 of 1000 on page 1\n",
      "Processing 541 of 1000 on page 1\n",
      "Processing 542 of 1000 on page 1\n",
      "Processing 543 of 1000 on page 1\n",
      "Processing 544 of 1000 on page 1\n",
      "Processing 545 of 1000 on page 1\n",
      "Processing 546 of 1000 on page 1\n",
      "Processing 547 of 1000 on page 1\n",
      "Processing 548 of 1000 on page 1\n",
      "Processing 549 of 1000 on page 1\n",
      "Processing 550 of 1000 on page 1\n",
      "Processing 551 of 1000 on page 1\n",
      "Processing 552 of 1000 on page 1\n",
      "Processing 553 of 1000 on page 1\n",
      "Processing 554 of 1000 on page 1\n",
      "Processing 555 of 1000 on page 1\n",
      "Processing 556 of 1000 on page 1\n",
      "Processing 557 of 1000 on page 1\n",
      "Processing 558 of 1000 on page 1\n",
      "Processing 559 of 1000 on page 1\n",
      "Processing 560 of 1000 on page 1\n",
      "Processing 561 of 1000 on page 1\n",
      "Processing 562 of 1000 on page 1\n",
      "Processing 563 of 1000 on page 1\n",
      "Processing 564 of 1000 on page 1\n",
      "Processing 565 of 1000 on page 1\n",
      "Processing 566 of 1000 on page 1\n",
      "Processing 567 of 1000 on page 1\n",
      "Processing 568 of 1000 on page 1\n",
      "Processing 569 of 1000 on page 1\n",
      "Processing 570 of 1000 on page 1\n",
      "Processing 571 of 1000 on page 1\n",
      "Processing 572 of 1000 on page 1\n",
      "Processing 573 of 1000 on page 1\n",
      "Processing 574 of 1000 on page 1\n",
      "Processing 575 of 1000 on page 1\n",
      "Processing 576 of 1000 on page 1\n",
      "Processing 577 of 1000 on page 1\n",
      "Processing 578 of 1000 on page 1\n",
      "Processing 579 of 1000 on page 1\n",
      "Processing 580 of 1000 on page 1\n",
      "Processing 581 of 1000 on page 1\n",
      "Processing 582 of 1000 on page 1\n",
      "Processing 583 of 1000 on page 1\n",
      "Processing 584 of 1000 on page 1\n",
      "Processing 585 of 1000 on page 1\n",
      "Processing 586 of 1000 on page 1\n",
      "Processing 587 of 1000 on page 1\n",
      "Processing 588 of 1000 on page 1\n",
      "Processing 589 of 1000 on page 1\n",
      "Processing 590 of 1000 on page 1\n",
      "Processing 591 of 1000 on page 1\n",
      "Processing 592 of 1000 on page 1\n",
      "Processing 593 of 1000 on page 1\n",
      "Processing 594 of 1000 on page 1\n",
      "Processing 595 of 1000 on page 1\n",
      "Processing 596 of 1000 on page 1\n",
      "Processing 597 of 1000 on page 1\n",
      "Processing 598 of 1000 on page 1\n",
      "Processing 599 of 1000 on page 1\n",
      "Processing 600 of 1000 on page 1\n",
      "Processing 601 of 1000 on page 1\n",
      "Processing 602 of 1000 on page 1\n",
      "Processing 603 of 1000 on page 1\n",
      "Processing 604 of 1000 on page 1\n",
      "Processing 605 of 1000 on page 1\n",
      "Processing 606 of 1000 on page 1\n",
      "Processing 607 of 1000 on page 1\n",
      "Processing 608 of 1000 on page 1\n",
      "Processing 609 of 1000 on page 1\n",
      "Processing 610 of 1000 on page 1\n",
      "Processing 611 of 1000 on page 1\n",
      "Processing 612 of 1000 on page 1\n",
      "Processing 613 of 1000 on page 1\n",
      "Processing 614 of 1000 on page 1\n",
      "Processing 615 of 1000 on page 1\n",
      "Processing 616 of 1000 on page 1\n",
      "Processing 617 of 1000 on page 1\n",
      "Processing 618 of 1000 on page 1\n",
      "Processing 619 of 1000 on page 1\n",
      "Processing 620 of 1000 on page 1\n",
      "Processing 621 of 1000 on page 1\n",
      "Processing 622 of 1000 on page 1\n",
      "Processing 623 of 1000 on page 1\n",
      "Processing 624 of 1000 on page 1\n",
      "Processing 625 of 1000 on page 1\n",
      "Processing 626 of 1000 on page 1\n",
      "Processing 627 of 1000 on page 1\n",
      "Processing 628 of 1000 on page 1\n",
      "Processing 629 of 1000 on page 1\n",
      "Processing 630 of 1000 on page 1\n",
      "Processing 631 of 1000 on page 1\n",
      "Processing 632 of 1000 on page 1\n",
      "Processing 633 of 1000 on page 1\n",
      "Processing 634 of 1000 on page 1\n",
      "Processing 635 of 1000 on page 1\n",
      "Processing 636 of 1000 on page 1\n",
      "Processing 637 of 1000 on page 1\n",
      "Processing 638 of 1000 on page 1\n",
      "Processing 639 of 1000 on page 1\n",
      "Processing 640 of 1000 on page 1\n",
      "Processing 641 of 1000 on page 1\n",
      "Processing 642 of 1000 on page 1\n",
      "Processing 643 of 1000 on page 1\n",
      "Processing 644 of 1000 on page 1\n",
      "Processing 645 of 1000 on page 1\n",
      "Processing 646 of 1000 on page 1\n",
      "Processing 647 of 1000 on page 1\n",
      "Processing 648 of 1000 on page 1\n",
      "Processing 649 of 1000 on page 1\n",
      "Processing 650 of 1000 on page 1\n",
      "Processing 651 of 1000 on page 1\n",
      "Processing 652 of 1000 on page 1\n",
      "Processing 653 of 1000 on page 1\n",
      "Processing 654 of 1000 on page 1\n",
      "Processing 655 of 1000 on page 1\n",
      "Processing 656 of 1000 on page 1\n",
      "Processing 657 of 1000 on page 1\n",
      "Processing 658 of 1000 on page 1\n",
      "Processing 659 of 1000 on page 1\n",
      "Processing 660 of 1000 on page 1\n",
      "Processing 661 of 1000 on page 1\n",
      "Processing 662 of 1000 on page 1\n",
      "Processing 663 of 1000 on page 1\n",
      "Processing 664 of 1000 on page 1\n",
      "Processing 665 of 1000 on page 1\n",
      "Processing 666 of 1000 on page 1\n",
      "Processing 667 of 1000 on page 1\n",
      "Processing 668 of 1000 on page 1\n",
      "Processing 669 of 1000 on page 1\n",
      "Processing 670 of 1000 on page 1\n",
      "Processing 671 of 1000 on page 1\n",
      "Processing 672 of 1000 on page 1\n",
      "Processing 673 of 1000 on page 1\n",
      "Processing 674 of 1000 on page 1\n",
      "Processing 675 of 1000 on page 1\n",
      "Processing 676 of 1000 on page 1\n",
      "Processing 677 of 1000 on page 1\n",
      "Processing 678 of 1000 on page 1\n",
      "Processing 679 of 1000 on page 1\n",
      "Processing 680 of 1000 on page 1\n",
      "Processing 681 of 1000 on page 1\n",
      "Processing 682 of 1000 on page 1\n",
      "Processing 683 of 1000 on page 1\n",
      "Processing 684 of 1000 on page 1\n",
      "Processing 685 of 1000 on page 1\n",
      "Processing 686 of 1000 on page 1\n",
      "Processing 687 of 1000 on page 1\n",
      "Processing 688 of 1000 on page 1\n",
      "Processing 689 of 1000 on page 1\n",
      "Processing 690 of 1000 on page 1\n",
      "Processing 691 of 1000 on page 1\n",
      "Processing 692 of 1000 on page 1\n",
      "Processing 693 of 1000 on page 1\n",
      "Processing 694 of 1000 on page 1\n",
      "Processing 695 of 1000 on page 1\n",
      "Processing 696 of 1000 on page 1\n",
      "Processing 697 of 1000 on page 1\n",
      "Processing 698 of 1000 on page 1\n",
      "Processing 699 of 1000 on page 1\n",
      "Processing 700 of 1000 on page 1\n",
      "Processing 701 of 1000 on page 1\n",
      "Processing 702 of 1000 on page 1\n",
      "Processing 703 of 1000 on page 1\n",
      "Processing 704 of 1000 on page 1\n",
      "Processing 705 of 1000 on page 1\n",
      "Processing 706 of 1000 on page 1\n",
      "Processing 707 of 1000 on page 1\n",
      "Processing 708 of 1000 on page 1\n",
      "Processing 709 of 1000 on page 1\n",
      "Processing 710 of 1000 on page 1\n",
      "Processing 711 of 1000 on page 1\n",
      "Processing 712 of 1000 on page 1\n",
      "Processing 713 of 1000 on page 1\n",
      "Processing 714 of 1000 on page 1\n",
      "Processing 715 of 1000 on page 1\n",
      "Processing 716 of 1000 on page 1\n",
      "Processing 717 of 1000 on page 1\n",
      "Processing 718 of 1000 on page 1\n",
      "Processing 719 of 1000 on page 1\n",
      "Processing 720 of 1000 on page 1\n",
      "Processing 721 of 1000 on page 1\n",
      "Processing 722 of 1000 on page 1\n",
      "Processing 723 of 1000 on page 1\n",
      "Processing 724 of 1000 on page 1\n",
      "Processing 725 of 1000 on page 1\n",
      "Processing 726 of 1000 on page 1\n",
      "Processing 727 of 1000 on page 1\n",
      "Processing 728 of 1000 on page 1\n",
      "Processing 729 of 1000 on page 1\n",
      "Processing 730 of 1000 on page 1\n",
      "Processing 731 of 1000 on page 1\n",
      "Processing 732 of 1000 on page 1\n",
      "Processing 733 of 1000 on page 1\n",
      "Processing 734 of 1000 on page 1\n",
      "Processing 735 of 1000 on page 1\n",
      "Processing 736 of 1000 on page 1\n",
      "Processing 737 of 1000 on page 1\n",
      "Processing 738 of 1000 on page 1\n",
      "Processing 739 of 1000 on page 1\n",
      "Processing 740 of 1000 on page 1\n",
      "Processing 741 of 1000 on page 1\n",
      "Processing 742 of 1000 on page 1\n",
      "Processing 743 of 1000 on page 1\n",
      "Processing 744 of 1000 on page 1\n",
      "Processing 745 of 1000 on page 1\n",
      "Processing 746 of 1000 on page 1\n",
      "Processing 747 of 1000 on page 1\n",
      "Processing 748 of 1000 on page 1\n",
      "Processing 749 of 1000 on page 1\n",
      "Processing 750 of 1000 on page 1\n",
      "Processing 751 of 1000 on page 1\n",
      "Processing 752 of 1000 on page 1\n",
      "Processing 753 of 1000 on page 1\n",
      "Processing 754 of 1000 on page 1\n",
      "Processing 755 of 1000 on page 1\n",
      "Processing 756 of 1000 on page 1\n",
      "Processing 757 of 1000 on page 1\n",
      "Processing 758 of 1000 on page 1\n",
      "Processing 759 of 1000 on page 1\n",
      "Processing 760 of 1000 on page 1\n",
      "Processing 761 of 1000 on page 1\n",
      "Processing 762 of 1000 on page 1\n",
      "Processing 763 of 1000 on page 1\n",
      "Processing 764 of 1000 on page 1\n",
      "Processing 765 of 1000 on page 1\n",
      "Processing 766 of 1000 on page 1\n",
      "Processing 767 of 1000 on page 1\n",
      "Processing 768 of 1000 on page 1\n",
      "Processing 769 of 1000 on page 1\n",
      "Processing 770 of 1000 on page 1\n",
      "Processing 771 of 1000 on page 1\n",
      "Processing 772 of 1000 on page 1\n",
      "Processing 773 of 1000 on page 1\n",
      "Processing 774 of 1000 on page 1\n",
      "Processing 775 of 1000 on page 1\n",
      "Processing 776 of 1000 on page 1\n",
      "Processing 777 of 1000 on page 1\n",
      "Processing 778 of 1000 on page 1\n",
      "Processing 779 of 1000 on page 1\n",
      "Processing 780 of 1000 on page 1\n",
      "Processing 781 of 1000 on page 1\n",
      "Processing 782 of 1000 on page 1\n",
      "Processing 783 of 1000 on page 1\n",
      "Processing 784 of 1000 on page 1\n",
      "Processing 785 of 1000 on page 1\n",
      "Processing 786 of 1000 on page 1\n",
      "Processing 787 of 1000 on page 1\n",
      "Processing 788 of 1000 on page 1\n",
      "Processing 789 of 1000 on page 1\n",
      "Processing 790 of 1000 on page 1\n",
      "Processing 791 of 1000 on page 1\n",
      "Processing 792 of 1000 on page 1\n",
      "Processing 793 of 1000 on page 1\n",
      "Processing 794 of 1000 on page 1\n",
      "Processing 795 of 1000 on page 1\n",
      "Processing 796 of 1000 on page 1\n",
      "Processing 797 of 1000 on page 1\n",
      "Processing 798 of 1000 on page 1\n",
      "Processing 799 of 1000 on page 1\n",
      "Processing 800 of 1000 on page 1\n",
      "Processing 801 of 1000 on page 1\n",
      "Processing 802 of 1000 on page 1\n",
      "Processing 803 of 1000 on page 1\n",
      "Processing 804 of 1000 on page 1\n",
      "Processing 805 of 1000 on page 1\n",
      "Processing 806 of 1000 on page 1\n",
      "Processing 807 of 1000 on page 1\n",
      "Processing 808 of 1000 on page 1\n",
      "Processing 809 of 1000 on page 1\n",
      "Processing 810 of 1000 on page 1\n",
      "Processing 811 of 1000 on page 1\n",
      "Processing 812 of 1000 on page 1\n",
      "Processing 813 of 1000 on page 1\n",
      "Processing 814 of 1000 on page 1\n",
      "Processing 815 of 1000 on page 1\n",
      "Processing 816 of 1000 on page 1\n",
      "Processing 817 of 1000 on page 1\n",
      "Processing 818 of 1000 on page 1\n",
      "Processing 819 of 1000 on page 1\n",
      "Processing 820 of 1000 on page 1\n",
      "Processing 821 of 1000 on page 1\n",
      "Processing 822 of 1000 on page 1\n",
      "Processing 823 of 1000 on page 1\n",
      "Processing 824 of 1000 on page 1\n",
      "Processing 825 of 1000 on page 1\n",
      "Processing 826 of 1000 on page 1\n",
      "Processing 827 of 1000 on page 1\n",
      "Processing 828 of 1000 on page 1\n",
      "Processing 829 of 1000 on page 1\n",
      "Processing 830 of 1000 on page 1\n",
      "Processing 831 of 1000 on page 1\n",
      "Processing 832 of 1000 on page 1\n",
      "Processing 833 of 1000 on page 1\n",
      "Processing 834 of 1000 on page 1\n",
      "Processing 835 of 1000 on page 1\n",
      "Processing 836 of 1000 on page 1\n",
      "Processing 837 of 1000 on page 1\n",
      "Processing 838 of 1000 on page 1\n",
      "Processing 839 of 1000 on page 1\n",
      "Processing 840 of 1000 on page 1\n",
      "Processing 841 of 1000 on page 1\n",
      "Processing 842 of 1000 on page 1\n",
      "Processing 843 of 1000 on page 1\n",
      "Processing 844 of 1000 on page 1\n",
      "Processing 845 of 1000 on page 1\n",
      "Processing 846 of 1000 on page 1\n",
      "Processing 847 of 1000 on page 1\n",
      "Processing 848 of 1000 on page 1\n",
      "Processing 849 of 1000 on page 1\n",
      "Processing 850 of 1000 on page 1\n",
      "Processing 851 of 1000 on page 1\n",
      "Processing 852 of 1000 on page 1\n",
      "Processing 853 of 1000 on page 1\n",
      "Processing 854 of 1000 on page 1\n",
      "Processing 855 of 1000 on page 1\n",
      "Processing 856 of 1000 on page 1\n",
      "Processing 857 of 1000 on page 1\n",
      "Processing 858 of 1000 on page 1\n",
      "Processing 859 of 1000 on page 1\n",
      "Processing 860 of 1000 on page 1\n",
      "Processing 861 of 1000 on page 1\n",
      "Processing 862 of 1000 on page 1\n",
      "Processing 863 of 1000 on page 1\n",
      "Processing 864 of 1000 on page 1\n",
      "Processing 865 of 1000 on page 1\n",
      "Processing 866 of 1000 on page 1\n",
      "Processing 867 of 1000 on page 1\n",
      "Processing 868 of 1000 on page 1\n",
      "Processing 869 of 1000 on page 1\n",
      "Processing 870 of 1000 on page 1\n",
      "Processing 871 of 1000 on page 1\n",
      "Processing 872 of 1000 on page 1\n",
      "Processing 873 of 1000 on page 1\n",
      "Processing 874 of 1000 on page 1\n",
      "Processing 875 of 1000 on page 1\n",
      "Processing 876 of 1000 on page 1\n",
      "Processing 877 of 1000 on page 1\n",
      "Processing 878 of 1000 on page 1\n",
      "Processing 879 of 1000 on page 1\n",
      "Processing 880 of 1000 on page 1\n",
      "Processing 881 of 1000 on page 1\n",
      "Processing 882 of 1000 on page 1\n",
      "Processing 883 of 1000 on page 1\n",
      "Processing 884 of 1000 on page 1\n",
      "Processing 885 of 1000 on page 1\n",
      "Processing 886 of 1000 on page 1\n",
      "Processing 887 of 1000 on page 1\n",
      "Processing 888 of 1000 on page 1\n",
      "Processing 889 of 1000 on page 1\n",
      "Processing 890 of 1000 on page 1\n",
      "Processing 891 of 1000 on page 1\n",
      "Processing 892 of 1000 on page 1\n",
      "Processing 893 of 1000 on page 1\n",
      "Processing 894 of 1000 on page 1\n",
      "Processing 895 of 1000 on page 1\n",
      "Processing 896 of 1000 on page 1\n",
      "Processing 897 of 1000 on page 1\n",
      "Processing 898 of 1000 on page 1\n",
      "Processing 899 of 1000 on page 1\n",
      "Processing 900 of 1000 on page 1\n",
      "Processing 901 of 1000 on page 1\n",
      "Processing 902 of 1000 on page 1\n",
      "Processing 903 of 1000 on page 1\n",
      "Processing 904 of 1000 on page 1\n",
      "Processing 905 of 1000 on page 1\n",
      "Processing 906 of 1000 on page 1\n",
      "Processing 907 of 1000 on page 1\n",
      "Processing 908 of 1000 on page 1\n",
      "Processing 909 of 1000 on page 1\n",
      "Processing 910 of 1000 on page 1\n",
      "Processing 911 of 1000 on page 1\n",
      "Processing 912 of 1000 on page 1\n",
      "Processing 913 of 1000 on page 1\n",
      "Processing 914 of 1000 on page 1\n",
      "Processing 915 of 1000 on page 1\n",
      "Processing 916 of 1000 on page 1\n",
      "Processing 917 of 1000 on page 1\n",
      "Processing 918 of 1000 on page 1\n",
      "Processing 919 of 1000 on page 1\n",
      "Processing 920 of 1000 on page 1\n",
      "Processing 921 of 1000 on page 1\n",
      "Processing 922 of 1000 on page 1\n",
      "Processing 923 of 1000 on page 1\n",
      "Processing 924 of 1000 on page 1\n",
      "Processing 925 of 1000 on page 1\n",
      "Processing 926 of 1000 on page 1\n",
      "Processing 927 of 1000 on page 1\n",
      "Processing 928 of 1000 on page 1\n",
      "Processing 929 of 1000 on page 1\n",
      "Processing 930 of 1000 on page 1\n",
      "Processing 931 of 1000 on page 1\n",
      "Processing 932 of 1000 on page 1\n",
      "Processing 933 of 1000 on page 1\n",
      "Processing 934 of 1000 on page 1\n",
      "Processing 935 of 1000 on page 1\n",
      "Processing 936 of 1000 on page 1\n",
      "Processing 937 of 1000 on page 1\n",
      "Processing 938 of 1000 on page 1\n",
      "Processing 939 of 1000 on page 1\n",
      "Processing 940 of 1000 on page 1\n",
      "Processing 941 of 1000 on page 1\n",
      "Processing 942 of 1000 on page 1\n",
      "Processing 943 of 1000 on page 1\n",
      "Processing 944 of 1000 on page 1\n",
      "Processing 945 of 1000 on page 1\n",
      "Processing 946 of 1000 on page 1\n",
      "Processing 947 of 1000 on page 1\n",
      "Processing 948 of 1000 on page 1\n",
      "Processing 949 of 1000 on page 1\n",
      "Processing 950 of 1000 on page 1\n",
      "Processing 951 of 1000 on page 1\n",
      "Processing 952 of 1000 on page 1\n",
      "Processing 953 of 1000 on page 1\n",
      "Processing 954 of 1000 on page 1\n",
      "Processing 955 of 1000 on page 1\n",
      "Processing 956 of 1000 on page 1\n",
      "Processing 957 of 1000 on page 1\n",
      "Processing 958 of 1000 on page 1\n",
      "Processing 959 of 1000 on page 1\n",
      "Processing 960 of 1000 on page 1\n",
      "Processing 961 of 1000 on page 1\n",
      "Processing 962 of 1000 on page 1\n",
      "Processing 963 of 1000 on page 1\n",
      "Processing 964 of 1000 on page 1\n",
      "Processing 965 of 1000 on page 1\n",
      "Processing 966 of 1000 on page 1\n",
      "Processing 967 of 1000 on page 1\n",
      "Processing 968 of 1000 on page 1\n",
      "Processing 969 of 1000 on page 1\n",
      "Processing 970 of 1000 on page 1\n",
      "Processing 971 of 1000 on page 1\n",
      "Processing 972 of 1000 on page 1\n",
      "Processing 973 of 1000 on page 1\n",
      "Processing 974 of 1000 on page 1\n",
      "Processing 975 of 1000 on page 1\n",
      "Processing 976 of 1000 on page 1\n",
      "Processing 977 of 1000 on page 1\n",
      "Processing 978 of 1000 on page 1\n",
      "Processing 979 of 1000 on page 1\n",
      "Processing 980 of 1000 on page 1\n",
      "Processing 981 of 1000 on page 1\n",
      "Processing 982 of 1000 on page 1\n",
      "Processing 983 of 1000 on page 1\n",
      "Processing 984 of 1000 on page 1\n",
      "Processing 985 of 1000 on page 1\n",
      "Processing 986 of 1000 on page 1\n",
      "Processing 987 of 1000 on page 1\n",
      "Processing 988 of 1000 on page 1\n",
      "Processing 989 of 1000 on page 1\n",
      "Processing 990 of 1000 on page 1\n",
      "Processing 991 of 1000 on page 1\n",
      "Processing 992 of 1000 on page 1\n",
      "Processing 993 of 1000 on page 1\n",
      "Processing 994 of 1000 on page 1\n",
      "Processing 995 of 1000 on page 1\n",
      "Processing 996 of 1000 on page 1\n",
      "Processing 997 of 1000 on page 1\n",
      "Processing 998 of 1000 on page 1\n",
      "Processing 999 of 1000 on page 1\n",
      "Processing 1000 of 1000 on page 1\n",
      "Processing 1 of 1000 on page 2\n",
      "Processing 2 of 1000 on page 2\n",
      "Processing 3 of 1000 on page 2\n",
      "Processing 4 of 1000 on page 2\n",
      "Processing 5 of 1000 on page 2\n",
      "Processing 6 of 1000 on page 2\n",
      "Processing 7 of 1000 on page 2\n",
      "Processing 8 of 1000 on page 2\n",
      "Processing 9 of 1000 on page 2\n",
      "Processing 10 of 1000 on page 2\n",
      "Processing 11 of 1000 on page 2\n",
      "Processing 12 of 1000 on page 2\n",
      "Processing 13 of 1000 on page 2\n",
      "Processing 14 of 1000 on page 2\n",
      "Processing 15 of 1000 on page 2\n",
      "Processing 16 of 1000 on page 2\n",
      "Processing 17 of 1000 on page 2\n",
      "Processing 18 of 1000 on page 2\n",
      "Processing 19 of 1000 on page 2\n",
      "Processing 20 of 1000 on page 2\n",
      "Processing 21 of 1000 on page 2\n",
      "Processing 22 of 1000 on page 2\n",
      "Processing 23 of 1000 on page 2\n",
      "Processing 24 of 1000 on page 2\n",
      "Processing 25 of 1000 on page 2\n",
      "Processing 26 of 1000 on page 2\n",
      "Processing 27 of 1000 on page 2\n",
      "Processing 28 of 1000 on page 2\n",
      "Processing 29 of 1000 on page 2\n",
      "Processing 30 of 1000 on page 2\n",
      "Processing 31 of 1000 on page 2\n",
      "Processing 32 of 1000 on page 2\n",
      "Processing 33 of 1000 on page 2\n",
      "Processing 34 of 1000 on page 2\n",
      "Processing 35 of 1000 on page 2\n",
      "Processing 36 of 1000 on page 2\n",
      "Processing 37 of 1000 on page 2\n",
      "Processing 38 of 1000 on page 2\n",
      "Processing 39 of 1000 on page 2\n",
      "Processing 40 of 1000 on page 2\n",
      "Processing 41 of 1000 on page 2\n",
      "Processing 42 of 1000 on page 2\n",
      "Processing 43 of 1000 on page 2\n",
      "Processing 44 of 1000 on page 2\n",
      "Processing 45 of 1000 on page 2\n",
      "Processing 46 of 1000 on page 2\n",
      "Processing 47 of 1000 on page 2\n",
      "Processing 48 of 1000 on page 2\n",
      "Processing 49 of 1000 on page 2\n",
      "Processing 50 of 1000 on page 2\n",
      "Processing 51 of 1000 on page 2\n",
      "Processing 52 of 1000 on page 2\n",
      "Processing 53 of 1000 on page 2\n",
      "Processing 54 of 1000 on page 2\n",
      "Processing 55 of 1000 on page 2\n",
      "Processing 56 of 1000 on page 2\n",
      "Processing 57 of 1000 on page 2\n",
      "Processing 58 of 1000 on page 2\n",
      "Processing 59 of 1000 on page 2\n",
      "Processing 60 of 1000 on page 2\n",
      "Processing 61 of 1000 on page 2\n",
      "Processing 62 of 1000 on page 2\n",
      "Processing 63 of 1000 on page 2\n",
      "Processing 64 of 1000 on page 2\n",
      "Processing 65 of 1000 on page 2\n",
      "Processing 66 of 1000 on page 2\n",
      "Processing 67 of 1000 on page 2\n",
      "Processing 68 of 1000 on page 2\n",
      "Processing 69 of 1000 on page 2\n",
      "Processing 70 of 1000 on page 2\n",
      "Processing 71 of 1000 on page 2\n",
      "Processing 72 of 1000 on page 2\n",
      "Processing 73 of 1000 on page 2\n",
      "Processing 74 of 1000 on page 2\n",
      "Processing 75 of 1000 on page 2\n",
      "Processing 76 of 1000 on page 2\n",
      "Processing 77 of 1000 on page 2\n",
      "Processing 78 of 1000 on page 2\n",
      "Processing 79 of 1000 on page 2\n",
      "Processing 80 of 1000 on page 2\n",
      "Processing 81 of 1000 on page 2\n",
      "Processing 82 of 1000 on page 2\n",
      "Processing 83 of 1000 on page 2\n",
      "Processing 84 of 1000 on page 2\n",
      "Processing 85 of 1000 on page 2\n",
      "Processing 86 of 1000 on page 2\n",
      "Processing 87 of 1000 on page 2\n",
      "Processing 88 of 1000 on page 2\n",
      "Processing 89 of 1000 on page 2\n",
      "Processing 90 of 1000 on page 2\n",
      "Processing 91 of 1000 on page 2\n",
      "Processing 92 of 1000 on page 2\n",
      "Processing 93 of 1000 on page 2\n",
      "Processing 94 of 1000 on page 2\n",
      "Processing 95 of 1000 on page 2\n",
      "Processing 96 of 1000 on page 2\n",
      "Processing 97 of 1000 on page 2\n",
      "Processing 98 of 1000 on page 2\n",
      "Processing 99 of 1000 on page 2\n",
      "Processing 100 of 1000 on page 2\n",
      "Processing 101 of 1000 on page 2\n",
      "Processing 102 of 1000 on page 2\n",
      "Processing 103 of 1000 on page 2\n",
      "Processing 104 of 1000 on page 2\n",
      "Processing 105 of 1000 on page 2\n",
      "Processing 106 of 1000 on page 2\n",
      "Processing 107 of 1000 on page 2\n",
      "Processing 108 of 1000 on page 2\n",
      "Processing 109 of 1000 on page 2\n",
      "Processing 110 of 1000 on page 2\n",
      "Processing 111 of 1000 on page 2\n",
      "Processing 112 of 1000 on page 2\n",
      "Processing 113 of 1000 on page 2\n",
      "Processing 114 of 1000 on page 2\n",
      "Processing 115 of 1000 on page 2\n",
      "Processing 116 of 1000 on page 2\n",
      "Processing 117 of 1000 on page 2\n",
      "Processing 118 of 1000 on page 2\n",
      "Processing 119 of 1000 on page 2\n",
      "Processing 120 of 1000 on page 2\n",
      "Processing 121 of 1000 on page 2\n",
      "Processing 122 of 1000 on page 2\n",
      "Processing 123 of 1000 on page 2\n",
      "Processing 124 of 1000 on page 2\n",
      "Processing 125 of 1000 on page 2\n",
      "Processing 126 of 1000 on page 2\n",
      "Processing 127 of 1000 on page 2\n",
      "Processing 128 of 1000 on page 2\n",
      "Processing 129 of 1000 on page 2\n",
      "Processing 130 of 1000 on page 2\n",
      "Processing 131 of 1000 on page 2\n",
      "Processing 132 of 1000 on page 2\n",
      "Processing 133 of 1000 on page 2\n",
      "Processing 134 of 1000 on page 2\n",
      "Processing 135 of 1000 on page 2\n",
      "Processing 136 of 1000 on page 2\n",
      "Processing 137 of 1000 on page 2\n",
      "Processing 138 of 1000 on page 2\n",
      "Processing 139 of 1000 on page 2\n",
      "Processing 140 of 1000 on page 2\n",
      "Processing 141 of 1000 on page 2\n",
      "Processing 142 of 1000 on page 2\n",
      "Processing 143 of 1000 on page 2\n",
      "Processing 144 of 1000 on page 2\n",
      "Processing 145 of 1000 on page 2\n",
      "Processing 146 of 1000 on page 2\n",
      "Processing 147 of 1000 on page 2\n",
      "Processing 148 of 1000 on page 2\n",
      "Processing 149 of 1000 on page 2\n",
      "Processing 150 of 1000 on page 2\n",
      "Processing 151 of 1000 on page 2\n",
      "Processing 152 of 1000 on page 2\n",
      "Processing 153 of 1000 on page 2\n",
      "Processing 154 of 1000 on page 2\n",
      "Processing 155 of 1000 on page 2\n",
      "Processing 156 of 1000 on page 2\n",
      "Processing 157 of 1000 on page 2\n",
      "Processing 158 of 1000 on page 2\n",
      "Processing 159 of 1000 on page 2\n",
      "Processing 160 of 1000 on page 2\n",
      "Processing 161 of 1000 on page 2\n",
      "Processing 162 of 1000 on page 2\n",
      "Processing 163 of 1000 on page 2\n",
      "Processing 164 of 1000 on page 2\n",
      "Processing 165 of 1000 on page 2\n",
      "Processing 166 of 1000 on page 2\n",
      "Processing 167 of 1000 on page 2\n",
      "Processing 168 of 1000 on page 2\n",
      "Processing 169 of 1000 on page 2\n",
      "Processing 170 of 1000 on page 2\n",
      "Processing 171 of 1000 on page 2\n",
      "Processing 172 of 1000 on page 2\n",
      "Processing 173 of 1000 on page 2\n",
      "Processing 174 of 1000 on page 2\n",
      "Processing 175 of 1000 on page 2\n",
      "Processing 176 of 1000 on page 2\n",
      "Processing 177 of 1000 on page 2\n",
      "Processing 178 of 1000 on page 2\n",
      "Processing 179 of 1000 on page 2\n",
      "Processing 180 of 1000 on page 2\n",
      "Processing 181 of 1000 on page 2\n",
      "Processing 182 of 1000 on page 2\n",
      "Processing 183 of 1000 on page 2\n",
      "Processing 184 of 1000 on page 2\n",
      "Processing 185 of 1000 on page 2\n",
      "Processing 186 of 1000 on page 2\n",
      "Processing 187 of 1000 on page 2\n",
      "Processing 188 of 1000 on page 2\n",
      "Processing 189 of 1000 on page 2\n",
      "Processing 190 of 1000 on page 2\n",
      "Processing 191 of 1000 on page 2\n",
      "Processing 192 of 1000 on page 2\n",
      "Processing 193 of 1000 on page 2\n",
      "Processing 194 of 1000 on page 2\n",
      "Processing 195 of 1000 on page 2\n",
      "Processing 196 of 1000 on page 2\n",
      "Processing 197 of 1000 on page 2\n",
      "Processing 198 of 1000 on page 2\n",
      "Processing 199 of 1000 on page 2\n",
      "Processing 200 of 1000 on page 2\n",
      "Processing 201 of 1000 on page 2\n",
      "Processing 202 of 1000 on page 2\n",
      "Processing 203 of 1000 on page 2\n",
      "Processing 204 of 1000 on page 2\n",
      "Processing 205 of 1000 on page 2\n",
      "Processing 206 of 1000 on page 2\n",
      "Processing 207 of 1000 on page 2\n",
      "Processing 208 of 1000 on page 2\n",
      "Processing 209 of 1000 on page 2\n",
      "Processing 210 of 1000 on page 2\n",
      "Processing 211 of 1000 on page 2\n",
      "Processing 212 of 1000 on page 2\n",
      "Processing 213 of 1000 on page 2\n",
      "Processing 214 of 1000 on page 2\n",
      "Processing 215 of 1000 on page 2\n",
      "Processing 216 of 1000 on page 2\n",
      "Processing 217 of 1000 on page 2\n",
      "Processing 218 of 1000 on page 2\n",
      "Processing 219 of 1000 on page 2\n",
      "Processing 220 of 1000 on page 2\n",
      "Processing 221 of 1000 on page 2\n",
      "Processing 222 of 1000 on page 2\n",
      "Processing 223 of 1000 on page 2\n",
      "Processing 224 of 1000 on page 2\n",
      "Processing 225 of 1000 on page 2\n",
      "Processing 226 of 1000 on page 2\n",
      "Processing 227 of 1000 on page 2\n",
      "Processing 228 of 1000 on page 2\n",
      "Processing 229 of 1000 on page 2\n",
      "Processing 230 of 1000 on page 2\n",
      "Processing 231 of 1000 on page 2\n",
      "Processing 232 of 1000 on page 2\n",
      "Processing 233 of 1000 on page 2\n",
      "Processing 234 of 1000 on page 2\n",
      "Processing 235 of 1000 on page 2\n",
      "Processing 236 of 1000 on page 2\n",
      "Processing 237 of 1000 on page 2\n",
      "Processing 238 of 1000 on page 2\n",
      "Processing 239 of 1000 on page 2\n",
      "Processing 240 of 1000 on page 2\n",
      "Processing 241 of 1000 on page 2\n",
      "Processing 242 of 1000 on page 2\n",
      "Processing 243 of 1000 on page 2\n",
      "Processing 244 of 1000 on page 2\n",
      "Processing 245 of 1000 on page 2\n",
      "Processing 246 of 1000 on page 2\n",
      "Processing 247 of 1000 on page 2\n",
      "Processing 248 of 1000 on page 2\n",
      "Processing 249 of 1000 on page 2\n",
      "Processing 250 of 1000 on page 2\n",
      "Processing 251 of 1000 on page 2\n",
      "Processing 252 of 1000 on page 2\n",
      "Processing 253 of 1000 on page 2\n",
      "Processing 254 of 1000 on page 2\n",
      "Processing 255 of 1000 on page 2\n",
      "Processing 256 of 1000 on page 2\n",
      "Processing 257 of 1000 on page 2\n",
      "Processing 258 of 1000 on page 2\n",
      "Processing 259 of 1000 on page 2\n",
      "Processing 260 of 1000 on page 2\n",
      "Processing 261 of 1000 on page 2\n",
      "Processing 262 of 1000 on page 2\n",
      "Processing 263 of 1000 on page 2\n",
      "Processing 264 of 1000 on page 2\n",
      "Processing 265 of 1000 on page 2\n",
      "Processing 266 of 1000 on page 2\n",
      "Processing 267 of 1000 on page 2\n",
      "Processing 268 of 1000 on page 2\n",
      "Processing 269 of 1000 on page 2\n",
      "Processing 270 of 1000 on page 2\n",
      "Processing 271 of 1000 on page 2\n",
      "Processing 272 of 1000 on page 2\n",
      "Processing 273 of 1000 on page 2\n",
      "Processing 274 of 1000 on page 2\n",
      "Processing 275 of 1000 on page 2\n",
      "Processing 276 of 1000 on page 2\n",
      "Processing 277 of 1000 on page 2\n",
      "Processing 278 of 1000 on page 2\n",
      "Processing 279 of 1000 on page 2\n",
      "Processing 280 of 1000 on page 2\n",
      "Processing 281 of 1000 on page 2\n",
      "Processing 282 of 1000 on page 2\n",
      "Processing 283 of 1000 on page 2\n",
      "Processing 284 of 1000 on page 2\n",
      "Processing 285 of 1000 on page 2\n",
      "Processing 286 of 1000 on page 2\n",
      "Processing 287 of 1000 on page 2\n",
      "Processing 288 of 1000 on page 2\n",
      "Processing 289 of 1000 on page 2\n",
      "Processing 290 of 1000 on page 2\n",
      "Processing 291 of 1000 on page 2\n",
      "Processing 292 of 1000 on page 2\n",
      "Processing 293 of 1000 on page 2\n",
      "Processing 294 of 1000 on page 2\n",
      "Processing 295 of 1000 on page 2\n",
      "Processing 296 of 1000 on page 2\n",
      "Processing 297 of 1000 on page 2\n",
      "Processing 298 of 1000 on page 2\n",
      "Processing 299 of 1000 on page 2\n",
      "Processing 300 of 1000 on page 2\n",
      "Processing 301 of 1000 on page 2\n",
      "Processing 302 of 1000 on page 2\n",
      "Processing 303 of 1000 on page 2\n",
      "Processing 304 of 1000 on page 2\n",
      "Processing 305 of 1000 on page 2\n",
      "******************************************\n",
      "Error encountered on fim_4_3_11_0/hand_datasets/11110202/branches/0/usgs_elev_table.csv\n",
      "(psycopg2.OperationalError) terminating connection due to administrator command\n",
      "SSL connection has been closed unexpectedly\n",
      "\n",
      "[SQL: INSERT INTO derived.hydrotable (\"HydroID\", feature_id, stage, discharge_cms, fim_version) VALUES (%(HydroID)s, %(feature_id)s, %(stage)s, %(discharge_cms)s, %(fim_version)s)]\n",
      "[parameters: ({'HydroID': 23640293, 'feature_id': 7766271, 'stage': 0.0, 'discharge_cms': 0.0, 'fim_version': '4.3.11.0'}, {'HydroID': 23640293, 'feature_id': 7766271, 'stage': 0.3048, 'discharge_cms': 5.5280675852020265, 'fim_version': '4.3.11.0'}, {'HydroID': 23640293, 'feature_id': 7766271, 'stage': 0.6096, 'discharge_cms': 19.443046606891567, 'fim_version': '4.3.11.0'}, {'HydroID': 23640293, 'feature_id': 7766271, 'stage': 0.9144, 'discharge_cms': 40.309052079369295, 'fim_version': '4.3.11.0'}, {'HydroID': 23640293, 'feature_id': 7766271, 'stage': 1.2192, 'discharge_cms': 67.09860797437516, 'fim_version': '4.3.11.0'}, {'HydroID': 23640293, 'feature_id': 7766271, 'stage': 1.524, 'discharge_cms': 102.27819286827841, 'fim_version': '4.3.11.0'}, {'HydroID': 23640293, 'feature_id': 7766271, 'stage': 1.8288, 'discharge_cms': 143.1335960623137, 'fim_version': '4.3.11.0'}, {'HydroID': 23640293, 'feature_id': 7766271, 'stage': 2.1336, 'discharge_cms': 188.57953455954376, 'fim_version': '4.3.11.0'}  ... displaying 10 of 84 total bound parameter sets ...  {'HydroID': 23640293, 'feature_id': 7766271, 'stage': 24.9936, 'discharge_cms': 63482.046478493074, 'fim_version': '4.3.11.0'}, {'HydroID': 23640293, 'feature_id': 7766271, 'stage': 25.2984, 'discharge_cms': 65288.83623138938, 'fim_version': '4.3.11.0'})]\n",
      "(Background on this error at: http://sqlalche.me/e/13/e3q8)\n",
      "******************************************\n",
      "Processing 306 of 1000 on page 2\n",
      "******************************************\n",
      "Error encountered on fim_4_3_11_0/hand_datasets/11110203/branches/0/usgs_elev_table.csv\n",
      "(psycopg2.OperationalError) could not connect to server: Connection refused\n",
      "\tIs the server running on host \"hydrovis-ti-viz-processing.c4vzypepnkx3.us-east-1.rds.amazonaws.com\" (10.26.150.24) and accepting\n",
      "\tTCP/IP connections on port 5432?\n",
      "\n",
      "(Background on this error at: http://sqlalche.me/e/13/e3q8)\n",
      "******************************************\n",
      "Processing 307 of 1000 on page 2\n",
      "Processing 308 of 1000 on page 2\n",
      "Processing 309 of 1000 on page 2\n",
      "Processing 310 of 1000 on page 2\n",
      "Processing 311 of 1000 on page 2\n",
      "Processing 312 of 1000 on page 2\n",
      "Processing 313 of 1000 on page 2\n",
      "Processing 314 of 1000 on page 2\n",
      "Processing 315 of 1000 on page 2\n",
      "Processing 316 of 1000 on page 2\n",
      "Processing 317 of 1000 on page 2\n",
      "Processing 318 of 1000 on page 2\n",
      "Processing 319 of 1000 on page 2\n",
      "Processing 320 of 1000 on page 2\n",
      "Processing 321 of 1000 on page 2\n",
      "Processing 322 of 1000 on page 2\n",
      "Processing 323 of 1000 on page 2\n",
      "Processing 324 of 1000 on page 2\n",
      "Processing 325 of 1000 on page 2\n",
      "Processing 326 of 1000 on page 2\n",
      "Processing 327 of 1000 on page 2\n",
      "Processing 328 of 1000 on page 2\n",
      "Processing 329 of 1000 on page 2\n",
      "Processing 330 of 1000 on page 2\n",
      "Processing 331 of 1000 on page 2\n",
      "Processing 332 of 1000 on page 2\n",
      "Processing 333 of 1000 on page 2\n",
      "Processing 334 of 1000 on page 2\n",
      "Processing 335 of 1000 on page 2\n",
      "Processing 336 of 1000 on page 2\n",
      "Processing 337 of 1000 on page 2\n",
      "Processing 338 of 1000 on page 2\n",
      "Processing 339 of 1000 on page 2\n",
      "Processing 340 of 1000 on page 2\n",
      "Processing 341 of 1000 on page 2\n",
      "Processing 342 of 1000 on page 2\n",
      "Processing 343 of 1000 on page 2\n",
      "Processing 344 of 1000 on page 2\n",
      "Processing 345 of 1000 on page 2\n",
      "Processing 346 of 1000 on page 2\n",
      "Processing 347 of 1000 on page 2\n",
      "Processing 348 of 1000 on page 2\n",
      "Processing 349 of 1000 on page 2\n",
      "Processing 350 of 1000 on page 2\n",
      "Processing 351 of 1000 on page 2\n",
      "Processing 352 of 1000 on page 2\n",
      "Processing 353 of 1000 on page 2\n",
      "Processing 354 of 1000 on page 2\n",
      "Processing 355 of 1000 on page 2\n",
      "Processing 356 of 1000 on page 2\n",
      "Processing 357 of 1000 on page 2\n",
      "Processing 358 of 1000 on page 2\n",
      "Processing 359 of 1000 on page 2\n",
      "Processing 360 of 1000 on page 2\n",
      "Processing 361 of 1000 on page 2\n",
      "Processing 362 of 1000 on page 2\n",
      "Processing 363 of 1000 on page 2\n",
      "Processing 364 of 1000 on page 2\n",
      "Processing 365 of 1000 on page 2\n",
      "Processing 366 of 1000 on page 2\n",
      "Processing 367 of 1000 on page 2\n",
      "Processing 368 of 1000 on page 2\n",
      "Processing 369 of 1000 on page 2\n",
      "Processing 370 of 1000 on page 2\n",
      "Processing 371 of 1000 on page 2\n",
      "Processing 372 of 1000 on page 2\n",
      "Processing 373 of 1000 on page 2\n",
      "Processing 374 of 1000 on page 2\n",
      "Processing 375 of 1000 on page 2\n",
      "Processing 376 of 1000 on page 2\n",
      "Processing 377 of 1000 on page 2\n",
      "Processing 378 of 1000 on page 2\n",
      "Processing 379 of 1000 on page 2\n",
      "Processing 380 of 1000 on page 2\n",
      "Processing 381 of 1000 on page 2\n",
      "Processing 382 of 1000 on page 2\n",
      "Processing 383 of 1000 on page 2\n",
      "Processing 384 of 1000 on page 2\n",
      "Processing 385 of 1000 on page 2\n",
      "Processing 386 of 1000 on page 2\n",
      "Processing 387 of 1000 on page 2\n",
      "Processing 388 of 1000 on page 2\n",
      "Processing 389 of 1000 on page 2\n",
      "Processing 390 of 1000 on page 2\n",
      "Processing 391 of 1000 on page 2\n",
      "Processing 392 of 1000 on page 2\n",
      "Processing 393 of 1000 on page 2\n",
      "Processing 394 of 1000 on page 2\n",
      "Processing 395 of 1000 on page 2\n",
      "Processing 396 of 1000 on page 2\n",
      "Processing 397 of 1000 on page 2\n",
      "Processing 398 of 1000 on page 2\n",
      "Processing 399 of 1000 on page 2\n",
      "Processing 400 of 1000 on page 2\n",
      "Processing 401 of 1000 on page 2\n",
      "Processing 402 of 1000 on page 2\n",
      "Processing 403 of 1000 on page 2\n",
      "Processing 404 of 1000 on page 2\n",
      "Processing 405 of 1000 on page 2\n",
      "Processing 406 of 1000 on page 2\n",
      "Processing 407 of 1000 on page 2\n",
      "Processing 408 of 1000 on page 2\n",
      "Processing 409 of 1000 on page 2\n",
      "Processing 410 of 1000 on page 2\n",
      "Processing 411 of 1000 on page 2\n",
      "Processing 412 of 1000 on page 2\n",
      "Processing 413 of 1000 on page 2\n",
      "Processing 414 of 1000 on page 2\n",
      "Processing 415 of 1000 on page 2\n",
      "Processing 416 of 1000 on page 2\n",
      "Processing 417 of 1000 on page 2\n",
      "Processing 418 of 1000 on page 2\n",
      "Processing 419 of 1000 on page 2\n",
      "Processing 420 of 1000 on page 2\n",
      "Processing 421 of 1000 on page 2\n",
      "Processing 422 of 1000 on page 2\n",
      "Processing 423 of 1000 on page 2\n",
      "Processing 424 of 1000 on page 2\n",
      "Processing 425 of 1000 on page 2\n",
      "Processing 426 of 1000 on page 2\n",
      "Processing 427 of 1000 on page 2\n",
      "Processing 428 of 1000 on page 2\n",
      "Processing 429 of 1000 on page 2\n",
      "Processing 430 of 1000 on page 2\n",
      "Processing 431 of 1000 on page 2\n",
      "Processing 432 of 1000 on page 2\n",
      "Processing 433 of 1000 on page 2\n",
      "Processing 434 of 1000 on page 2\n",
      "Processing 435 of 1000 on page 2\n",
      "Processing 436 of 1000 on page 2\n",
      "Processing 437 of 1000 on page 2\n",
      "Processing 438 of 1000 on page 2\n",
      "Processing 439 of 1000 on page 2\n",
      "Processing 440 of 1000 on page 2\n",
      "Processing 441 of 1000 on page 2\n",
      "Processing 442 of 1000 on page 2\n",
      "Processing 443 of 1000 on page 2\n",
      "Processing 444 of 1000 on page 2\n",
      "Processing 445 of 1000 on page 2\n",
      "Processing 446 of 1000 on page 2\n",
      "Processing 447 of 1000 on page 2\n",
      "Processing 448 of 1000 on page 2\n",
      "Processing 449 of 1000 on page 2\n",
      "Processing 450 of 1000 on page 2\n",
      "Processing 451 of 1000 on page 2\n",
      "Processing 452 of 1000 on page 2\n",
      "Processing 453 of 1000 on page 2\n",
      "Processing 454 of 1000 on page 2\n",
      "Processing 455 of 1000 on page 2\n",
      "Processing 456 of 1000 on page 2\n",
      "Processing 457 of 1000 on page 2\n",
      "Processing 458 of 1000 on page 2\n",
      "Processing 459 of 1000 on page 2\n",
      "Processing 460 of 1000 on page 2\n",
      "Processing 461 of 1000 on page 2\n",
      "Processing 462 of 1000 on page 2\n",
      "Processing 463 of 1000 on page 2\n",
      "Processing 464 of 1000 on page 2\n",
      "Processing 465 of 1000 on page 2\n",
      "Processing 466 of 1000 on page 2\n",
      "Processing 467 of 1000 on page 2\n",
      "Processing 468 of 1000 on page 2\n",
      "Processing 469 of 1000 on page 2\n",
      "Processing 470 of 1000 on page 2\n",
      "Processing 471 of 1000 on page 2\n",
      "Processing 472 of 1000 on page 2\n",
      "Processing 473 of 1000 on page 2\n",
      "Processing 474 of 1000 on page 2\n",
      "Processing 475 of 1000 on page 2\n",
      "Processing 476 of 1000 on page 2\n",
      "Processing 477 of 1000 on page 2\n",
      "Processing 478 of 1000 on page 2\n",
      "Processing 479 of 1000 on page 2\n",
      "Processing 480 of 1000 on page 2\n",
      "Processing 481 of 1000 on page 2\n",
      "Processing 482 of 1000 on page 2\n",
      "Processing 483 of 1000 on page 2\n",
      "Processing 484 of 1000 on page 2\n",
      "Processing 485 of 1000 on page 2\n",
      "Processing 486 of 1000 on page 2\n",
      "Processing 487 of 1000 on page 2\n",
      "Processing 488 of 1000 on page 2\n",
      "Processing 489 of 1000 on page 2\n",
      "Processing 490 of 1000 on page 2\n",
      "Processing 491 of 1000 on page 2\n",
      "Processing 492 of 1000 on page 2\n",
      "Processing 493 of 1000 on page 2\n",
      "Processing 494 of 1000 on page 2\n",
      "Processing 495 of 1000 on page 2\n",
      "Processing 496 of 1000 on page 2\n",
      "Processing 497 of 1000 on page 2\n",
      "Processing 498 of 1000 on page 2\n",
      "Processing 499 of 1000 on page 2\n",
      "Processing 500 of 1000 on page 2\n",
      "Processing 501 of 1000 on page 2\n",
      "Processing 502 of 1000 on page 2\n",
      "Processing 503 of 1000 on page 2\n",
      "Processing 504 of 1000 on page 2\n",
      "Processing 505 of 1000 on page 2\n",
      "Processing 506 of 1000 on page 2\n",
      "Processing 507 of 1000 on page 2\n",
      "Processing 508 of 1000 on page 2\n",
      "Processing 509 of 1000 on page 2\n",
      "Processing 510 of 1000 on page 2\n",
      "Processing 511 of 1000 on page 2\n",
      "Processing 512 of 1000 on page 2\n",
      "Processing 513 of 1000 on page 2\n",
      "Processing 514 of 1000 on page 2\n",
      "Processing 515 of 1000 on page 2\n",
      "Processing 516 of 1000 on page 2\n",
      "Processing 517 of 1000 on page 2\n",
      "Processing 518 of 1000 on page 2\n",
      "Processing 519 of 1000 on page 2\n",
      "Processing 520 of 1000 on page 2\n",
      "Processing 521 of 1000 on page 2\n",
      "Processing 522 of 1000 on page 2\n",
      "Processing 523 of 1000 on page 2\n",
      "Processing 524 of 1000 on page 2\n",
      "Processing 525 of 1000 on page 2\n",
      "Processing 526 of 1000 on page 2\n",
      "Processing 527 of 1000 on page 2\n",
      "Processing 528 of 1000 on page 2\n",
      "Processing 529 of 1000 on page 2\n",
      "Processing 530 of 1000 on page 2\n",
      "Processing 531 of 1000 on page 2\n",
      "Processing 532 of 1000 on page 2\n",
      "Processing 533 of 1000 on page 2\n",
      "Processing 534 of 1000 on page 2\n",
      "Processing 535 of 1000 on page 2\n",
      "Processing 536 of 1000 on page 2\n",
      "Processing 537 of 1000 on page 2\n",
      "Processing 538 of 1000 on page 2\n",
      "Processing 539 of 1000 on page 2\n",
      "Processing 540 of 1000 on page 2\n",
      "Processing 541 of 1000 on page 2\n",
      "Processing 542 of 1000 on page 2\n",
      "Processing 543 of 1000 on page 2\n",
      "Processing 544 of 1000 on page 2\n",
      "Processing 545 of 1000 on page 2\n",
      "Processing 546 of 1000 on page 2\n",
      "Processing 547 of 1000 on page 2\n",
      "Processing 548 of 1000 on page 2\n",
      "Processing 549 of 1000 on page 2\n",
      "Processing 550 of 1000 on page 2\n",
      "Processing 551 of 1000 on page 2\n",
      "Processing 552 of 1000 on page 2\n",
      "Processing 553 of 1000 on page 2\n",
      "Processing 554 of 1000 on page 2\n",
      "Processing 555 of 1000 on page 2\n",
      "Processing 556 of 1000 on page 2\n",
      "Processing 557 of 1000 on page 2\n",
      "Processing 558 of 1000 on page 2\n",
      "Processing 559 of 1000 on page 2\n",
      "Processing 560 of 1000 on page 2\n",
      "Processing 561 of 1000 on page 2\n",
      "Processing 562 of 1000 on page 2\n",
      "Processing 563 of 1000 on page 2\n",
      "Processing 564 of 1000 on page 2\n",
      "Processing 565 of 1000 on page 2\n",
      "Processing 566 of 1000 on page 2\n",
      "Processing 567 of 1000 on page 2\n",
      "Processing 568 of 1000 on page 2\n",
      "Processing 569 of 1000 on page 2\n",
      "Processing 570 of 1000 on page 2\n",
      "Processing 571 of 1000 on page 2\n",
      "Processing 572 of 1000 on page 2\n",
      "Processing 573 of 1000 on page 2\n",
      "Processing 574 of 1000 on page 2\n",
      "Processing 575 of 1000 on page 2\n",
      "Processing 576 of 1000 on page 2\n",
      "Processing 577 of 1000 on page 2\n",
      "Processing 578 of 1000 on page 2\n",
      "Processing 579 of 1000 on page 2\n",
      "Processing 580 of 1000 on page 2\n",
      "Processing 581 of 1000 on page 2\n",
      "Processing 582 of 1000 on page 2\n",
      "Processing 583 of 1000 on page 2\n",
      "Processing 584 of 1000 on page 2\n",
      "Processing 585 of 1000 on page 2\n",
      "Processing 586 of 1000 on page 2\n",
      "Processing 587 of 1000 on page 2\n",
      "Processing 588 of 1000 on page 2\n",
      "Processing 589 of 1000 on page 2\n",
      "Processing 590 of 1000 on page 2\n",
      "Processing 591 of 1000 on page 2\n",
      "Processing 592 of 1000 on page 2\n",
      "Processing 593 of 1000 on page 2\n",
      "Processing 594 of 1000 on page 2\n",
      "Processing 595 of 1000 on page 2\n",
      "Processing 596 of 1000 on page 2\n",
      "Processing 597 of 1000 on page 2\n",
      "Processing 598 of 1000 on page 2\n",
      "Processing 599 of 1000 on page 2\n",
      "Processing 600 of 1000 on page 2\n",
      "Processing 601 of 1000 on page 2\n",
      "Processing 602 of 1000 on page 2\n",
      "Processing 603 of 1000 on page 2\n",
      "Processing 604 of 1000 on page 2\n",
      "Processing 605 of 1000 on page 2\n",
      "Processing 606 of 1000 on page 2\n",
      "Processing 607 of 1000 on page 2\n",
      "Processing 608 of 1000 on page 2\n",
      "Processing 609 of 1000 on page 2\n",
      "Processing 610 of 1000 on page 2\n",
      "Processing 611 of 1000 on page 2\n",
      "Processing 612 of 1000 on page 2\n",
      "Processing 613 of 1000 on page 2\n",
      "Processing 614 of 1000 on page 2\n",
      "Processing 615 of 1000 on page 2\n",
      "Processing 616 of 1000 on page 2\n",
      "Processing 617 of 1000 on page 2\n",
      "Processing 618 of 1000 on page 2\n",
      "Processing 619 of 1000 on page 2\n",
      "Processing 620 of 1000 on page 2\n",
      "Processing 621 of 1000 on page 2\n",
      "Processing 622 of 1000 on page 2\n",
      "Processing 623 of 1000 on page 2\n",
      "Processing 624 of 1000 on page 2\n",
      "Processing 625 of 1000 on page 2\n",
      "Processing 626 of 1000 on page 2\n",
      "Processing 627 of 1000 on page 2\n",
      "Processing 628 of 1000 on page 2\n",
      "Processing 629 of 1000 on page 2\n",
      "Processing 630 of 1000 on page 2\n",
      "Processing 631 of 1000 on page 2\n",
      "Processing 632 of 1000 on page 2\n",
      "Processing 633 of 1000 on page 2\n",
      "Processing 634 of 1000 on page 2\n",
      "Processing 635 of 1000 on page 2\n",
      "Processing 636 of 1000 on page 2\n",
      "Processing 637 of 1000 on page 2\n",
      "Processing 638 of 1000 on page 2\n",
      "Processing 639 of 1000 on page 2\n",
      "Processing 640 of 1000 on page 2\n",
      "Processing 641 of 1000 on page 2\n",
      "Processing 642 of 1000 on page 2\n",
      "Processing 643 of 1000 on page 2\n",
      "Processing 644 of 1000 on page 2\n",
      "Processing 645 of 1000 on page 2\n",
      "Processing 646 of 1000 on page 2\n",
      "Processing 647 of 1000 on page 2\n",
      "Processing 648 of 1000 on page 2\n",
      "Processing 649 of 1000 on page 2\n",
      "Processing 650 of 1000 on page 2\n",
      "Processing 651 of 1000 on page 2\n",
      "Processing 652 of 1000 on page 2\n",
      "Processing 653 of 1000 on page 2\n",
      "Processing 654 of 1000 on page 2\n",
      "Processing 655 of 1000 on page 2\n",
      "Processing 656 of 1000 on page 2\n",
      "Processing 657 of 1000 on page 2\n",
      "Processing 658 of 1000 on page 2\n",
      "Processing 659 of 1000 on page 2\n",
      "Processing 660 of 1000 on page 2\n",
      "Processing 661 of 1000 on page 2\n",
      "Processing 662 of 1000 on page 2\n",
      "Processing 663 of 1000 on page 2\n",
      "Processing 664 of 1000 on page 2\n",
      "Processing 665 of 1000 on page 2\n",
      "Processing 666 of 1000 on page 2\n",
      "Processing 667 of 1000 on page 2\n",
      "Processing 668 of 1000 on page 2\n",
      "Processing 669 of 1000 on page 2\n",
      "Processing 670 of 1000 on page 2\n",
      "Processing 671 of 1000 on page 2\n",
      "Processing 672 of 1000 on page 2\n",
      "Processing 673 of 1000 on page 2\n",
      "Processing 674 of 1000 on page 2\n",
      "Processing 675 of 1000 on page 2\n",
      "Processing 676 of 1000 on page 2\n",
      "Processing 677 of 1000 on page 2\n",
      "Processing 678 of 1000 on page 2\n",
      "Processing 679 of 1000 on page 2\n",
      "Processing 680 of 1000 on page 2\n",
      "Processing 681 of 1000 on page 2\n",
      "Processing 682 of 1000 on page 2\n",
      "Processing 683 of 1000 on page 2\n",
      "Processing 684 of 1000 on page 2\n",
      "Processing 685 of 1000 on page 2\n",
      "Processing 686 of 1000 on page 2\n",
      "Processing 687 of 1000 on page 2\n",
      "Processing 688 of 1000 on page 2\n",
      "Processing 689 of 1000 on page 2\n",
      "Processing 690 of 1000 on page 2\n",
      "Processing 691 of 1000 on page 2\n",
      "Processing 692 of 1000 on page 2\n",
      "Processing 693 of 1000 on page 2\n",
      "Processing 694 of 1000 on page 2\n",
      "Processing 695 of 1000 on page 2\n",
      "Processing 696 of 1000 on page 2\n",
      "Processing 697 of 1000 on page 2\n",
      "Processing 698 of 1000 on page 2\n",
      "Processing 699 of 1000 on page 2\n",
      "Processing 700 of 1000 on page 2\n",
      "Processing 701 of 1000 on page 2\n",
      "Processing 702 of 1000 on page 2\n",
      "Processing 703 of 1000 on page 2\n",
      "Processing 704 of 1000 on page 2\n",
      "Processing 705 of 1000 on page 2\n",
      "Processing 706 of 1000 on page 2\n",
      "Processing 707 of 1000 on page 2\n",
      "Processing 708 of 1000 on page 2\n",
      "Processing 709 of 1000 on page 2\n",
      "Processing 710 of 1000 on page 2\n",
      "Processing 711 of 1000 on page 2\n",
      "Processing 712 of 1000 on page 2\n",
      "Processing 713 of 1000 on page 2\n",
      "Processing 714 of 1000 on page 2\n",
      "Processing 715 of 1000 on page 2\n",
      "Processing 716 of 1000 on page 2\n",
      "Processing 717 of 1000 on page 2\n",
      "Processing 718 of 1000 on page 2\n",
      "Processing 719 of 1000 on page 2\n",
      "Processing 720 of 1000 on page 2\n",
      "Processing 721 of 1000 on page 2\n",
      "Processing 722 of 1000 on page 2\n",
      "Processing 723 of 1000 on page 2\n",
      "Processing 724 of 1000 on page 2\n",
      "Processing 725 of 1000 on page 2\n",
      "Processing 726 of 1000 on page 2\n",
      "Processing 727 of 1000 on page 2\n",
      "Processing 728 of 1000 on page 2\n",
      "Processing 729 of 1000 on page 2\n",
      "Processing 730 of 1000 on page 2\n",
      "Processing 731 of 1000 on page 2\n",
      "Processing 732 of 1000 on page 2\n",
      "Processing 733 of 1000 on page 2\n",
      "Processing 734 of 1000 on page 2\n",
      "Processing 735 of 1000 on page 2\n",
      "Processing 736 of 1000 on page 2\n",
      "Processing 737 of 1000 on page 2\n",
      "Processing 738 of 1000 on page 2\n",
      "Processing 739 of 1000 on page 2\n",
      "Processing 740 of 1000 on page 2\n",
      "Processing 741 of 1000 on page 2\n",
      "Processing 742 of 1000 on page 2\n",
      "Processing 743 of 1000 on page 2\n",
      "Processing 744 of 1000 on page 2\n",
      "Processing 745 of 1000 on page 2\n",
      "Processing 746 of 1000 on page 2\n",
      "Processing 747 of 1000 on page 2\n",
      "Processing 748 of 1000 on page 2\n",
      "Processing 749 of 1000 on page 2\n",
      "Processing 750 of 1000 on page 2\n",
      "Processing 751 of 1000 on page 2\n",
      "Processing 752 of 1000 on page 2\n",
      "Processing 753 of 1000 on page 2\n",
      "Processing 754 of 1000 on page 2\n",
      "Processing 755 of 1000 on page 2\n",
      "Processing 756 of 1000 on page 2\n",
      "Processing 757 of 1000 on page 2\n",
      "Processing 758 of 1000 on page 2\n",
      "Processing 759 of 1000 on page 2\n",
      "Processing 760 of 1000 on page 2\n",
      "Processing 761 of 1000 on page 2\n",
      "Processing 762 of 1000 on page 2\n",
      "Processing 763 of 1000 on page 2\n",
      "Processing 764 of 1000 on page 2\n",
      "Processing 765 of 1000 on page 2\n",
      "Processing 766 of 1000 on page 2\n",
      "Processing 767 of 1000 on page 2\n",
      "Processing 768 of 1000 on page 2\n",
      "Processing 769 of 1000 on page 2\n",
      "Processing 770 of 1000 on page 2\n",
      "Processing 771 of 1000 on page 2\n",
      "Processing 772 of 1000 on page 2\n",
      "Processing 773 of 1000 on page 2\n",
      "Processing 774 of 1000 on page 2\n",
      "Processing 775 of 1000 on page 2\n",
      "Processing 776 of 1000 on page 2\n",
      "Processing 777 of 1000 on page 2\n",
      "Processing 778 of 1000 on page 2\n",
      "Processing 779 of 1000 on page 2\n",
      "Processing 780 of 1000 on page 2\n",
      "Processing 781 of 1000 on page 2\n",
      "Processing 782 of 1000 on page 2\n",
      "Processing 783 of 1000 on page 2\n",
      "Processing 784 of 1000 on page 2\n",
      "Processing 785 of 1000 on page 2\n",
      "Processing 786 of 1000 on page 2\n",
      "Processing 787 of 1000 on page 2\n",
      "Processing 788 of 1000 on page 2\n",
      "Processing 789 of 1000 on page 2\n",
      "Processing 790 of 1000 on page 2\n",
      "Processing 791 of 1000 on page 2\n",
      "Processing 792 of 1000 on page 2\n",
      "Processing 793 of 1000 on page 2\n",
      "Processing 794 of 1000 on page 2\n",
      "Processing 795 of 1000 on page 2\n",
      "Processing 796 of 1000 on page 2\n",
      "Processing 797 of 1000 on page 2\n",
      "Processing 798 of 1000 on page 2\n",
      "Processing 799 of 1000 on page 2\n",
      "Processing 800 of 1000 on page 2\n",
      "Processing 801 of 1000 on page 2\n",
      "Processing 802 of 1000 on page 2\n",
      "Processing 803 of 1000 on page 2\n",
      "Processing 804 of 1000 on page 2\n",
      "Processing 805 of 1000 on page 2\n",
      "Processing 806 of 1000 on page 2\n",
      "Processing 807 of 1000 on page 2\n",
      "Processing 808 of 1000 on page 2\n",
      "Processing 809 of 1000 on page 2\n",
      "Processing 810 of 1000 on page 2\n",
      "Processing 811 of 1000 on page 2\n",
      "Processing 812 of 1000 on page 2\n",
      "Processing 813 of 1000 on page 2\n",
      "Processing 814 of 1000 on page 2\n",
      "Processing 815 of 1000 on page 2\n",
      "Processing 816 of 1000 on page 2\n",
      "Processing 817 of 1000 on page 2\n",
      "Processing 818 of 1000 on page 2\n",
      "Processing 819 of 1000 on page 2\n",
      "Processing 820 of 1000 on page 2\n",
      "Processing 821 of 1000 on page 2\n",
      "Processing 822 of 1000 on page 2\n",
      "Processing 823 of 1000 on page 2\n",
      "Processing 824 of 1000 on page 2\n",
      "Processing 825 of 1000 on page 2\n",
      "Processing 826 of 1000 on page 2\n",
      "Processing 827 of 1000 on page 2\n",
      "Processing 828 of 1000 on page 2\n",
      "Processing 829 of 1000 on page 2\n",
      "Processing 830 of 1000 on page 2\n",
      "Processing 831 of 1000 on page 2\n",
      "Processing 832 of 1000 on page 2\n",
      "Processing 833 of 1000 on page 2\n",
      "Processing 834 of 1000 on page 2\n",
      "Processing 835 of 1000 on page 2\n",
      "Processing 836 of 1000 on page 2\n",
      "Processing 837 of 1000 on page 2\n",
      "Processing 838 of 1000 on page 2\n",
      "Processing 839 of 1000 on page 2\n",
      "Processing 840 of 1000 on page 2\n",
      "Processing 841 of 1000 on page 2\n",
      "Processing 842 of 1000 on page 2\n",
      "Processing 843 of 1000 on page 2\n",
      "Processing 844 of 1000 on page 2\n",
      "Processing 845 of 1000 on page 2\n",
      "Processing 846 of 1000 on page 2\n",
      "Processing 847 of 1000 on page 2\n",
      "Processing 848 of 1000 on page 2\n",
      "Processing 849 of 1000 on page 2\n",
      "Processing 850 of 1000 on page 2\n",
      "Processing 851 of 1000 on page 2\n",
      "Processing 852 of 1000 on page 2\n",
      "Processing 853 of 1000 on page 2\n",
      "Processing 854 of 1000 on page 2\n",
      "Processing 855 of 1000 on page 2\n",
      "Processing 856 of 1000 on page 2\n",
      "Processing 857 of 1000 on page 2\n",
      "Processing 858 of 1000 on page 2\n",
      "Processing 859 of 1000 on page 2\n",
      "Processing 860 of 1000 on page 2\n",
      "Processing 861 of 1000 on page 2\n",
      "Processing 862 of 1000 on page 2\n",
      "Processing 863 of 1000 on page 2\n",
      "Processing 864 of 1000 on page 2\n",
      "Processing 865 of 1000 on page 2\n",
      "Processing 866 of 1000 on page 2\n",
      "Processing 867 of 1000 on page 2\n",
      "Processing 868 of 1000 on page 2\n",
      "Processing 869 of 1000 on page 2\n",
      "Processing 870 of 1000 on page 2\n",
      "Processing 871 of 1000 on page 2\n",
      "Processing 872 of 1000 on page 2\n",
      "Processing 873 of 1000 on page 2\n",
      "Processing 874 of 1000 on page 2\n",
      "Processing 875 of 1000 on page 2\n",
      "Processing 876 of 1000 on page 2\n",
      "Processing 877 of 1000 on page 2\n",
      "Processing 878 of 1000 on page 2\n",
      "Processing 879 of 1000 on page 2\n",
      "Processing 880 of 1000 on page 2\n",
      "Processing 881 of 1000 on page 2\n",
      "Processing 882 of 1000 on page 2\n",
      "Processing 883 of 1000 on page 2\n",
      "Processing 884 of 1000 on page 2\n",
      "Processing 885 of 1000 on page 2\n",
      "Processing 886 of 1000 on page 2\n",
      "Processing 887 of 1000 on page 2\n",
      "Processing 888 of 1000 on page 2\n",
      "Processing 889 of 1000 on page 2\n",
      "Processing 890 of 1000 on page 2\n",
      "Processing 891 of 1000 on page 2\n",
      "Processing 892 of 1000 on page 2\n",
      "Processing 893 of 1000 on page 2\n",
      "Processing 894 of 1000 on page 2\n",
      "Processing 895 of 1000 on page 2\n",
      "Processing 896 of 1000 on page 2\n",
      "Processing 897 of 1000 on page 2\n",
      "Processing 898 of 1000 on page 2\n",
      "Processing 899 of 1000 on page 2\n",
      "Processing 900 of 1000 on page 2\n",
      "Processing 901 of 1000 on page 2\n",
      "Processing 902 of 1000 on page 2\n",
      "Processing 903 of 1000 on page 2\n",
      "Processing 904 of 1000 on page 2\n",
      "Processing 905 of 1000 on page 2\n",
      "Processing 906 of 1000 on page 2\n",
      "Processing 907 of 1000 on page 2\n",
      "Processing 908 of 1000 on page 2\n",
      "Processing 909 of 1000 on page 2\n",
      "Processing 910 of 1000 on page 2\n",
      "Processing 911 of 1000 on page 2\n",
      "Processing 912 of 1000 on page 2\n",
      "Processing 913 of 1000 on page 2\n",
      "Processing 914 of 1000 on page 2\n",
      "Processing 915 of 1000 on page 2\n",
      "Processing 916 of 1000 on page 2\n",
      "Processing 917 of 1000 on page 2\n",
      "Processing 918 of 1000 on page 2\n",
      "Processing 919 of 1000 on page 2\n",
      "Processing 920 of 1000 on page 2\n",
      "Processing 921 of 1000 on page 2\n",
      "Processing 922 of 1000 on page 2\n",
      "Processing 923 of 1000 on page 2\n",
      "Processing 924 of 1000 on page 2\n",
      "Processing 925 of 1000 on page 2\n",
      "Processing 926 of 1000 on page 2\n",
      "Processing 927 of 1000 on page 2\n",
      "Processing 928 of 1000 on page 2\n",
      "Processing 929 of 1000 on page 2\n",
      "Processing 930 of 1000 on page 2\n",
      "Processing 931 of 1000 on page 2\n",
      "Processing 932 of 1000 on page 2\n",
      "Processing 933 of 1000 on page 2\n",
      "Processing 934 of 1000 on page 2\n",
      "Processing 935 of 1000 on page 2\n",
      "Processing 936 of 1000 on page 2\n",
      "Processing 937 of 1000 on page 2\n",
      "Processing 938 of 1000 on page 2\n",
      "Processing 939 of 1000 on page 2\n",
      "Processing 940 of 1000 on page 2\n",
      "Processing 941 of 1000 on page 2\n",
      "Processing 942 of 1000 on page 2\n",
      "Processing 943 of 1000 on page 2\n",
      "Processing 944 of 1000 on page 2\n",
      "Processing 945 of 1000 on page 2\n",
      "Processing 946 of 1000 on page 2\n",
      "Processing 947 of 1000 on page 2\n",
      "Processing 948 of 1000 on page 2\n",
      "Processing 949 of 1000 on page 2\n",
      "Processing 950 of 1000 on page 2\n",
      "Processing 951 of 1000 on page 2\n",
      "Processing 952 of 1000 on page 2\n",
      "Processing 953 of 1000 on page 2\n",
      "Processing 954 of 1000 on page 2\n",
      "Processing 955 of 1000 on page 2\n",
      "Processing 956 of 1000 on page 2\n",
      "Processing 957 of 1000 on page 2\n",
      "Processing 958 of 1000 on page 2\n",
      "Processing 959 of 1000 on page 2\n",
      "Processing 960 of 1000 on page 2\n",
      "Processing 961 of 1000 on page 2\n",
      "Processing 962 of 1000 on page 2\n",
      "Processing 963 of 1000 on page 2\n",
      "Processing 964 of 1000 on page 2\n",
      "Processing 965 of 1000 on page 2\n",
      "Processing 966 of 1000 on page 2\n",
      "Processing 967 of 1000 on page 2\n",
      "Processing 968 of 1000 on page 2\n",
      "Processing 969 of 1000 on page 2\n",
      "Processing 970 of 1000 on page 2\n",
      "Processing 971 of 1000 on page 2\n",
      "Processing 972 of 1000 on page 2\n",
      "Processing 973 of 1000 on page 2\n",
      "Processing 974 of 1000 on page 2\n",
      "Processing 975 of 1000 on page 2\n",
      "Processing 976 of 1000 on page 2\n",
      "Processing 977 of 1000 on page 2\n",
      "Processing 978 of 1000 on page 2\n",
      "Processing 979 of 1000 on page 2\n",
      "Processing 980 of 1000 on page 2\n",
      "Processing 981 of 1000 on page 2\n",
      "Processing 982 of 1000 on page 2\n",
      "Processing 983 of 1000 on page 2\n",
      "Processing 984 of 1000 on page 2\n",
      "Processing 985 of 1000 on page 2\n",
      "Processing 986 of 1000 on page 2\n",
      "Processing 987 of 1000 on page 2\n",
      "Processing 988 of 1000 on page 2\n",
      "Processing 989 of 1000 on page 2\n",
      "Processing 990 of 1000 on page 2\n",
      "Processing 991 of 1000 on page 2\n",
      "Processing 992 of 1000 on page 2\n",
      "Processing 993 of 1000 on page 2\n",
      "Processing 994 of 1000 on page 2\n",
      "Processing 995 of 1000 on page 2\n",
      "Processing 996 of 1000 on page 2\n",
      "Processing 997 of 1000 on page 2\n",
      "Processing 998 of 1000 on page 2\n",
      "Processing 999 of 1000 on page 2\n",
      "Processing 1000 of 1000 on page 2\n",
      "Processing 1 of 135 on page 3\n",
      "Processing 2 of 135 on page 3\n",
      "Processing 3 of 135 on page 3\n",
      "Processing 4 of 135 on page 3\n",
      "Processing 5 of 135 on page 3\n",
      "Processing 6 of 135 on page 3\n",
      "Processing 7 of 135 on page 3\n",
      "Processing 8 of 135 on page 3\n",
      "Processing 9 of 135 on page 3\n",
      "Processing 10 of 135 on page 3\n",
      "Processing 11 of 135 on page 3\n",
      "Processing 12 of 135 on page 3\n",
      "Processing 13 of 135 on page 3\n",
      "Processing 14 of 135 on page 3\n",
      "Processing 15 of 135 on page 3\n",
      "Processing 16 of 135 on page 3\n",
      "Processing 17 of 135 on page 3\n",
      "Processing 18 of 135 on page 3\n",
      "Processing 19 of 135 on page 3\n",
      "Processing 20 of 135 on page 3\n",
      "Processing 21 of 135 on page 3\n",
      "Processing 22 of 135 on page 3\n",
      "Processing 23 of 135 on page 3\n",
      "Processing 24 of 135 on page 3\n",
      "Processing 25 of 135 on page 3\n",
      "Processing 26 of 135 on page 3\n",
      "Processing 27 of 135 on page 3\n",
      "Processing 28 of 135 on page 3\n",
      "Processing 29 of 135 on page 3\n",
      "Processing 30 of 135 on page 3\n",
      "Processing 31 of 135 on page 3\n",
      "Processing 32 of 135 on page 3\n",
      "Processing 33 of 135 on page 3\n",
      "Processing 34 of 135 on page 3\n",
      "Processing 35 of 135 on page 3\n",
      "Processing 36 of 135 on page 3\n",
      "Processing 37 of 135 on page 3\n",
      "Processing 38 of 135 on page 3\n",
      "Processing 39 of 135 on page 3\n",
      "Processing 40 of 135 on page 3\n",
      "Processing 41 of 135 on page 3\n",
      "Processing 42 of 135 on page 3\n",
      "Processing 43 of 135 on page 3\n",
      "Processing 44 of 135 on page 3\n",
      "Processing 45 of 135 on page 3\n",
      "Processing 46 of 135 on page 3\n",
      "Processing 47 of 135 on page 3\n",
      "Processing 48 of 135 on page 3\n",
      "Processing 49 of 135 on page 3\n",
      "Processing 50 of 135 on page 3\n",
      "Processing 51 of 135 on page 3\n",
      "Processing 52 of 135 on page 3\n",
      "Processing 53 of 135 on page 3\n",
      "Processing 54 of 135 on page 3\n",
      "Processing 55 of 135 on page 3\n",
      "Processing 56 of 135 on page 3\n",
      "Processing 57 of 135 on page 3\n",
      "Processing 58 of 135 on page 3\n",
      "Processing 59 of 135 on page 3\n",
      "Processing 60 of 135 on page 3\n",
      "Processing 61 of 135 on page 3\n",
      "Processing 62 of 135 on page 3\n",
      "Processing 63 of 135 on page 3\n",
      "Processing 64 of 135 on page 3\n",
      "Processing 65 of 135 on page 3\n",
      "Processing 66 of 135 on page 3\n",
      "Processing 67 of 135 on page 3\n",
      "Processing 68 of 135 on page 3\n",
      "Processing 69 of 135 on page 3\n",
      "Processing 70 of 135 on page 3\n",
      "Processing 71 of 135 on page 3\n",
      "Processing 72 of 135 on page 3\n",
      "Processing 73 of 135 on page 3\n",
      "Processing 74 of 135 on page 3\n",
      "Processing 75 of 135 on page 3\n",
      "Processing 76 of 135 on page 3\n",
      "Processing 77 of 135 on page 3\n",
      "Processing 78 of 135 on page 3\n",
      "Processing 79 of 135 on page 3\n",
      "Processing 80 of 135 on page 3\n",
      "Processing 81 of 135 on page 3\n",
      "Processing 82 of 135 on page 3\n",
      "Processing 83 of 135 on page 3\n",
      "Processing 84 of 135 on page 3\n",
      "Processing 85 of 135 on page 3\n",
      "Processing 86 of 135 on page 3\n",
      "Processing 87 of 135 on page 3\n",
      "Processing 88 of 135 on page 3\n",
      "Processing 89 of 135 on page 3\n",
      "Processing 90 of 135 on page 3\n",
      "Processing 91 of 135 on page 3\n",
      "Processing 92 of 135 on page 3\n",
      "Processing 93 of 135 on page 3\n",
      "Processing 94 of 135 on page 3\n",
      "Processing 95 of 135 on page 3\n",
      "Processing 96 of 135 on page 3\n",
      "Processing 97 of 135 on page 3\n",
      "Processing 98 of 135 on page 3\n",
      "Processing 99 of 135 on page 3\n",
      "Processing 100 of 135 on page 3\n",
      "Processing 101 of 135 on page 3\n",
      "Processing 102 of 135 on page 3\n",
      "Processing 103 of 135 on page 3\n",
      "Processing 104 of 135 on page 3\n",
      "Processing 105 of 135 on page 3\n",
      "Processing 106 of 135 on page 3\n",
      "Processing 107 of 135 on page 3\n",
      "Processing 108 of 135 on page 3\n",
      "Processing 109 of 135 on page 3\n",
      "Processing 110 of 135 on page 3\n",
      "Processing 111 of 135 on page 3\n",
      "Processing 112 of 135 on page 3\n",
      "Processing 113 of 135 on page 3\n",
      "Processing 114 of 135 on page 3\n",
      "Processing 115 of 135 on page 3\n",
      "Processing 116 of 135 on page 3\n",
      "Processing 117 of 135 on page 3\n",
      "Processing 118 of 135 on page 3\n",
      "Processing 119 of 135 on page 3\n",
      "Processing 120 of 135 on page 3\n",
      "Processing 121 of 135 on page 3\n",
      "Processing 122 of 135 on page 3\n",
      "Processing 123 of 135 on page 3\n",
      "Processing 124 of 135 on page 3\n",
      "Processing 125 of 135 on page 3\n",
      "Processing 126 of 135 on page 3\n",
      "Processing 127 of 135 on page 3\n",
      "Processing 128 of 135 on page 3\n",
      "Processing 129 of 135 on page 3\n",
      "Processing 130 of 135 on page 3\n",
      "Processing 131 of 135 on page 3\n",
      "Processing 132 of 135 on page 3\n",
      "Processing 133 of 135 on page 3\n",
      "Processing 134 of 135 on page 3\n",
      "Processing 135 of 135 on page 3\n"
     ]
    }
   ],
   "source": [
    "sql = '''\n",
    "DROP TABLE IF EXISTS derived.hydrotable;\n",
    "DROP TABLE IF EXISTS derived.usgs_elev_table;\n",
    "\n",
    "SELECT distinct LPAD(huc8::text, 8, '0') as huc8 FROM derived.featureid_huc_crosswalk WHERE huc8 is not null;\n",
    "'''\n",
    "df = run_sql_in_db(sql)\n",
    "\n",
    "col_names = ['HydroID', 'feature_id', 'NextDownID', 'order_', 'Number of Cells',\n",
    "       'SurfaceArea (m2)', 'BedArea (m2)', 'TopWidth (m)', 'LENGTHKM',\n",
    "       'AREASQKM', 'WettedPerimeter (m)', 'HydraulicRadius (m)',\n",
    "       'WetArea (m2)', 'Volume (m3)', 'SLOPE', 'ManningN', 'stage',\n",
    "       'default_discharge_cms', 'default_Volume (m3)', 'default_WetArea (m2)',\n",
    "       'default_HydraulicRadius (m)', 'default_ManningN',\n",
    "       'precalb_discharge_cms', 'calb_coef_spatial', 'HUC', 'LakeID',\n",
    "       'subdiv_applied', 'channel_n', 'overbank_n', 'subdiv_discharge_cms',\n",
    "       'last_updated', 'submitter', 'calb_coef_usgs', 'obs_source',\n",
    "       'calb_coef_final', 'calb_applied', 'discharge_cms']\n",
    "\n",
    "usecols = ['HydroID', 'feature_id', 'stage', 'discharge_cms']\n",
    "\n",
    "paginator = s3_client.get_paginator('list_objects')\n",
    "operation_parameters = {'Bucket': fim_bucket,\n",
    "                        'Prefix': f'{hand_datasets}/',\n",
    "                        'Delimiter': '/'}\n",
    "page_iterator = paginator.paginate(**operation_parameters)\n",
    "page_count = 0\n",
    "for page in page_iterator:\n",
    "    page_count += 1\n",
    "    prefix_objects = page['CommonPrefixes']\n",
    "    for i, prefix_obj in enumerate(prefix_objects):\n",
    "        print(f\"Processing {i+1} of {len(prefix_objects)} on page {page_count}\")\n",
    "        branch_prefix = f'{prefix_obj.get(\"Prefix\")}branches/0/'\n",
    "#         ## UNCOMMENT FOR ALL BRANCHES - NOT JUST 0\n",
    "#         huc_branches_prefix = f'{prefix_obj.get(\"Prefix\")}branches/'\n",
    "#         branches_result = s3_client.list_objects(Bucket=fim_bucket, Prefix=huc_branches_prefix, Delimiter='/')\n",
    "#         branch_prefix_objects = branches_result.get('CommonPrefixes')\n",
    "#         for i, branch_prefix_obj in enumerate(branch_prefix_objects):\n",
    "#             branch_prefix = branch_prefix_obj['Prefix']\n",
    "#         ## END UNCOMMENT\n",
    "        ## [UN]INDENT FROM HERE TO THE END IF [COMMENTED]UNCOMMENTED\n",
    "        branch_files_result = s3_client.list_objects(Bucket=fim_bucket, Prefix=branch_prefix, Delimiter='/')\n",
    "        hydro_table_key = None\n",
    "        usgs_elev_table_key = None\n",
    "        for content_obj in branch_files_result.get('Contents'):\n",
    "            branch_file_prefix = content_obj['Key']\n",
    "            if 'hydroTable' in branch_file_prefix:\n",
    "                hydro_table_key = branch_file_prefix\n",
    "            elif 'usgs_elev_table.csv' in branch_file_prefix:\n",
    "                usgs_elev_table_key = branch_file_prefix\n",
    "\n",
    "        if hydro_table_key and usgs_elev_table_key:\n",
    "#             print(f\"Found usgs_elev_table and hydroTable in {branch_prefix}\")\n",
    "            try:\n",
    "#                 print(\"...Fetching csvs...\")\n",
    "                uet = s3_client.get_object(Bucket=fim_bucket, Key=usgs_elev_table_key)['Body']\n",
    "                ht = s3_client.get_object(Bucket=fim_bucket, Key=hydro_table_key)['Body']\n",
    "#                 print(\"...Reading with pandas...\")\n",
    "                uet_df = pd.read_csv(uet)\n",
    "                ht_df = pd.read_csv(ht, header=0, names=col_names, usecols=usecols)\n",
    "#                 print('...Writing to db...')\n",
    "                uet_df['fim_version'] = fim_version\n",
    "                try:\n",
    "                    drop_indicies = []\n",
    "                    for index, row in uet_df.iterrows():\n",
    "                        if 'HydroID' in row and row['HydroID']:\n",
    "                            iter_hydro_id = row['HydroID']\n",
    "#                             print(f\"Subsetting hydrotable where HydroID == {iter_hydro_id}\")\n",
    "                            ht_df_iter = ht_df[ht_df['HydroID']==iter_hydro_id]\n",
    "                            if ht_df_iter.empty:\n",
    "                                drop_indicies.append(index)\n",
    "                                continue\n",
    "                            ht_df_iter['fim_version'] = fim_version\n",
    "                            ht_df_iter.to_sql(con=viz_engine, schema='derived', name='hydrotable', index=False, if_exists='append')\n",
    "                        else:\n",
    "                            drop_indicies.append(index)\n",
    "                    mod_uet_df = uet_df.drop(drop_indicies)\n",
    "                    mod_uet_df.to_sql(con=viz_engine, dtype={\"location_id\": Text(), \"nws_data_huc\": Text()}, schema='derived', name='usgs_elev_table', index=False, if_exists='append')\n",
    "                except Exception as e:\n",
    "                    print('******************************************')\n",
    "                    print(f'Error encountered on {branch_file_prefix}')\n",
    "                    print(e)\n",
    "                    print('******************************************')       \n",
    "            except Exception as e:\n",
    "                raise e\n",
    "                print(f'Fetch failed: {e}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "77065317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This result object does not return rows. It has been closed automatically.\n"
     ]
    }
   ],
   "source": [
    "sql = '''\n",
    "DROP TABLE IF EXISTS derived.hydrotable_staggered;\n",
    "SELECT\n",
    "    et.location_id,\n",
    "    ht.feature_id,\n",
    "    (stage + et.dem_adj_elevation) * 3.28084 as elevation_ft,\n",
    "    LEAD((stage + et.dem_adj_elevation) * 3.28084) OVER (PARTITION BY ht.feature_id ORDER BY ht.feature_id, stage) as next_elevation_ft,\n",
    "    discharge_cms * 35.3147 as discharge_cfs,\n",
    "    LEAD(discharge_cms * 35.3147) OVER (PARTITION BY ht.feature_id ORDER BY ht.feature_id, stage) as next_discharge_cfs\n",
    "INTO derived.hydrotable_staggered\n",
    "FROM derived.hydrotable AS ht\n",
    "JOIN derived.usgs_elev_table AS et ON ht.\"HydroID\" = et.\"HydroID\" AND et.location_id IS NOT NULL;\n",
    "'''\n",
    "\n",
    "run_sql_in_db(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "056a2702",
   "metadata": {},
   "outputs": [],
   "source": [
    "usgs_curves_key = f'{qa_datasets}/usgs_rating_curves.csv'\n",
    "obj_body = s3_client.get_object(Bucket=fim_bucket, Key=usgs_curves_key)['Body']\n",
    "df = pd.read_csv(obj_body)\n",
    "df.to_sql(con=viz_engine, schema='derived', name='usgs_rating_curves', index=False, if_exists='replace', chunksize=150000, method='multi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825e5aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "DROP TABLE IF EXISTS derived.usgs_rating_curves_staggered;\n",
    "SELECT \n",
    "    location_id,\n",
    "    flow as discharge_cfs, \n",
    "    LEAD(flow) OVER (PARTITION BY location_id ORDER BY location_id, stage) as next_discharge_cfs,\n",
    "    stage,\n",
    "    navd88_datum,\n",
    "    elevation_navd88 as elevation_ft,\n",
    "    LEAD(elevation_navd88) OVER (PARTITION BY location_id ORDER BY location_id, stage) as next_elevation_ft\n",
    "INTO derived.usgs_rating_curves_staggered\n",
    "FROM derived.usgs_rating_curves;\n",
    "'''\n",
    "\n",
    "run_sql_in_db(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18b8887",
   "metadata": {},
   "source": [
    "<h2>7 - UPDATE SRC SKILL METRICS IN DB</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88aed34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_stats_key = f'{fim_folder}/qa_datasets/agg_nwm_recurr_flow_elev_stats_location_id.csv'\n",
    "obj_body = s3_client.get_object(Bucket=fim_bucket, Key=src_stats_key)['Body']\n",
    "df = pd.read_csv(obj_body)\n",
    "# df['fim_version'] = fim_version\n",
    "print(df.to_dict('records'))\n",
    "# df.to_sql(con=viz_engine, schema='derived', name='src_skill', index=False, if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b2880f",
   "metadata": {},
   "source": [
    "<h2>8 - UPDATE FIM PERFORMANCE METRICS IN DB</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "974dac60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fim_4_3_11_0/qa_datasets/fim_performance_points.csv\n",
      "--> Downloading hydrovis-ti-deployment-us-east-1/fim_4_3_11_0/qa_datasets/fim_performance_points.csv to brad_data/qc_fim_data/fim_performance_points.csv\n",
      "Reading file...\n",
      "File read.\n",
      "Loading data into DB...\n",
      "Dropping reference.fim_performance_points if it exists\n",
      "Getting sql to create table\n",
      "CREATE TABLE reference.fim_performance_points (\n",
      "\"oid\" INTEGER,\n",
      "  \"nws_lid\" TEXT,\n",
      "  \"lid_name\" TEXT,\n",
      "  \"feature_id\" TEXT,\n",
      "  \"wfo\" TEXT,\n",
      "  \"state\" TEXT,\n",
      "  \"county\" TEXT,\n",
      "  \"huc\" TEXT,\n",
      "  \"version\" TEXT,\n",
      "  \"magnitude\" TEXT,\n",
      "  \"tp_area_km\" DOUBLE PRECISION,\n",
      "  \"fp_area_km\" DOUBLE PRECISION,\n",
      "  \"tn_area_km\" DOUBLE PRECISION,\n",
      "  \"fn_area_km\" DOUBLE PRECISION,\n",
      "  \"csi\" DOUBLE PRECISION,\n",
      "  \"far\" DOUBLE PRECISION,\n",
      "  \"tpr\" DOUBLE PRECISION,\n",
      "  \"pnd\" DOUBLE PRECISION,\n",
      "  \"mcc\" DOUBLE PRECISION,\n",
      "  \"equitable_\" DOUBLE PRECISION,\n",
      "  \"source\" TEXT,\n",
      "  \"geom\" GEOMETRY(Geometry,3857)\n",
      ")\n",
      "Creating reference.fim_performance_points\n",
      "Adding data to reference.fim_performance_points\n",
      "fim_4_3_11_0/qa_datasets/fim_performance_polys.csv\n",
      "--> Downloading hydrovis-ti-deployment-us-east-1/fim_4_3_11_0/qa_datasets/fim_performance_polys.csv to brad_data/qc_fim_data/fim_performance_polys.csv\n",
      "Reading file...\n",
      "File read.\n",
      "Loading data into DB...\n",
      "Dropping reference.fim_performance_polys if it exists\n",
      "Getting sql to create table\n",
      "CREATE TABLE reference.fim_performance_polys (\n",
      "\"oid\" INTEGER,\n",
      "  \"version\" TEXT,\n",
      "  \"magnitude\" TEXT,\n",
      "  \"huc8\" TEXT,\n",
      "  \"tp_area_km\" DOUBLE PRECISION,\n",
      "  \"fp_area_km\" DOUBLE PRECISION,\n",
      "  \"tn_area_km\" DOUBLE PRECISION,\n",
      "  \"fn_area_km\" DOUBLE PRECISION,\n",
      "  \"csi\" DOUBLE PRECISION,\n",
      "  \"far\" DOUBLE PRECISION,\n",
      "  \"tpr\" DOUBLE PRECISION,\n",
      "  \"pnd\" DOUBLE PRECISION,\n",
      "  \"mcc\" DOUBLE PRECISION,\n",
      "  \"equitable_\" DOUBLE PRECISION,\n",
      "  \"source\" TEXT,\n",
      "  \"geom\" GEOMETRY(Geometry,3857)\n",
      ")\n",
      "Creating reference.fim_performance_polys\n",
      "Adding data to reference.fim_performance_polys\n",
      "fim_4_3_11_0/qa_datasets/fim_performance_catchments.csv\n",
      "--> Downloading hydrovis-ti-deployment-us-east-1/fim_4_3_11_0/qa_datasets/fim_performance_catchments.csv to brad_data/qc_fim_data/fim_performance_catchments.csv\n",
      "Reading file...\n",
      "File read.\n",
      "Loading data into DB...\n",
      "Chunk loading...\n",
      "100000\n",
      "Dropping reference.fim_performance_catchments if it exists\n",
      "Getting sql to create table\n",
      "CREATE TABLE reference.fim_performance_catchments (\n",
      "\"oid\" INTEGER,\n",
      "  \"hydroid\" INTEGER,\n",
      "  \"geom\" GEOMETRY(Geometry,3857),\n",
      "  \"huc8\" TEXT,\n",
      "  \"contingency_tot_area_km2\" DOUBLE PRECISION,\n",
      "  \"csi\" DOUBLE PRECISION,\n",
      "  \"far\" DOUBLE PRECISION,\n",
      "  \"tpr\" DOUBLE PRECISION,\n",
      "  \"tnr\" DOUBLE PRECISION,\n",
      "  \"ppv\" DOUBLE PRECISION,\n",
      "  \"npv\" DOUBLE PRECISION,\n",
      "  \"bal_acc\" DOUBLE PRECISION,\n",
      "  \"mcc\" DOUBLE PRECISION,\n",
      "  \"equitable_threat_score\" DOUBLE PRECISION,\n",
      "  \"prevalence\" DOUBLE PRECISION,\n",
      "  \"bias\" DOUBLE PRECISION,\n",
      "  \"f1_score\" DOUBLE PRECISION,\n",
      "  \"masked_perc\" DOUBLE PRECISION,\n",
      "  \"mag\" TEXT,\n",
      "  \"bench\" TEXT,\n",
      "  \"version\" TEXT\n",
      ")\n",
      "Creating reference.fim_performance_catchments\n",
      "Adding data to reference.fim_performance_catchments\n",
      "100000\n",
      "Adding data to reference.fim_performance_catchments\n",
      "100000\n",
      "Adding data to reference.fim_performance_catchments\n",
      "100000\n",
      "Adding data to reference.fim_performance_catchments\n",
      "100000\n",
      "Adding data to reference.fim_performance_catchments\n",
      "100000\n",
      "Adding data to reference.fim_performance_catchments\n",
      "100000\n",
      "Adding data to reference.fim_performance_catchments\n",
      "100000\n",
      "Adding data to reference.fim_performance_catchments\n",
      "100000\n",
      "Adding data to reference.fim_performance_catchments\n",
      "88620\n",
      "Adding data to reference.fim_performance_catchments\n",
      "Making valid...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from helper_functions.shared_functions import *\n",
    "import boto3\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "# os.environ['EGIS_DB_HOST'] = \"hv-ti-egis-rds-pg-egdb.c4vzypepnkx3.us-east-1.rds.amazonaws.com\"  #TI DB\n",
    "os.environ['EGIS_DB_HOST'] = \"hv-ti-egis-rds-pg-egdb.c4vzypepnkx3.us-east-1.rds.amazonaws.com\"  #Dev DB\n",
    "os.environ['EGIS_DB_DATABASE'] = \"hydrovis\"\n",
    "os.environ['EGIS_DB_USERNAME'] = \"hydrovis\"\n",
    "os.environ['EGIS_DB_PASSWORD'] = \"hardwork123donehere\"\n",
    "\n",
    "db_type = \"egis\"\n",
    "db_engine = get_db_engine(db_type)\n",
    "db_type = \"egis\"\n",
    "db_engine = get_db_engine(db_type)\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Define bucket and parent directories.\n",
    "bucket = \"hydrovis-ti-deployment-us-east-1\"\n",
    "parent_directory = \"qc_fim_data\"\n",
    "local_download_parent_directory = f'brad_data/qc_fim_data'\n",
    "\n",
    "file_handles = ['fim_performance_points.csv', 'fim_performance_polys.csv', 'fim_performance_catchments.csv']\n",
    "\n",
    "for file_handle in file_handles:\n",
    "    # Define path to file to download and its local download path, the download.\n",
    "    filename = f\"{fim_folder}/qa_datasets/{file_handle}\"\n",
    "    print(filename)\n",
    "    local_download_path = os.path.join(local_download_parent_directory, f'{file_handle}')\n",
    "    print(f\"--> Downloading {fim_bucket}/{filename} to {local_download_path}\")\n",
    "    s3.download_file(fim_bucket, filename, local_download_path)\n",
    "    \n",
    "    #  -- Open file and reformat -- #\n",
    "    print(\"Reading file...\")\n",
    "    df = pd.read_csv(local_download_path)\n",
    "    print(\"File read.\")\n",
    "    # Rename headers.\n",
    "    if file_handle == 'fim_performance_points.csv':\n",
    "        df = df.rename(columns={'Unnamed: 0': 'oid', 'geometry': 'geom'})\n",
    "    else:\n",
    "        df = df.rename(columns={'Unnamed: 0': 'oid', 'geometry': 'geom', 'huc':'huc8'})\n",
    "        \n",
    "    # Convert all field names to lowercase (needed for ArcGIS Pro).\n",
    "    df.columns= df.columns.str.lower()\n",
    "\n",
    "    # Enforce data types on df before loading in DB (TODO: need to create special cases for each layer).\n",
    "    if file_handle == 'fim_performance_points.csv':\n",
    "        df = df.astype({'huc': 'str'})\n",
    "    else:\n",
    "        df = df.astype({'huc8': 'str'})\n",
    "    df = df.fillna(0)\n",
    "    try:\n",
    "        df = df.astype({'feature_id': 'int'})\n",
    "        df = df.astype({'feature_id': 'str'})\n",
    "    except KeyError:  # If there is no feature_id field\n",
    "        pass\n",
    "    try:\n",
    "        df = df.astype({'nwm_seg': 'int'})\n",
    "        df = df.astype({'nwm_seg': 'str'})\n",
    "    except KeyError:  # If there is no nwm_seg field\n",
    "        pass\n",
    "    try:\n",
    "        df = df.astype({'usgs_gage': 'int'})\n",
    "        df = df.astype({'usgs_gage': 'str'})\n",
    "    except KeyError:  # If there is no usgs_gage field\n",
    "        pass\n",
    "        \n",
    "    # zfill HUC8 field.\n",
    "    if file_handle == 'fim_performance_points.csv':\n",
    "        df['huc'] = df['huc'].apply(lambda x: x.zfill(8))\n",
    "    else:\n",
    "        df['huc8'] = df['huc8'].apply(lambda x: x.zfill(8))\n",
    " \n",
    "    # Upload df to database.\n",
    "    stripped_layer_name = file_handle.replace(\".csv\", \"\")\n",
    "    table_name = \"reference.\" + stripped_layer_name\n",
    "    print(\"Loading data into DB...\")\n",
    "    \n",
    "    # Chunk load data into DB\n",
    "    if file_handle in ['fim_performance_catchments.csv']:\n",
    "        print(\"Chunk loading...\")\n",
    "        # Create list of df chunks\n",
    "        n = 100000  #chunk row size\n",
    "        list_df = [df[i:i+n] for i in range(0,df.shape[0],n)]\n",
    "        \n",
    "        # Load the first chunk into the DB as a new table\n",
    "        first_chunk_df = list_df[0]\n",
    "        print(first_chunk_df.shape[0])\n",
    "        load_df_into_db(table_name, db_engine, first_chunk_df, epsg=3857, drop_first=True)\n",
    "        \n",
    "        # Load remaining chunks into newly created table\n",
    "        for remaining_chunk in list_df[1:]:\n",
    "            print(remaining_chunk.shape[0])\n",
    "            load_df_into_db(table_name, db_engine, remaining_chunk, epsg=3857, drop_first=False)\n",
    "        db_engine.execute(f'CREATE INDEX ON reference.{stripped_layer_name} USING GIST (geom);')\n",
    "        \n",
    "    else:\n",
    "        load_df_into_db(table_name, db_engine, df, epsg=3857, drop_first=True)\n",
    "        db_engine.execute(f'CREATE INDEX ON reference.{stripped_layer_name} USING GIST (geom);')\n",
    "    \n",
    "    if file_handle == 'fim_performance_catchments.csv':\n",
    "        print(\"Making valid...\")\n",
    "        make_valid_sql = f\"\"\"\n",
    "\n",
    "        ALTER TABLE reference.fim_performance_catchments\n",
    "          ADD COLUMN geom_invalid geometry\n",
    "          DEFAULT NULL;\n",
    "\n",
    "        UPDATE reference.fim_performance_catchments\n",
    "          SET geom = ST_MakeValid(geom),\n",
    "              geom_invalid = geom\n",
    "          WHERE NOT ST_IsValid(geom);\n",
    "          \"\"\"\n",
    "\n",
    "        db_engine.execute(make_valid_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4c2db3",
   "metadata": {},
   "source": [
    "<h2>9 - UPDATE STAGE-BASED CATFIM DATA IN DB</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9b5937a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fim_4_3_3_4/qa_datasets/stage_based_catfim_sites.csv\n",
      "--> Downloading hydrovis-ti-deployment-us-east-1/fim_4_3_3_4/qa_datasets/stage_based_catfim_sites.csv to brad_data/qc_fim_data/stage_based_catfim_sites.csv\n",
      "Reading file...\n",
      "File read.\n",
      "Retrieving values for *\n",
      "Loading data into DB...\n",
      "Dataframe shape\n",
      "5160\n",
      "Dropping reference.stage_based_catfim_sites if it exists\n",
      "Getting sql to create table\n",
      "CREATE TABLE reference.stage_based_catfim_sites (\n",
      "\"oid\" INTEGER,\n",
      "  \"wrds_timestamp\" TEXT,\n",
      "  \"nrldb_timestamp\" TEXT,\n",
      "  \"nwis_timestamp\" TEXT,\n",
      "  \"metadata_sources\" TEXT,\n",
      "  \"ahps_lid\" TEXT,\n",
      "  \"usgs_gage\" TEXT,\n",
      "  \"nwm_seg\" TEXT,\n",
      "  \"identifiers_goes_id\" TEXT,\n",
      "  \"identifiers_env_can_gage_id\" DOUBLE PRECISION,\n",
      "  \"nws_data_name\" TEXT,\n",
      "  \"nws_data_wfo\" TEXT,\n",
      "  \"nws_data_rfc\" TEXT,\n",
      "  \"nws_data_geo_rfc\" TEXT,\n",
      "  \"nws_data_latitude\" DOUBLE PRECISION,\n",
      "  \"nws_data_longitude\" DOUBLE PRECISION,\n",
      "  \"nws_data_map_link\" TEXT,\n",
      "  \"nws_data_horizontal_datum_name\" TEXT,\n",
      "  \"nws_data_state\" TEXT,\n",
      "  \"nws_data_county\" TEXT,\n",
      "  \"nws_data_county_code\" DOUBLE PRECISION,\n",
      "  \"nws_data_huc\" TEXT,\n",
      "  \"nws_data_hsa\" TEXT,\n",
      "  \"nws_data_zero_datum\" DOUBLE PRECISION,\n",
      "  \"nws_data_vertical_datum_name\" TEXT,\n",
      "  \"nws_data_rfc_forecast_point\" TEXT,\n",
      "  \"nws_data_rfc_defined_fcst_point\" TEXT,\n",
      "  \"nws_data_riverpoint\" TEXT,\n",
      "  \"usgs_data_name\" TEXT,\n",
      "  \"usgs_data_geo_rfc\" TEXT,\n",
      "  \"usgs_data_latitude\" DOUBLE PRECISION,\n",
      "  \"usgs_data_longitude\" DOUBLE PRECISION,\n",
      "  \"usgs_data_map_link\" TEXT,\n",
      "  \"usgs_data_coord_accuracy_code\" TEXT,\n",
      "  \"usgs_data_latlon_datum_name\" TEXT,\n",
      "  \"usgs_data_coord_method_code\" TEXT,\n",
      "  \"usgs_data_state\" TEXT,\n",
      "  \"usgs_data_huc\" DOUBLE PRECISION,\n",
      "  \"usgs_data_site_type\" TEXT,\n",
      "  \"usgs_data_altitude\" DOUBLE PRECISION,\n",
      "  \"usgs_data_alt_accuracy_code\" DOUBLE PRECISION,\n",
      "  \"usgs_data_alt_datum_code\" TEXT,\n",
      "  \"usgs_data_alt_method_code\" TEXT,\n",
      "  \"usgs_data_drainage_area\" DOUBLE PRECISION,\n",
      "  \"usgs_data_drainage_area_units\" TEXT,\n",
      "  \"usgs_data_contrib_drainage_area\" DOUBLE PRECISION,\n",
      "  \"usgs_data_active\" DOUBLE PRECISION,\n",
      "  \"usgs_data_gages_ii_reference\" DOUBLE PRECISION,\n",
      "  \"nwm_feature_data_downstream_feature_id\" DOUBLE PRECISION,\n",
      "  \"nwm_feature_data_latitude\" DOUBLE PRECISION,\n",
      "  \"nwm_feature_data_longitude\" DOUBLE PRECISION,\n",
      "  \"nwm_feature_data_altitude\" DOUBLE PRECISION,\n",
      "  \"nwm_feature_data_stream_length\" DOUBLE PRECISION,\n",
      "  \"nwm_feature_data_stream_order\" DOUBLE PRECISION,\n",
      "  \"nwm_feature_data_mannings_roughness\" DOUBLE PRECISION,\n",
      "  \"nwm_feature_data_slope\" DOUBLE PRECISION,\n",
      "  \"nwm_feature_data_channel_side_slope\" DOUBLE PRECISION,\n",
      "  \"nwm_feature_data_nhd_waterbody_comid\" DOUBLE PRECISION,\n",
      "  \"env_can_gage_data_name\" DOUBLE PRECISION,\n",
      "  \"env_can_gage_data_latitude\" DOUBLE PRECISION,\n",
      "  \"env_can_gage_data_longitude\" DOUBLE PRECISION,\n",
      "  \"env_can_gage_data_map_link\" DOUBLE PRECISION,\n",
      "  \"env_can_gage_data_drainage_area\" DOUBLE PRECISION,\n",
      "  \"env_can_gage_data_contrib_drainage_area\" DOUBLE PRECISION,\n",
      "  \"env_can_gage_data_water_course\" DOUBLE PRECISION,\n",
      "  \"nws_preferred_name\" TEXT,\n",
      "  \"nws_preferred_latitude\" DOUBLE PRECISION,\n",
      "  \"nws_preferred_longitude\" DOUBLE PRECISION,\n",
      "  \"nws_preferred_latlon_datum_name\" TEXT,\n",
      "  \"nws_preferred_state\" TEXT,\n",
      "  \"nws_preferred_huc\" TEXT,\n",
      "  \"usgs_preferred_name\" TEXT,\n",
      "  \"usgs_preferred_latitude\" DOUBLE PRECISION,\n",
      "  \"usgs_preferred_longitude\" DOUBLE PRECISION,\n",
      "  \"usgs_preferred_latlon_datum_name\" TEXT,\n",
      "  \"usgs_preferred_state\" TEXT,\n",
      "  \"usgs_preferred_huc\" TEXT,\n",
      "  \"crosswalk_datasets_location_nwm_crosswalk_dataset_location_nwm_crosswalk_dataset_id\" DOUBLE PRECISION,\n",
      "  \"crosswalk_datasets_location_nwm_crosswalk_dataset_name\" TEXT,\n",
      "  \"crosswalk_datasets_location_nwm_crosswalk_dataset_description\" TEXT,\n",
      "  \"crosswalk_datasets_nws_usgs_crosswalk_dataset_nws_usgs_crosswalk_dataset_id\" DOUBLE PRECISION,\n",
      "  \"crosswalk_datasets_nws_usgs_crosswalk_dataset_name\" TEXT,\n",
      "  \"crosswalk_datasets_nws_usgs_crosswalk_dataset_description\" TEXT,\n",
      "  \"assigned_crs\" TEXT,\n",
      "  \"huc8\" TEXT,\n",
      "  \"name\" TEXT,\n",
      "  \"states\" TEXT,\n",
      "  \"mapped\" TEXT,\n",
      "  \"status\" TEXT,\n",
      "  \"acceptable_coord_acc_code_list\" TEXT,\n",
      "  \"acceptable_coord_method_code_list\" TEXT,\n",
      "  \"acceptable_alt_acc_thresh\" DOUBLE PRECISION,\n",
      "  \"acceptable_alt_meth_code_list\" TEXT,\n",
      "  \"acceptable_site_type_list\" TEXT,\n",
      "  \"geom\" GEOMETRY(Geometry,3857)\n",
      ")\n",
      "Creating reference.stage_based_catfim_sites\n",
      "Adding data to reference.stage_based_catfim_sites\n",
      "fim_4_3_3_4/qa_datasets/stage_based_catfim.csv\n",
      "--> Downloading hydrovis-ti-deployment-us-east-1/fim_4_3_3_4/qa_datasets/stage_based_catfim.csv to brad_data/qc_fim_data/stage_based_catfim.csv\n",
      "Reading file...\n",
      "File read.\n",
      "Retrieving values for *\n",
      "Loading data into DB...\n",
      "Dataframe shape\n",
      "22477\n",
      "Chunk loading...\n",
      "1000\n",
      "Dropping reference.stage_based_catfim if it exists\n",
      "Getting sql to create table\n",
      "CREATE TABLE reference.stage_based_catfim (\n",
      "\"oid\" INTEGER,\n",
      "  \"ahps_lid\" TEXT,\n",
      "  \"magnitude\" TEXT,\n",
      "  \"version\" TEXT,\n",
      "  \"huc8\" TEXT,\n",
      "  \"interval_stage\" DOUBLE PRECISION,\n",
      "  \"name\" TEXT,\n",
      "  \"wfo\" TEXT,\n",
      "  \"rfc\" TEXT,\n",
      "  \"state\" TEXT,\n",
      "  \"county\" TEXT,\n",
      "  \"q\" DOUBLE PRECISION,\n",
      "  \"q_uni\" TEXT,\n",
      "  \"q_src\" TEXT,\n",
      "  \"stage\" DOUBLE PRECISION,\n",
      "  \"stage_uni\" TEXT,\n",
      "  \"s_src\" TEXT,\n",
      "  \"wrds_time\" TEXT,\n",
      "  \"nrldb_time\" TEXT,\n",
      "  \"nwis_time\" TEXT,\n",
      "  \"lat\" DOUBLE PRECISION,\n",
      "  \"lon\" DOUBLE PRECISION,\n",
      "  \"dtm_adj_ft\" DOUBLE PRECISION,\n",
      "  \"dadj_w_ft\" DOUBLE PRECISION,\n",
      "  \"dadj_w_m\" DOUBLE PRECISION,\n",
      "  \"lid_alt_ft\" DOUBLE PRECISION,\n",
      "  \"lid_alt_m\" DOUBLE PRECISION,\n",
      "  \"viz\" TEXT,\n",
      "  \"geom\" GEOMETRY(Geometry,3857)\n",
      ")\n",
      "Creating reference.stage_based_catfim\n",
      "Adding data to reference.stage_based_catfim\n",
      "1000\n",
      "Adding data to reference.stage_based_catfim\n",
      "1000\n",
      "Adding data to reference.stage_based_catfim\n",
      "1000\n",
      "Adding data to reference.stage_based_catfim\n",
      "1000\n",
      "Adding data to reference.stage_based_catfim\n",
      "1000\n",
      "Adding data to reference.stage_based_catfim\n",
      "1000\n",
      "Adding data to reference.stage_based_catfim\n",
      "1000\n",
      "Adding data to reference.stage_based_catfim\n",
      "1000\n",
      "Adding data to reference.stage_based_catfim\n",
      "1000\n",
      "Adding data to reference.stage_based_catfim\n",
      "1000\n",
      "Adding data to reference.stage_based_catfim\n",
      "1000\n",
      "Adding data to reference.stage_based_catfim\n",
      "1000\n",
      "Adding data to reference.stage_based_catfim\n",
      "1000\n",
      "Adding data to reference.stage_based_catfim\n",
      "1000\n",
      "Adding data to reference.stage_based_catfim\n",
      "1000\n",
      "Adding data to reference.stage_based_catfim\n",
      "1000\n",
      "Adding data to reference.stage_based_catfim\n",
      "1000\n",
      "Adding data to reference.stage_based_catfim\n",
      "1000\n",
      "Adding data to reference.stage_based_catfim\n",
      "1000\n",
      "Adding data to reference.stage_based_catfim\n",
      "1000\n",
      "Adding data to reference.stage_based_catfim\n",
      "1000\n",
      "Adding data to reference.stage_based_catfim\n",
      "477\n",
      "Adding data to reference.stage_based_catfim\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from helper_functions.shared_functions import *\n",
    "import boto3\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "# os.environ['EGIS_DB_HOST'] = \"hv-ti-egis-rds-pg-egdb.c4vzypepnkx3.us-east-1.rds.amazonaws.com\"  #TI DB\n",
    "os.environ['EGIS_DB_HOST'] = \"hv-ti-egis-rds-pg-egdb.c4vzypepnkx3.us-east-1.rds.amazonaws.com\"  #Dev DB\n",
    "os.environ['EGIS_DB_DATABASE'] = \"hydrovis\"\n",
    "os.environ['EGIS_DB_USERNAME'] = \"hydrovis\"\n",
    "os.environ['EGIS_DB_PASSWORD'] = \"hardwork123donehere\"\n",
    "\n",
    "db_type = \"egis\"\n",
    "db_engine = get_db_engine(db_type)\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Define bucket and parent directories.\n",
    "bucket = \"hydrovis-ti-deployment-us-east-1\"\n",
    "parent_directory = \"qc_fim_data\"\n",
    "local_download_parent_directory = f'brad_data/qc_fim_data'\n",
    "\n",
    "file_handles = ['stage_based_catfim_sites.csv', 'stage_based_catfim.csv']\n",
    "\n",
    "for file_handle in file_handles:\n",
    "    # Define path to file to download and its local download path, the download.\n",
    "    filename = f\"{fim_folder}/qa_datasets/{file_handle}\"\n",
    "    print(filename)\n",
    "    local_download_path = os.path.join(local_download_parent_directory, f'{file_handle}')\n",
    "    print(f\"--> Downloading {fim_bucket}/{filename} to {local_download_path}\")\n",
    "    s3.download_file(fim_bucket, filename, local_download_path)\n",
    "    \n",
    "    #  -- Open file and reformat -- #\n",
    "    print(\"Reading file...\")\n",
    "    df = pd.read_csv(local_download_path)\n",
    "    print(\"File read.\")\n",
    "    \n",
    "    # Rename headers.\n",
    "    df = df.rename(columns={'Unnamed: 0': 'oid', 'geometry': 'geom', 'huc':'huc8'})\n",
    "        \n",
    "    # Convert all field names to lowercase (needed for ArcGIS Pro).\n",
    "    df.columns= df.columns.str.lower()\n",
    "\n",
    "    # Remove sites that are in derived.ahps_restricted_sites\n",
    "    restricted_sites_df = get_db_values(\"derived.ahps_restricted_sites\", \"*\")\n",
    "    restricted_dict = restricted_sites_df.to_dict('records')\n",
    "\n",
    "    # Change 'mapped' to 'no' if sites are present in restricted_sites_df\n",
    "    for site in restricted_dict:\n",
    "        nws_lid = site['nws_lid'].lower()\n",
    "        if \"sites\" in file_handle:\n",
    "            df.loc[df.ahps_lid==nws_lid, 'mapped'] = 'no'\n",
    "            df.loc[df.ahps_lid==nws_lid, 'status'] = site['restricted_reason']\n",
    "        else:\n",
    "            df.loc[df.ahps_lid==nws_lid, 'viz'] = 'no'\n",
    "            df = df[df['viz']=='yes']  # Subset df to only sites desired for mapping\n",
    "            \n",
    "    for sea_level_site in ['qutg1', 'augg1', 'baxg1', 'lamf1', 'adlg1', 'hrag1', 'stng1']:\n",
    "        if \"sites\" in file_handle:\n",
    "            df.loc[df.ahps_lid==sea_level_site, 'mapped'] = 'no'\n",
    "            df.loc[df.ahps_lid==sea_level_site, 'status'] = 'Stage thresholds seem to be based on sea level and not channel thalweg'\n",
    "        else:\n",
    "            df.loc[df.ahps_lid==sea_level_site, 'viz'] = 'no'\n",
    "            df = df[df['viz']=='yes']  # Subset df to only sites desired for mapping\n",
    "    \n",
    "    # Enforce data types on df before loading in DB (TODO: need to create special cases for each layer).\n",
    "    df = df.astype({'huc8': 'str'})\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    try:\n",
    "        df = df.astype({'feature_id': 'int'})\n",
    "        df = df.astype({'feature_id': 'str'})\n",
    "    except KeyError:  # If there is no feature_id field\n",
    "        pass\n",
    "    try:\n",
    "        df = df.astype({'nwm_seg': 'int'})\n",
    "        df = df.astype({'nwm_seg': 'str'})\n",
    "    except KeyError:  # If there is no nwm_seg field\n",
    "        pass\n",
    "    try:\n",
    "        df = df.astype({'usgs_gage': 'int'})\n",
    "        df = df.astype({'usgs_gage': 'str'})\n",
    "    except KeyError:  # If there is no usgs_gage field\n",
    "        pass\n",
    "        \n",
    "    # zfill HUC8 field.\n",
    "    df['huc8'] = df['huc8'].apply(lambda x: x.zfill(8))\n",
    "    \n",
    "    if file_handle in ['stage_based_catfim_sites.csv']:\n",
    "        df = df.astype({'nws_data_rfc_forecast_point': 'str'})\n",
    "        df = df.astype({'nws_data_rfc_defined_fcst_point': 'str'})\n",
    "        df = df.astype({'nws_data_riverpoint': 'str'})\n",
    "        \n",
    "    # Upload df to database.\n",
    "    stripped_layer_name = file_handle.replace(\".csv\", \"\")\n",
    "    table_name = \"reference.\" + stripped_layer_name\n",
    "    print(\"Loading data into DB...\")\n",
    "    \n",
    "    print(\"Dataframe shape\")\n",
    "    print(df.shape[0])\n",
    "    \n",
    "    # Chunk load data into DB\n",
    "    if file_handle in ['stage_based_catfim.csv']:\n",
    "        print(\"Chunk loading...\")\n",
    "        # Create list of df chunks\n",
    "        n = 1000  #chunk row size\n",
    "        list_df = [df[i:i+n] for i in range(0,df.shape[0],n)]\n",
    "        \n",
    "        # Load the first chunk into the DB as a new table\n",
    "        first_chunk_df = list_df[0]\n",
    "        print(first_chunk_df.shape[0])\n",
    "        load_df_into_db(table_name, db_engine, first_chunk_df, epsg=3857, drop_first=True)\n",
    "        \n",
    "        # Load remaining chunks into newly created table\n",
    "        for remaining_chunk in list_df[1:]:\n",
    "            print(remaining_chunk.shape[0])\n",
    "            load_df_into_db(table_name, db_engine, remaining_chunk, epsg=3857, drop_first=False)\n",
    "        db_engine.execute(f'CREATE INDEX ON reference.{stripped_layer_name} USING GIST (geom);')\n",
    "        \n",
    "    else:\n",
    "        load_df_into_db(table_name, db_engine, df, epsg=3857, drop_first=True)\n",
    "        db_engine.execute(f'CREATE INDEX ON reference.{stripped_layer_name} USING GIST (geom);')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2ced9d",
   "metadata": {},
   "source": [
    "<h2>10 - UPDATE FLOW-BASED CATFIM DATA IN DB</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a87c54b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fim_4_3_11_0/qa_datasets/flow_based_catfim.csv\n",
      "--> Downloading hydrovis-ti-deployment-us-east-1/fim_4_3_11_0/qa_datasets/flow_based_catfim.csv to brad_data/qc_fim_data/flow_based_catfim.csv\n",
      "Reading file...\n",
      "File read.\n",
      "Retrieving values for *\n",
      "Loading data into DB...\n",
      "Dataframe shape\n",
      "13957\n",
      "Chunk loading...\n",
      "10000\n",
      "Dropping reference.flow_based_catfim if it exists\n",
      "Getting sql to create table\n",
      "CREATE TABLE reference.flow_based_catfim (\n",
      "\"oid\" INTEGER,\n",
      "  \"ahps_lid\" TEXT,\n",
      "  \"magnitude\" TEXT,\n",
      "  \"version\" TEXT,\n",
      "  \"huc8\" TEXT,\n",
      "  \"interval_stage\" DOUBLE PRECISION,\n",
      "  \"name\" TEXT,\n",
      "  \"wfo\" TEXT,\n",
      "  \"rfc\" TEXT,\n",
      "  \"state\" TEXT,\n",
      "  \"county\" TEXT,\n",
      "  \"q\" DOUBLE PRECISION,\n",
      "  \"q_uni\" TEXT,\n",
      "  \"q_src\" TEXT,\n",
      "  \"stage\" DOUBLE PRECISION,\n",
      "  \"stage_uni\" TEXT,\n",
      "  \"s_src\" TEXT,\n",
      "  \"wrds_time\" TEXT,\n",
      "  \"nrldb_time\" TEXT,\n",
      "  \"nwis_time\" TEXT,\n",
      "  \"lat\" DOUBLE PRECISION,\n",
      "  \"lon\" DOUBLE PRECISION,\n",
      "  \"viz\" TEXT,\n",
      "  \"geom\" GEOMETRY(Geometry,3857)\n",
      ")\n",
      "Creating reference.flow_based_catfim\n",
      "Adding data to reference.flow_based_catfim\n",
      "3957\n",
      "Adding data to reference.flow_based_catfim\n",
      "fim_4_3_11_0/qa_datasets/flow_based_catfim_sites.csv\n",
      "--> Downloading hydrovis-ti-deployment-us-east-1/fim_4_3_11_0/qa_datasets/flow_based_catfim_sites.csv to brad_data/qc_fim_data/flow_based_catfim_sites.csv\n",
      "Reading file...\n",
      "File read.\n",
      "Retrieving values for *\n",
      "Loading data into DB...\n",
      "Dataframe shape\n",
      "5195\n",
      "Dropping reference.flow_based_catfim_sites if it exists\n",
      "Getting sql to create table\n",
      "CREATE TABLE reference.flow_based_catfim_sites (\n",
      "\"oid\" INTEGER,\n",
      "  \"wrds_timestamp\" TEXT,\n",
      "  \"nrldb_timestamp\" TEXT,\n",
      "  \"nwis_timestamp\" TEXT,\n",
      "  \"metadata_sources\" TEXT,\n",
      "  \"ahps_lid\" TEXT,\n",
      "  \"usgs_gage\" TEXT,\n",
      "  \"nwm_seg\" TEXT,\n",
      "  \"identifiers_goes_id\" TEXT,\n",
      "  \"identifiers_env_can_gage_id\" DOUBLE PRECISION,\n",
      "  \"nws_data_name\" TEXT,\n",
      "  \"nws_data_wfo\" TEXT,\n",
      "  \"nws_data_rfc\" TEXT,\n",
      "  \"nws_data_geo_rfc\" TEXT,\n",
      "  \"nws_data_latitude\" DOUBLE PRECISION,\n",
      "  \"nws_data_longitude\" DOUBLE PRECISION,\n",
      "  \"nws_data_map_link\" TEXT,\n",
      "  \"nws_data_horizontal_datum_name\" TEXT,\n",
      "  \"nws_data_state\" TEXT,\n",
      "  \"nws_data_county\" TEXT,\n",
      "  \"nws_data_county_code\" DOUBLE PRECISION,\n",
      "  \"nws_data_huc\" TEXT,\n",
      "  \"nws_data_hsa\" TEXT,\n",
      "  \"nws_data_zero_datum\" DOUBLE PRECISION,\n",
      "  \"nws_data_vertical_datum_name\" TEXT,\n",
      "  \"nws_data_rfc_forecast_point\" TEXT,\n",
      "  \"nws_data_rfc_defined_fcst_point\" TEXT,\n",
      "  \"nws_data_riverpoint\" TEXT,\n",
      "  \"usgs_data_name\" TEXT,\n",
      "  \"usgs_data_geo_rfc\" TEXT,\n",
      "  \"usgs_data_latitude\" DOUBLE PRECISION,\n",
      "  \"usgs_data_longitude\" DOUBLE PRECISION,\n",
      "  \"usgs_data_map_link\" TEXT,\n",
      "  \"usgs_data_coord_accuracy_code\" TEXT,\n",
      "  \"usgs_data_latlon_datum_name\" TEXT,\n",
      "  \"usgs_data_coord_method_code\" TEXT,\n",
      "  \"usgs_data_state\" TEXT,\n",
      "  \"usgs_data_huc\" DOUBLE PRECISION,\n",
      "  \"usgs_data_site_type\" TEXT,\n",
      "  \"usgs_data_altitude\" DOUBLE PRECISION,\n",
      "  \"usgs_data_alt_accuracy_code\" DOUBLE PRECISION,\n",
      "  \"usgs_data_alt_datum_code\" TEXT,\n",
      "  \"usgs_data_alt_method_code\" TEXT,\n",
      "  \"usgs_data_drainage_area\" DOUBLE PRECISION,\n",
      "  \"usgs_data_drainage_area_units\" TEXT,\n",
      "  \"usgs_data_contrib_drainage_area\" DOUBLE PRECISION,\n",
      "  \"usgs_data_active\" DOUBLE PRECISION,\n",
      "  \"usgs_data_gages_ii_reference\" DOUBLE PRECISION,\n",
      "  \"nwm_feature_data_downstream_feature_id\" DOUBLE PRECISION,\n",
      "  \"nwm_feature_data_latitude\" DOUBLE PRECISION,\n",
      "  \"nwm_feature_data_longitude\" DOUBLE PRECISION,\n",
      "  \"nwm_feature_data_altitude\" DOUBLE PRECISION,\n",
      "  \"nwm_feature_data_stream_length\" DOUBLE PRECISION,\n",
      "  \"nwm_feature_data_stream_order\" DOUBLE PRECISION,\n",
      "  \"nwm_feature_data_mannings_roughness\" DOUBLE PRECISION,\n",
      "  \"nwm_feature_data_slope\" DOUBLE PRECISION,\n",
      "  \"nwm_feature_data_channel_side_slope\" DOUBLE PRECISION,\n",
      "  \"nwm_feature_data_nhd_waterbody_comid\" DOUBLE PRECISION,\n",
      "  \"env_can_gage_data_name\" DOUBLE PRECISION,\n",
      "  \"env_can_gage_data_latitude\" DOUBLE PRECISION,\n",
      "  \"env_can_gage_data_longitude\" DOUBLE PRECISION,\n",
      "  \"env_can_gage_data_map_link\" DOUBLE PRECISION,\n",
      "  \"env_can_gage_data_drainage_area\" DOUBLE PRECISION,\n",
      "  \"env_can_gage_data_contrib_drainage_area\" DOUBLE PRECISION,\n",
      "  \"env_can_gage_data_water_course\" DOUBLE PRECISION,\n",
      "  \"nws_preferred_name\" TEXT,\n",
      "  \"nws_preferred_latitude\" DOUBLE PRECISION,\n",
      "  \"nws_preferred_longitude\" DOUBLE PRECISION,\n",
      "  \"nws_preferred_latlon_datum_name\" TEXT,\n",
      "  \"nws_preferred_state\" TEXT,\n",
      "  \"nws_preferred_huc\" TEXT,\n",
      "  \"usgs_preferred_name\" TEXT,\n",
      "  \"usgs_preferred_latitude\" DOUBLE PRECISION,\n",
      "  \"usgs_preferred_longitude\" DOUBLE PRECISION,\n",
      "  \"usgs_preferred_latlon_datum_name\" TEXT,\n",
      "  \"usgs_preferred_state\" TEXT,\n",
      "  \"usgs_preferred_huc\" TEXT,\n",
      "  \"crosswalk_datasets_location_nwm_crosswalk_dataset_location_nwm_crosswalk_dataset_id\" DOUBLE PRECISION,\n",
      "  \"crosswalk_datasets_location_nwm_crosswalk_dataset_name\" TEXT,\n",
      "  \"crosswalk_datasets_location_nwm_crosswalk_dataset_description\" TEXT,\n",
      "  \"crosswalk_datasets_nws_usgs_crosswalk_dataset_nws_usgs_crosswalk_dataset_id\" DOUBLE PRECISION,\n",
      "  \"crosswalk_datasets_nws_usgs_crosswalk_dataset_name\" TEXT,\n",
      "  \"crosswalk_datasets_nws_usgs_crosswalk_dataset_description\" TEXT,\n",
      "  \"assigned_crs\" TEXT,\n",
      "  \"huc8\" TEXT,\n",
      "  \"name\" TEXT,\n",
      "  \"states\" TEXT,\n",
      "  \"mapped\" TEXT,\n",
      "  \"status\" TEXT,\n",
      "  \"geom\" GEOMETRY(Geometry,3857)\n",
      ")\n",
      "Creating reference.flow_based_catfim_sites\n",
      "Adding data to reference.flow_based_catfim_sites\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from helper_functions.shared_functions import *\n",
    "import boto3\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "# os.environ['EGIS_DB_HOST'] = \"hv-ti-egis-rds-pg-egdb.c4vzypepnkx3.us-east-1.rds.amazonaws.com\"  #TI DB\n",
    "os.environ['EGIS_DB_HOST'] = \"hv-ti-egis-rds-pg-egdb.c4vzypepnkx3.us-east-1.rds.amazonaws.com\"  #Dev DB\n",
    "os.environ['EGIS_DB_DATABASE'] = \"hydrovis\"\n",
    "os.environ['EGIS_DB_USERNAME'] = \"hydrovis\"\n",
    "os.environ['EGIS_DB_PASSWORD'] = \"hardwork123donehere\"\n",
    "\n",
    "db_type = \"egis\"\n",
    "db_engine = get_db_engine(db_type)\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Define bucket and parent directories.\n",
    "bucket = \"hydrovis-ti-deployment-us-east-1\"\n",
    "parent_directory = \"qc_fim_data\"\n",
    "local_download_parent_directory = f'brad_data/qc_fim_data'\n",
    "\n",
    "file_handles = ['flow_based_catfim.csv', 'flow_based_catfim_sites.csv']\n",
    "\n",
    "for file_handle in file_handles:\n",
    "    # Define path to file to download and its local download path, the download.\n",
    "    filename = f\"{fim_folder}/qa_datasets/{file_handle}\"\n",
    "    print(filename)\n",
    "    local_download_path = os.path.join(local_download_parent_directory, f'{file_handle}')\n",
    "    print(f\"--> Downloading {fim_bucket}/{filename} to {local_download_path}\")\n",
    "    #s3.download_file(fim_bucket, filename, local_download_path)\n",
    "    \n",
    "    #  -- Open file and reformat -- #\n",
    "    print(\"Reading file...\")\n",
    "    df = pd.read_csv(local_download_path)\n",
    "    print(\"File read.\")\n",
    "    # Rename headers.\n",
    "    df = df.rename(columns={'Unnamed: 0': 'oid', 'geometry': 'geom', 'huc':'huc8'})\n",
    "        \n",
    "    # Convert all field names to lowercase (needed for ArcGIS Pro).\n",
    "    df.columns= df.columns.str.lower()\n",
    "\n",
    "    # Remove sites that are in derived.ahps_restricted_sites\n",
    "    restricted_sites_df = get_db_values(\"derived.ahps_restricted_sites\", \"*\")\n",
    "    restricted_dict = restricted_sites_df.to_dict('records')\n",
    "\n",
    "    # Change 'mapped' to 'no' if sites are present in restricted_sites_df\n",
    "    for site in restricted_dict:\n",
    "        nws_lid = site['nws_lid'].lower()\n",
    "        if \"sites\" in file_handle:\n",
    "            #print(True)\n",
    "            #print(nws_lid)\n",
    "            df.loc[df.ahps_lid==nws_lid, 'mapped'] = 'no'\n",
    "            df.loc[df.ahps_lid==nws_lid, 'status'] = site['restricted_reason']\n",
    "            #print(df.loc[df.ahps_lid==nws_lid]['status'])\n",
    "        else:\n",
    "            df.loc[df.ahps_lid==nws_lid, 'viz'] = 'no'\n",
    "            df = df[df['viz']=='yes']\n",
    "    \n",
    "    # Enforce data types on df before loading in DB (TODO: need to create special cases for each layer).\n",
    "    df = df.astype({'huc8': 'str'})\n",
    "    df = df.fillna(0)\n",
    "    try:\n",
    "        df = df.astype({'feature_id': 'int'})\n",
    "        df = df.astype({'feature_id': 'str'})\n",
    "    except KeyError:  # If there is no feature_id field\n",
    "        pass\n",
    "    try:\n",
    "        df = df.astype({'nwm_seg': 'int'})\n",
    "        df = df.astype({'nwm_seg': 'str'})\n",
    "    except KeyError:  # If there is no nwm_seg field\n",
    "        pass\n",
    "    try:\n",
    "        df = df.astype({'usgs_gage': 'int'})\n",
    "        df = df.astype({'usgs_gage': 'str'})\n",
    "    except KeyError:  # If there is no usgs_gage field\n",
    "        pass\n",
    "        \n",
    "    # zfill HUC8 field.\n",
    "    df['huc8'] = df['huc8'].apply(lambda x: x.zfill(8))\n",
    "    \n",
    "    if file_handle in ['flow_based_catfim_sites.csv']:\n",
    "        df = df.astype({'nws_data_rfc_forecast_point': 'str'})\n",
    "        df = df.astype({'nws_data_rfc_defined_fcst_point': 'str'})\n",
    "        df = df.astype({'nws_data_riverpoint': 'str'})\n",
    "        \n",
    "    # Upload df to database.\n",
    "    stripped_layer_name = file_handle.replace(\".csv\", \"\")\n",
    "    table_name = \"reference.\" + stripped_layer_name\n",
    "    print(\"Loading data into DB...\")\n",
    "    \n",
    "    print(\"Dataframe shape\")\n",
    "    print(df.shape[0])\n",
    "    \n",
    "    # Chunk load data into DB\n",
    "    if file_handle in ['flow_based_catfim.csv']:\n",
    "        print(\"Chunk loading...\")\n",
    "        # Create list of df chunks\n",
    "        n = 10000  #chunk row size\n",
    "        list_df = [df[i:i+n] for i in range(0,df.shape[0],n)]\n",
    "        \n",
    "        # Load the first chunk into the DB as a new table\n",
    "        first_chunk_df = list_df[0]\n",
    "        print(first_chunk_df.shape[0])\n",
    "        load_df_into_db(table_name, db_engine, first_chunk_df, epsg=3857, drop_first=True)\n",
    "        \n",
    "        # Load remaining chunks into newly created table\n",
    "        for remaining_chunk in list_df[1:]:\n",
    "            print(remaining_chunk.shape[0])\n",
    "            load_df_into_db(table_name, db_engine, remaining_chunk, epsg=3857, drop_first=False)\n",
    "        db_engine.execute(f'CREATE INDEX ON reference.{stripped_layer_name} USING GIST (geom);')\n",
    "        \n",
    "    else:\n",
    "        load_df_into_db(table_name, db_engine, df, epsg=3857, drop_first=True)\n",
    "        db_engine.execute(f'CREATE INDEX ON reference.{stripped_layer_name} USING GIST (geom);')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97f37f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
