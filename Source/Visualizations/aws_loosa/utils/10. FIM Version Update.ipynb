{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa47408",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions.shared_functions import check_if_s3_key_exists, run_sql_in_db, get_db_engine\n",
    "from helper_functions.viz_classes import database\n",
    "import os\n",
    "import codecs\n",
    "import csv\n",
    "import boto3\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from io import StringIO\n",
    "from sqlalchemy.exc import DataError\n",
    "from sqlalchemy.types import Text\n",
    "\n",
    "fim_version = \"4.3.11.0\"\n",
    "fim_folder = f\"fim_{fim_version.replace('.', '_')}\"\n",
    "hand_datasets = f\"{fim_folder}/hand_datasets\"\n",
    "qa_datasets = f\"{fim_folder}/qa_datasets\"\n",
    "fim_bucket = \"hydrovis-ti-deployment-us-east-1\"\n",
    "fim_crosswalk = os.path.join(hand_datasets, \"crosswalk_table.csv\")\n",
    "pipeline_arn = \"arn:aws:states:us-east-1:526904826677:stateMachine:viz_pipeline_ti\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424df167",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['VIZ_DB_USERNAME'] = ''\n",
    "os.environ['VIZ_DB_PASSWORD'] = ''\n",
    "ti_access_key = ''\n",
    "ti_secret_key = ''\n",
    "ti_token = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d15b693",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client(\"s3\")\n",
    "client = boto3.client('stepfunctions')\n",
    "viz_engine = get_db_engine('viz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab4df22",
   "metadata": {},
   "source": [
    "<h2>1 - UPDATE VLAB REPO WITH NEW FIM VERSION</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3eec0b2",
   "metadata": {},
   "source": [
    "<h2>2 - UPLOAD FIM4 HYDRO ID/FEATURE ID CROSSWALK</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c66890",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Getting column name from {fim_crosswalk}\")\n",
    "data = s3_client.get_object(Bucket=fim_bucket, Key=fim_crosswalk)\n",
    "d_reader = csv.DictReader(codecs.getreader(\"utf-8\")(data[\"Body\"]))\n",
    "headers = d_reader.fieldnames\n",
    "\n",
    "header_str = \"(\"\n",
    "for header in headers:\n",
    "    header_str += header\n",
    "    if header in ['HydroID', 'LakeID', 'feature_id']:\n",
    "        header_str += ' integer,'\n",
    "    elif header in ['BranchID']:\n",
    "        header_str += ' bigint,'\n",
    "    else:\n",
    "        header_str += ' TEXT,'\n",
    "header_str = header_str[:-1] + \")\"\n",
    "\n",
    "\n",
    "db = database(db_type=\"viz\")\n",
    "with db.get_db_connection() as conn, conn.cursor() as cur:\n",
    "    print(f\"Deleting/Creating derived.fim4_featureid_crosswalk using columns {header_str}\")\n",
    "    sql = f\"DROP TABLE IF EXISTS derived.fim4_featureid_crosswalk; CREATE TABLE derived.fim4_featureid_crosswalk {header_str};\"\n",
    "    cur.execute(sql)\n",
    "    conn.commit()\n",
    "\n",
    "    print(f\"Importing {fim_crosswalk} to derived.fim4_featureid_crosswalk\")\n",
    "    sql = f\"\"\"\n",
    "        SELECT aws_s3.table_import_from_s3(\n",
    "           'derived.fim4_featureid_crosswalk',\n",
    "           '', \n",
    "           '(format csv, HEADER true)',\n",
    "           (SELECT aws_commons.create_s3_uri(\n",
    "               '{fim_bucket}',\n",
    "               '{fim_crosswalk}',\n",
    "               'us-east-1'\n",
    "                ) AS s3_uri\n",
    "            ),\n",
    "            aws_commons.create_aws_credentials('{ti_access_key}', '{ti_secret_key}', '{ti_token}')\n",
    "           );\n",
    "        \"\"\"\n",
    "    cur.execute(sql)\n",
    "    conn.commit()\n",
    "\n",
    "    print(f\"Adding fim_version column to derived.fim4_featureid_crosswalk\")\n",
    "    sql = f\"ALTER TABLE derived.fim4_featureid_crosswalk ADD COLUMN fim_version text DEFAULT '{fim_version}';\"\n",
    "    cur.execute(sql)\n",
    "    conn.commit()\n",
    "\n",
    "    print(f\"Renaming columns in derived.fim4_featureid_crosswalk\")\n",
    "    sql = f\"\"\"\n",
    "        ALTER TABLE derived.fim4_featureid_crosswalk RENAME COLUMN HydroID TO hydro_id;\n",
    "        ALTER TABLE derived.fim4_featureid_crosswalk RENAME COLUMN LakeID TO lake_id;\n",
    "        ALTER TABLE derived.fim4_featureid_crosswalk RENAME COLUMN BranchID  TO branch_id;\n",
    "    \"\"\"\n",
    "    cur.execute(sql)\n",
    "    conn.commit()\n",
    "\n",
    "    print(f\"Adding feature id index to derived.fim4_featureid_crosswalk\")\n",
    "    sql = f\"CREATE INDEX fim4_crosswalk_feature_id ON derived.fim4_featureid_crosswalk USING btree (feature_id)\"\n",
    "    cur.execute(sql)\n",
    "    conn.commit()\n",
    "\n",
    "    print(f\"Adding hydro id index to derived.fim4_featureid_crosswalk\")\n",
    "    sql = f\"CREATE INDEX fim4_crosswalk_hydro_id ON derived.fim4_featureid_crosswalk USING btree (hydro_id)\"\n",
    "    cur.execute(sql)\n",
    "    conn.commit()\n",
    "\n",
    "print(f\"Successully updated derived.fim4_featureid_crosswalk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54888022",
   "metadata": {},
   "source": [
    "<h2>3 - UPDATE FIM HAND PROCESSING LAMBDA WITH NEW FIM VERSION</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7988cdb",
   "metadata": {},
   "source": [
    "<h2>4 - UPDATE FIM DATA PREP LAMBDA WITH NEW FIM VERSION</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf60845c",
   "metadata": {},
   "source": [
    "<h2>5 - UPDATE INITIALIZE PIPELINE LAMBDA TO RUN ONLY AEP FIM SERVICE. UNCOMMENT 2 FIM CONFIGS AT A TIME TO RUN THE AEP CONFIGURATION. CHECK FOR STEP FUNCTION FINISHING BEFORE STARTING NEW ONE.</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e028000",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_input = {\n",
    "  \"configuration\": \"reference\",\n",
    "  \"job_type\": \"auto\",\n",
    "  \"data_type\": \"channel\",\n",
    "  \"keep_raw\": False,\n",
    "  \"reference_time\": datetime.now().strftime('%Y-%m-%d 00:00:00'),\n",
    "  \"configuration_data_flow\": {\n",
    "    \"db_max_flows\": [],\n",
    "    \"db_ingest_groups\": [],\n",
    "    \"lambda_max_flows\": []\n",
    "  },\n",
    "  \"pipeline_products\": [\n",
    "    {\n",
    "      \"product\": \"static_nwm_aep_inundation_extent_library\",\n",
    "      \"configuration\": \"reference\",\n",
    "      \"product_type\": \"fim\",\n",
    "      \"run\": True,\n",
    "      \"fim_configs\": [\n",
    "        {\n",
    "          \"name\": \"rf_2_inundation\",\n",
    "          \"target_table\": \"aep_fim.rf_2_inundation\",\n",
    "          \"fim_type\": \"hand\",\n",
    "          \"sql_file\": \"rf_2_inundation\"\n",
    "        }\n",
    "      ],\n",
    "      \"services\": [\n",
    "        \"static_nwm_aep_inundation_extent_library_noaa\"\n",
    "      ],\n",
    "      \"raster_outputs\": {\n",
    "        \"output_bucket\": \"\",\n",
    "        \"output_raster_workspaces\": []\n",
    "      },\n",
    "      \"postprocess_sql\": [],\n",
    "      \"product_summaries\": [],\n",
    "      \"lambda_max_flow_dependent\": False\n",
    "    }\n",
    "  ],\n",
    "  \"sql_rename_dict\": {}\n",
    "}\n",
    "\n",
    "client.start_execution(\n",
    "    stateMachineArn = pipeline_arn,\n",
    "    name = f\"sagemaker_aep_2_{datetime.now().strftime('%Y%m%dT%H%M')}\",\n",
    "    input= json.dumps(pipeline_input)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96220332",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_input = {\n",
    "  \"configuration\": \"reference\",\n",
    "  \"job_type\": \"auto\",\n",
    "  \"data_type\": \"channel\",\n",
    "  \"keep_raw\": False,\n",
    "  \"reference_time\": datetime.now().strftime('%Y-%m-%d 00:00:00'),\n",
    "  \"configuration_data_flow\": {\n",
    "    \"db_max_flows\": [],\n",
    "    \"db_ingest_groups\": [],\n",
    "    \"lambda_max_flows\": []\n",
    "  },\n",
    "  \"pipeline_products\": [\n",
    "    {\n",
    "      \"product\": \"static_nwm_aep_inundation_extent_library\",\n",
    "      \"configuration\": \"reference\",\n",
    "      \"product_type\": \"fim\",\n",
    "      \"run\": True,\n",
    "      \"fim_configs\": [\n",
    "        {\n",
    "          \"name\": \"rf_5_inundation\",\n",
    "          \"target_table\": \"aep_fim.rf_5_inundation\",\n",
    "          \"fim_type\": \"hand\",\n",
    "          \"sql_file\": \"rf_5_inundation\"\n",
    "        }\n",
    "      ],\n",
    "      \"services\": [\n",
    "        \"static_nwm_aep_inundation_extent_library_noaa\"\n",
    "      ],\n",
    "      \"raster_outputs\": {\n",
    "        \"output_bucket\": \"\",\n",
    "        \"output_raster_workspaces\": []\n",
    "      },\n",
    "      \"postprocess_sql\": [],\n",
    "      \"product_summaries\": [],\n",
    "      \"lambda_max_flow_dependent\": False\n",
    "    }\n",
    "  ],\n",
    "  \"sql_rename_dict\": {}\n",
    "}\n",
    "\n",
    "client.start_execution(\n",
    "    stateMachineArn = pipeline_arn,\n",
    "    name = f\"sagemaker_aep_5_{datetime.now().strftime('%Y%m%dT%H%M')}\",\n",
    "    input= json.dumps(pipeline_input)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e08b040",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_input = {\n",
    "  \"configuration\": \"reference\",\n",
    "  \"job_type\": \"auto\",\n",
    "  \"data_type\": \"channel\",\n",
    "  \"keep_raw\": False,\n",
    "  \"reference_time\": datetime.now().strftime('%Y-%m-%d 00:00:00'),\n",
    "  \"configuration_data_flow\": {\n",
    "    \"db_max_flows\": [],\n",
    "    \"db_ingest_groups\": [],\n",
    "    \"lambda_max_flows\": []\n",
    "  },\n",
    "  \"pipeline_products\": [\n",
    "    {\n",
    "      \"product\": \"static_nwm_aep_inundation_extent_library\",\n",
    "      \"configuration\": \"reference\",\n",
    "      \"product_type\": \"fim\",\n",
    "      \"run\": True,\n",
    "      \"fim_configs\": [\n",
    "        {\n",
    "          \"name\": \"rf_10_inundation\",\n",
    "          \"target_table\": \"aep_fim.rf_10_inundation\",\n",
    "          \"fim_type\": \"hand\",\n",
    "          \"sql_file\": \"rf_10_inundation\"\n",
    "        }\n",
    "      ],\n",
    "      \"services\": [\n",
    "        \"static_nwm_aep_inundation_extent_library_noaa\"\n",
    "      ],\n",
    "      \"raster_outputs\": {\n",
    "        \"output_bucket\": \"\",\n",
    "        \"output_raster_workspaces\": []\n",
    "      },\n",
    "      \"postprocess_sql\": [],\n",
    "      \"product_summaries\": [],\n",
    "      \"lambda_max_flow_dependent\": False\n",
    "    }\n",
    "  ],\n",
    "  \"sql_rename_dict\": {}\n",
    "}\n",
    "\n",
    "client.start_execution(\n",
    "    stateMachineArn = pipeline_arn,\n",
    "    name = f\"sagemaker_aep_10_{datetime.now().strftime('%Y%m%dT%H%M')}\",\n",
    "    input= json.dumps(pipeline_input)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adae4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_input = {\n",
    "  \"configuration\": \"reference\",\n",
    "  \"job_type\": \"auto\",\n",
    "  \"data_type\": \"channel\",\n",
    "  \"keep_raw\": False,\n",
    "  \"reference_time\": datetime.now().strftime('%Y-%m-%d 00:00:00'),\n",
    "  \"configuration_data_flow\": {\n",
    "    \"db_max_flows\": [],\n",
    "    \"db_ingest_groups\": [],\n",
    "    \"lambda_max_flows\": []\n",
    "  },\n",
    "  \"pipeline_products\": [\n",
    "    {\n",
    "      \"product\": \"static_nwm_aep_inundation_extent_library\",\n",
    "      \"configuration\": \"reference\",\n",
    "      \"product_type\": \"fim\",\n",
    "      \"run\": True,\n",
    "      \"fim_configs\": [\n",
    "        {\n",
    "          \"name\": \"rf_25_inundation\",\n",
    "          \"target_table\": \"aep_fim.rf_25_inundation\",\n",
    "          \"fim_type\": \"hand\",\n",
    "          \"sql_file\": \"rf_25_inundation\"\n",
    "        }\n",
    "      ],\n",
    "      \"services\": [\n",
    "        \"static_nwm_aep_inundation_extent_library_noaa\"\n",
    "      ],\n",
    "      \"raster_outputs\": {\n",
    "        \"output_bucket\": \"\",\n",
    "        \"output_raster_workspaces\": []\n",
    "      },\n",
    "      \"postprocess_sql\": [],\n",
    "      \"product_summaries\": [],\n",
    "      \"lambda_max_flow_dependent\": False\n",
    "    }\n",
    "  ],\n",
    "  \"sql_rename_dict\": {}\n",
    "}\n",
    "\n",
    "client.start_execution(\n",
    "    stateMachineArn = pipeline_arn,\n",
    "    name = f\"sagemaker_aep_25_{datetime.now().strftime('%Y%m%dT%H%M')}\",\n",
    "    input= json.dumps(pipeline_input)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b8c6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_input = {\n",
    "  \"configuration\": \"reference\",\n",
    "  \"job_type\": \"auto\",\n",
    "  \"data_type\": \"channel\",\n",
    "  \"keep_raw\": False,\n",
    "  \"reference_time\": datetime.now().strftime('%Y-%m-%d 00:00:00'),\n",
    "  \"configuration_data_flow\": {\n",
    "    \"db_max_flows\": [],\n",
    "    \"db_ingest_groups\": [],\n",
    "    \"lambda_max_flows\": []\n",
    "  },\n",
    "  \"pipeline_products\": [\n",
    "    {\n",
    "      \"product\": \"static_nwm_aep_inundation_extent_library\",\n",
    "      \"configuration\": \"reference\",\n",
    "      \"product_type\": \"fim\",\n",
    "      \"run\": True,\n",
    "      \"fim_configs\": [\n",
    "        {\n",
    "          \"name\": \"rf_50_inundation\",\n",
    "          \"target_table\": \"aep_fim.rf_50_inundation\",\n",
    "          \"fim_type\": \"hand\",\n",
    "          \"sql_file\": \"rf_50_inundation\"\n",
    "        }\n",
    "      ],\n",
    "      \"services\": [\n",
    "        \"static_nwm_aep_inundation_extent_library_noaa\"\n",
    "      ],\n",
    "      \"raster_outputs\": {\n",
    "        \"output_bucket\": \"\",\n",
    "        \"output_raster_workspaces\": []\n",
    "      },\n",
    "      \"postprocess_sql\": [],\n",
    "      \"product_summaries\": [],\n",
    "      \"lambda_max_flow_dependent\": False\n",
    "    }\n",
    "  ],\n",
    "  \"sql_rename_dict\": {}\n",
    "}\n",
    "\n",
    "client.start_execution(\n",
    "    stateMachineArn = pipeline_arn,\n",
    "    name = f\"sagemaker_aep_50_{datetime.now().strftime('%Y%m%dT%H%M')}\",\n",
    "    input= json.dumps(pipeline_input)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f21313",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_input = {\n",
    "  \"configuration\": \"reference\",\n",
    "  \"job_type\": \"auto\",\n",
    "  \"data_type\": \"channel\",\n",
    "  \"keep_raw\": False,\n",
    "  \"reference_time\": datetime.now().strftime('%Y-%m-%d 00:00:00'),\n",
    "  \"configuration_data_flow\": {\n",
    "    \"db_max_flows\": [],\n",
    "    \"db_ingest_groups\": [],\n",
    "    \"lambda_max_flows\": []\n",
    "  },\n",
    "  \"pipeline_products\": [\n",
    "    {\n",
    "      \"product\": \"static_nwm_aep_inundation_extent_library\",\n",
    "      \"configuration\": \"reference\",\n",
    "      \"product_type\": \"fim\",\n",
    "      \"run\": True,\n",
    "      \"fim_configs\": [\n",
    "        {\n",
    "          \"name\": \"rf_high_water_inundation\",\n",
    "          \"target_table\": \"aep_fim.rf_high_water_inundation\",\n",
    "          \"fim_type\": \"hand\",\n",
    "          \"sql_file\": \"rf_high_water_inundation\"\n",
    "        }\n",
    "      ],\n",
    "      \"services\": [\n",
    "        \"static_nwm_aep_inundation_extent_library_noaa\"\n",
    "      ],\n",
    "      \"raster_outputs\": {\n",
    "        \"output_bucket\": \"\",\n",
    "        \"output_raster_workspaces\": []\n",
    "      },\n",
    "      \"postprocess_sql\": [],\n",
    "      \"product_summaries\": [],\n",
    "      \"lambda_max_flow_dependent\": False\n",
    "    }\n",
    "  ],\n",
    "  \"sql_rename_dict\": {}\n",
    "}\n",
    "\n",
    "client.start_execution(\n",
    "    stateMachineArn = pipeline_arn,\n",
    "    name = f\"sagemaker_aep_hw_{datetime.now().strftime('%Y%m%dT%H%M')}\",\n",
    "    input= json.dumps(pipeline_input)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc470f2",
   "metadata": {},
   "source": [
    "<h2>6 - RUN CATCHMENT WORKFLOWS 2 CONFIGS AT A TIME. CHECK FOR STEP FUNCTION FINISHING BEFORE STARTING NEW ONE</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f92aaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_input = {\n",
    "  \"configuration\": \"reference\",\n",
    "  \"job_type\": \"auto\",\n",
    "  \"data_type\": \"channel\",\n",
    "  \"keep_raw\": False,\n",
    "  \"reference_time\": datetime.now().strftime('%Y-%m-%d 00:00:00'),\n",
    "  \"configuration_data_flow\": {\n",
    "    \"db_max_flows\": [],\n",
    "    \"db_ingest_groups\": [],\n",
    "    \"lambda_max_flows\": []\n",
    "  },\n",
    "  \"pipeline_products\": [\n",
    "    {\n",
    "      \"product\": \"static_hand_catchments_0_branches\",\n",
    "      \"configuration\": \"reference\",\n",
    "      \"product_type\": \"fim\",\n",
    "      \"run\": True,\n",
    "      \"fim_configs\": [\n",
    "        {\n",
    "          \"name\": \"branch_0_catchments\",\n",
    "          \"target_table\": \"fim_catchments.branch_0_catchments\",\n",
    "          \"fim_type\": \"hand\",\n",
    "          \"sql_file\": \"branch_0_catchments\"\n",
    "        }\n",
    "      ],\n",
    "      \"services\": [\n",
    "        \"static_hand_catchments_0_branches_noaa\"\n",
    "      ],\n",
    "      \"raster_outputs\": {\n",
    "        \"output_bucket\": \"\",\n",
    "        \"output_raster_workspaces\": []\n",
    "      },\n",
    "      \"postprocess_sql\": [],\n",
    "      \"product_summaries\": [],\n",
    "      \"lambda_max_flow_dependent\": False\n",
    "    },\n",
    "    {\n",
    "      \"product\": \"static_hand_catchments_0_branches_hi\",\n",
    "      \"configuration\": \"reference\",\n",
    "      \"product_type\": \"fim\",\n",
    "      \"run\": True,\n",
    "      \"fim_configs\": [\n",
    "        {\n",
    "          \"name\": \"branch_0_catchments_hi\",\n",
    "          \"target_table\": \"fim_catchments.branch_0_catchments_hi\",\n",
    "          \"fim_type\": \"hand\",\n",
    "          \"sql_file\": \"branch_0_catchments_hi\"\n",
    "        }\n",
    "      ],\n",
    "      \"services\": [\n",
    "        \"static_hand_catchments_0_branches_hi_noaa\"\n",
    "      ],\n",
    "      \"raster_outputs\": {\n",
    "        \"output_bucket\": \"\",\n",
    "        \"output_raster_workspaces\": []\n",
    "      },\n",
    "      \"postprocess_sql\": [],\n",
    "      \"product_summaries\": [],\n",
    "      \"lambda_max_flow_dependent\": False\n",
    "    },\n",
    "    {\n",
    "      \"product\": \"static_hand_catchments_0_branches_prvi\",\n",
    "      \"configuration\": \"reference\",\n",
    "      \"product_type\": \"fim\",\n",
    "      \"run\": True,\n",
    "      \"fim_configs\": [\n",
    "        {\n",
    "          \"name\": \"branch_0_catchments_prvi\",\n",
    "          \"target_table\": \"fim_catchments.branch_0_catchments_prvi\",\n",
    "          \"fim_type\": \"hand\",\n",
    "          \"sql_file\": \"branch_0_catchments_prvi\"\n",
    "        }\n",
    "      ],\n",
    "      \"services\": [\n",
    "        \"static_hand_catchments_0_branches_prvi_noaa\"\n",
    "      ],\n",
    "      \"raster_outputs\": {\n",
    "        \"output_bucket\": \"\",\n",
    "        \"output_raster_workspaces\": []\n",
    "      },\n",
    "      \"postprocess_sql\": [],\n",
    "      \"product_summaries\": [],\n",
    "      \"lambda_max_flow_dependent\": False\n",
    "    }\n",
    "  ],\n",
    "  \"sql_rename_dict\": {}\n",
    "}\n",
    "\n",
    "client.start_execution(\n",
    "    stateMachineArn = pipeline_arn,\n",
    "    name = f\"sagemaker_0_catchments_{datetime.now().strftime('%Y%m%dT%H%M')}\",\n",
    "    input= json.dumps(pipeline_input)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8206a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_input = {\n",
    "  \"configuration\": \"reference\",\n",
    "  \"job_type\": \"auto\",\n",
    "  \"data_type\": \"channel\",\n",
    "  \"keep_raw\": False,\n",
    "  \"reference_time\": datetime.now().strftime('%Y-%m-%d 00:00:00'),\n",
    "  \"configuration_data_flow\": {\n",
    "    \"db_max_flows\": [],\n",
    "    \"db_ingest_groups\": [],\n",
    "    \"lambda_max_flows\": []\n",
    "  },\n",
    "  \"pipeline_products\": [\n",
    "    {\n",
    "      \"product\": \"static_hand_catchments_gms_branches\",\n",
    "      \"configuration\": \"reference\",\n",
    "      \"product_type\": \"fim\",\n",
    "      \"run\": True,\n",
    "      \"fim_configs\": [\n",
    "        {\n",
    "          \"name\": \"branch_gms_catchments\",\n",
    "          \"target_table\": \"fim_catchments.branch_gms_catchments\",\n",
    "          \"fim_type\": \"hand\",\n",
    "          \"sql_file\": \"branch_gms_catchments\"\n",
    "        }\n",
    "      ],\n",
    "      \"services\": [\n",
    "        \"static_hand_catchments_gms_branches_noaa\"\n",
    "      ],\n",
    "      \"raster_outputs\": {\n",
    "        \"output_bucket\": \"\",\n",
    "        \"output_raster_workspaces\": []\n",
    "      },\n",
    "      \"postprocess_sql\": [],\n",
    "      \"product_summaries\": [],\n",
    "      \"lambda_max_flow_dependent\": False\n",
    "    },\n",
    "    {\n",
    "      \"product\": \"static_hand_catchments_gms_branches_hi\",\n",
    "      \"configuration\": \"reference\",\n",
    "      \"product_type\": \"fim\",\n",
    "      \"run\": True,\n",
    "      \"fim_configs\": [\n",
    "        {\n",
    "          \"name\": \"branch_gms_catchments_hi\",\n",
    "          \"target_table\": \"fim_catchments.branch_gms_catchments_hi\",\n",
    "          \"fim_type\": \"hand\",\n",
    "          \"sql_file\": \"branch_gms_catchments_hi\"\n",
    "        }\n",
    "      ],\n",
    "      \"services\": [\n",
    "        \"static_hand_catchments_gms_branches_hi_noaa\"\n",
    "      ],\n",
    "      \"raster_outputs\": {\n",
    "        \"output_bucket\": \"\",\n",
    "        \"output_raster_workspaces\": []\n",
    "      },\n",
    "      \"postprocess_sql\": [],\n",
    "      \"product_summaries\": [],\n",
    "      \"lambda_max_flow_dependent\": False\n",
    "    },\n",
    "    {\n",
    "      \"product\": \"static_hand_catchments_gms_branches_prvi\",\n",
    "      \"configuration\": \"reference\",\n",
    "      \"product_type\": \"fim\",\n",
    "      \"run\": True,\n",
    "      \"fim_configs\": [\n",
    "        {\n",
    "          \"name\": \"branch_gms_catchments_prvi\",\n",
    "          \"target_table\": \"fim_catchments.branch_gms_catchments_prvi\",\n",
    "          \"fim_type\": \"hand\",\n",
    "          \"sql_file\": \"branch_gms_catchments_prvi\"\n",
    "        }\n",
    "      ],\n",
    "      \"services\": [\n",
    "        \"static_hand_catchments_gms_branches_prvi_noaa\"\n",
    "      ],\n",
    "      \"raster_outputs\": {\n",
    "        \"output_bucket\": \"\",\n",
    "        \"output_raster_workspaces\": []\n",
    "      },\n",
    "      \"postprocess_sql\": [],\n",
    "      \"product_summaries\": [],\n",
    "      \"lambda_max_flow_dependent\": False\n",
    "    }\n",
    "  ],\n",
    "  \"sql_rename_dict\": {}\n",
    "}\n",
    "\n",
    "client.start_execution(\n",
    "    stateMachineArn = pipeline_arn,\n",
    "    name = f\"sagemaker_gms_catchments_{datetime.now().strftime('%Y%m%dT%H%M')}\",\n",
    "    input= json.dumps(pipeline_input)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cd5db0",
   "metadata": {},
   "source": [
    "<h2>6 - UPDATE RATING CURVES IN DB</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e345b70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sql = '''\n",
    "DROP TABLE IF EXISTS derived.hydrotable;\n",
    "DROP TABLE IF EXISTS derived.usgs_elev_table;\n",
    "\n",
    "SELECT distinct LPAD(huc8::text, 8, '0') as huc8 FROM derived.featureid_huc_crosswalk WHERE huc8 is not null;\n",
    "'''\n",
    "df = run_sql_in_db(sql)\n",
    "\n",
    "col_names = ['HydroID', 'feature_id', 'NextDownID', 'order_', 'Number of Cells',\n",
    "       'SurfaceArea (m2)', 'BedArea (m2)', 'TopWidth (m)', 'LENGTHKM',\n",
    "       'AREASQKM', 'WettedPerimeter (m)', 'HydraulicRadius (m)',\n",
    "       'WetArea (m2)', 'Volume (m3)', 'SLOPE', 'ManningN', 'stage',\n",
    "       'default_discharge_cms', 'default_Volume (m3)', 'default_WetArea (m2)',\n",
    "       'default_HydraulicRadius (m)', 'default_ManningN',\n",
    "       'precalb_discharge_cms', 'calb_coef_spatial', 'HUC', 'LakeID',\n",
    "       'subdiv_applied', 'channel_n', 'overbank_n', 'subdiv_discharge_cms',\n",
    "       'last_updated', 'submitter', 'calb_coef_usgs', 'obs_source',\n",
    "       'calb_coef_final', 'calb_applied', 'discharge_cms']\n",
    "\n",
    "usecols = ['HydroID', 'feature_id', 'stage', 'discharge_cms']\n",
    "\n",
    "paginator = s3_client.get_paginator('list_objects')\n",
    "operation_parameters = {'Bucket': fim_bucket,\n",
    "                        'Prefix': f'{hand_datasets}/',\n",
    "                        'Delimiter': '/'}\n",
    "page_iterator = paginator.paginate(**operation_parameters)\n",
    "page_count = 0\n",
    "for page in page_iterator:\n",
    "    page_count += 1\n",
    "    prefix_objects = page['CommonPrefixes']\n",
    "    for i, prefix_obj in enumerate(prefix_objects):\n",
    "        print(f\"Processing {i+1} of {len(prefix_objects)} on page {page_count}\")\n",
    "        branch_prefix = f'{prefix_obj.get(\"Prefix\")}branches/0/'\n",
    "#         ## UNCOMMENT FOR ALL BRANCHES - NOT JUST 0\n",
    "#         huc_branches_prefix = f'{prefix_obj.get(\"Prefix\")}branches/'\n",
    "#         branches_result = s3_client.list_objects(Bucket=fim_bucket, Prefix=huc_branches_prefix, Delimiter='/')\n",
    "#         branch_prefix_objects = branches_result.get('CommonPrefixes')\n",
    "#         for i, branch_prefix_obj in enumerate(branch_prefix_objects):\n",
    "#             branch_prefix = branch_prefix_obj['Prefix']\n",
    "#         ## END UNCOMMENT\n",
    "        ## [UN]INDENT FROM HERE TO THE END IF [COMMENTED]UNCOMMENTED\n",
    "        branch_files_result = s3_client.list_objects(Bucket=fim_bucket, Prefix=branch_prefix, Delimiter='/')\n",
    "        hydro_table_key = None\n",
    "        usgs_elev_table_key = None\n",
    "        for content_obj in branch_files_result.get('Contents'):\n",
    "            branch_file_prefix = content_obj['Key']\n",
    "            if 'hydroTable' in branch_file_prefix:\n",
    "                hydro_table_key = branch_file_prefix\n",
    "            elif 'usgs_elev_table.csv' in branch_file_prefix:\n",
    "                usgs_elev_table_key = branch_file_prefix\n",
    "\n",
    "        if hydro_table_key and usgs_elev_table_key:\n",
    "#             print(f\"Found usgs_elev_table and hydroTable in {branch_prefix}\")\n",
    "            try:\n",
    "#                 print(\"...Fetching csvs...\")\n",
    "                uet = s3_client.get_object(Bucket=fim_bucket, Key=usgs_elev_table_key)['Body']\n",
    "                ht = s3_client.get_object(Bucket=fim_bucket, Key=hydro_table_key)['Body']\n",
    "#                 print(\"...Reading with pandas...\")\n",
    "                uet_df = pd.read_csv(uet)\n",
    "                ht_df = pd.read_csv(ht, header=0, names=col_names, usecols=usecols)\n",
    "#                 print('...Writing to db...')\n",
    "                uet_df['fim_version'] = fim_version\n",
    "                try:\n",
    "                    drop_indicies = []\n",
    "                    for index, row in uet_df.iterrows():\n",
    "                        if 'HydroID' in row and row['HydroID']:\n",
    "                            iter_hydro_id = row['HydroID']\n",
    "#                             print(f\"Subsetting hydrotable where HydroID == {iter_hydro_id}\")\n",
    "                            ht_df_iter = ht_df[ht_df['HydroID']==iter_hydro_id]\n",
    "                            if ht_df_iter.empty:\n",
    "                                drop_indicies.append(index)\n",
    "                                continue\n",
    "                            ht_df_iter['fim_version'] = fim_version\n",
    "                            ht_df_iter.to_sql(con=viz_engine, schema='derived', name='hydrotable', index=False, if_exists='append')\n",
    "                        else:\n",
    "                            drop_indicies.append(index)\n",
    "                    mod_uet_df = uet_df.drop(drop_indicies)\n",
    "                    mod_uet_df.to_sql(con=viz_engine, dtype={\"location_id\": Text(), \"nws_data_huc\": Text()}, schema='derived', name='usgs_elev_table', index=False, if_exists='append')\n",
    "                except Exception as e:\n",
    "                    print('******************************************')\n",
    "                    print(f'Error encountered on {branch_file_prefix}')\n",
    "                    print(e)\n",
    "                    print('******************************************')       \n",
    "            except Exception as e:\n",
    "                raise e\n",
    "                print(f'Fetch failed: {e}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bdd1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "DROP TABLE IF EXISTS derived.hydrotable_staggered;\n",
    "SELECT\n",
    "    et.location_id,\n",
    "    ht.feature_id,\n",
    "    (stage + et.dem_adj_elevation) * 3.28084 as elevation_ft,\n",
    "    LEAD((stage + et.dem_adj_elevation) * 3.28084) OVER (PARTITION BY ht.feature_id ORDER BY ht.feature_id, stage) as next_elevation_ft,\n",
    "    discharge_cms * 35.3147 as discharge_cfs,\n",
    "    LEAD(discharge_cms * 35.3147) OVER (PARTITION BY ht.feature_id ORDER BY ht.feature_id, stage) as next_discharge_cfs\n",
    "INTO derived.hydrotable_staggered\n",
    "FROM derived.hydrotable AS ht\n",
    "JOIN derived.usgs_elev_table AS et ON ht.\"HydroID\" = et.\"HydroID\" AND et.location_id IS NOT NULL;\n",
    "'''\n",
    "\n",
    "run_sql_in_db(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a16430",
   "metadata": {},
   "outputs": [],
   "source": [
    "usgs_curves_key = f'{qa_datasets}/usgs_rating_curves.csv'\n",
    "obj_body = s3_client.get_object(Bucket=fim_bucket, Key=usgs_curves_key)['Body']\n",
    "df = pd.read_csv(obj_body)\n",
    "df.to_sql(con=viz_engine, schema='derived', name='usgs_rating_curves', index=False, if_exists='replace', chunksize=150000, method='multi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48298ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "DROP TABLE IF EXISTS derived.usgs_rating_curves_staggered;\n",
    "SELECT \n",
    "    location_id,\n",
    "    flow as discharge_cfs, \n",
    "    LEAD(flow) OVER (PARTITION BY location_id ORDER BY location_id, stage) as next_discharge_cfs,\n",
    "    stage,\n",
    "    navd88_datum,\n",
    "    elevation_navd88 as elevation_ft,\n",
    "    LEAD(elevation_navd88) OVER (PARTITION BY location_id ORDER BY location_id, stage) as next_elevation_ft\n",
    "INTO derived.usgs_rating_curves_staggered\n",
    "FROM derived.usgs_rating_curves;\n",
    "'''\n",
    "\n",
    "run_sql_in_db(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7b7e4d",
   "metadata": {},
   "source": [
    "<h2>7 - UPDATE SRC SKILL METRICS IN DB</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f4dc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_stats_key = f'{fim_folder}/qa_datasets/agg_nwm_recurr_flow_elev_stats_location_id.csv'\n",
    "obj_body = s3_client.get_object(Bucket=fim_bucket, Key=src_stats_key)['Body']\n",
    "df = pd.read_csv(obj_body)\n",
    "# df['fim_version'] = fim_version\n",
    "print(df.to_dict('records'))\n",
    "# df.to_sql(con=viz_engine, schema='derived', name='src_skill', index=False, if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510ab3ca",
   "metadata": {},
   "source": [
    "<h2>8 - UPDATE FIM PERFORMANCE METRICS IN DB</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfeaffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from helper_functions.shared_functions import *\n",
    "import boto3\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "# os.environ['EGIS_DB_HOST'] = ''\n",
    "os.environ['EGIS_DB_HOST'] = ''\n",
    "os.environ['EGIS_DB_DATABASE'] = ''\n",
    "os.environ['EGIS_DB_USERNAME'] = ''\n",
    "os.environ['EGIS_DB_PASSWORD'] = ''\n",
    "\n",
    "db_type = \"egis\"\n",
    "db_engine = get_db_engine(db_type)\n",
    "db_type = \"egis\"\n",
    "db_engine = get_db_engine(db_type)\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Define bucket and parent directories.\n",
    "bucket = \"hydrovis-ti-deployment-us-east-1\"\n",
    "parent_directory = \"qc_fim_data\"\n",
    "local_download_parent_directory = f'brad_data/qc_fim_data'\n",
    "\n",
    "file_handles = ['fim_performance_points.csv', 'fim_performance_polys.csv', 'fim_performance_catchments.csv']\n",
    "\n",
    "for file_handle in file_handles:\n",
    "    # Define path to file to download and its local download path, the download.\n",
    "    filename = f\"{fim_folder}/qa_datasets/{file_handle}\"\n",
    "    print(filename)\n",
    "    local_download_path = os.path.join(local_download_parent_directory, f'{file_handle}')\n",
    "    print(f\"--> Downloading {fim_bucket}/{filename} to {local_download_path}\")\n",
    "    s3.download_file(fim_bucket, filename, local_download_path)\n",
    "    \n",
    "    #  -- Open file and reformat -- #\n",
    "    print(\"Reading file...\")\n",
    "    df = pd.read_csv(local_download_path)\n",
    "    print(\"File read.\")\n",
    "    # Rename headers.\n",
    "    if file_handle == 'fim_performance_points.csv':\n",
    "        df = df.rename(columns={'Unnamed: 0': 'oid', 'geometry': 'geom'})\n",
    "    else:\n",
    "        df = df.rename(columns={'Unnamed: 0': 'oid', 'geometry': 'geom', 'huc':'huc8'})\n",
    "        \n",
    "    # Convert all field names to lowercase (needed for ArcGIS Pro).\n",
    "    df.columns= df.columns.str.lower()\n",
    "\n",
    "    # Enforce data types on df before loading in DB (TODO: need to create special cases for each layer).\n",
    "    if file_handle == 'fim_performance_points.csv':\n",
    "        df = df.astype({'huc': 'str'})\n",
    "    else:\n",
    "        df = df.astype({'huc8': 'str'})\n",
    "    df = df.fillna(0)\n",
    "    try:\n",
    "        df = df.astype({'feature_id': 'int'})\n",
    "        df = df.astype({'feature_id': 'str'})\n",
    "    except KeyError:  # If there is no feature_id field\n",
    "        pass\n",
    "    try:\n",
    "        df = df.astype({'nwm_seg': 'int'})\n",
    "        df = df.astype({'nwm_seg': 'str'})\n",
    "    except KeyError:  # If there is no nwm_seg field\n",
    "        pass\n",
    "    try:\n",
    "        df = df.astype({'usgs_gage': 'int'})\n",
    "        df = df.astype({'usgs_gage': 'str'})\n",
    "    except KeyError:  # If there is no usgs_gage field\n",
    "        pass\n",
    "        \n",
    "    # zfill HUC8 field.\n",
    "    if file_handle == 'fim_performance_points.csv':\n",
    "        df['huc'] = df['huc'].apply(lambda x: x.zfill(8))\n",
    "    else:\n",
    "        df['huc8'] = df['huc8'].apply(lambda x: x.zfill(8))\n",
    " \n",
    "    # Upload df to database.\n",
    "    stripped_layer_name = file_handle.replace(\".csv\", \"\")\n",
    "    table_name = \"reference.\" + stripped_layer_name\n",
    "    print(\"Loading data into DB...\")\n",
    "    \n",
    "    # Chunk load data into DB\n",
    "    if file_handle in ['fim_performance_catchments.csv']:\n",
    "        print(\"Chunk loading...\")\n",
    "        # Create list of df chunks\n",
    "        n = 100000  #chunk row size\n",
    "        list_df = [df[i:i+n] for i in range(0,df.shape[0],n)]\n",
    "        \n",
    "        # Load the first chunk into the DB as a new table\n",
    "        first_chunk_df = list_df[0]\n",
    "        print(first_chunk_df.shape[0])\n",
    "        load_df_into_db(table_name, db_engine, first_chunk_df, epsg=3857, drop_first=True)\n",
    "        \n",
    "        # Load remaining chunks into newly created table\n",
    "        for remaining_chunk in list_df[1:]:\n",
    "            print(remaining_chunk.shape[0])\n",
    "            load_df_into_db(table_name, db_engine, remaining_chunk, epsg=3857, drop_first=False)\n",
    "        db_engine.execute(f'CREATE INDEX ON reference.{stripped_layer_name} USING GIST (geom);')\n",
    "        \n",
    "    else:\n",
    "        load_df_into_db(table_name, db_engine, df, epsg=3857, drop_first=True)\n",
    "        db_engine.execute(f'CREATE INDEX ON reference.{stripped_layer_name} USING GIST (geom);')\n",
    "    \n",
    "    if file_handle == 'fim_performance_catchments.csv':\n",
    "        print(\"Making valid...\")\n",
    "        make_valid_sql = f\"\"\"\n",
    "\n",
    "        ALTER TABLE reference.fim_performance_catchments\n",
    "          ADD COLUMN geom_invalid geometry\n",
    "          DEFAULT NULL;\n",
    "\n",
    "        UPDATE reference.fim_performance_catchments\n",
    "          SET geom = ST_MakeValid(geom),\n",
    "              geom_invalid = geom\n",
    "          WHERE NOT ST_IsValid(geom);\n",
    "          \"\"\"\n",
    "\n",
    "        db_engine.execute(make_valid_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850b9c0e",
   "metadata": {},
   "source": [
    "<h2>9 - UPDATE STAGE-BASED CATFIM DATA IN DB</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc813146",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from helper_functions.shared_functions import *\n",
    "import boto3\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "# os.environ['EGIS_DB_HOST'] = ''\n",
    "os.environ['EGIS_DB_HOST'] = ''\n",
    "os.environ['EGIS_DB_DATABASE'] = ''\n",
    "os.environ['EGIS_DB_USERNAME'] = ''\n",
    "os.environ['EGIS_DB_PASSWORD'] = ''\n",
    "\n",
    "db_type = \"egis\"\n",
    "db_engine = get_db_engine(db_type)\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Define bucket and parent directories.\n",
    "bucket = \"hydrovis-ti-deployment-us-east-1\"\n",
    "parent_directory = \"qc_fim_data\"\n",
    "local_download_parent_directory = f'brad_data/qc_fim_data'\n",
    "\n",
    "file_handles = ['stage_based_catfim_sites.csv', 'stage_based_catfim.csv']\n",
    "\n",
    "for file_handle in file_handles:\n",
    "    # Define path to file to download and its local download path, the download.\n",
    "    filename = f\"{fim_folder}/qa_datasets/{file_handle}\"\n",
    "    print(filename)\n",
    "    local_download_path = os.path.join(local_download_parent_directory, f'{file_handle}')\n",
    "    print(f\"--> Downloading {fim_bucket}/{filename} to {local_download_path}\")\n",
    "    s3.download_file(fim_bucket, filename, local_download_path)\n",
    "    \n",
    "    #  -- Open file and reformat -- #\n",
    "    print(\"Reading file...\")\n",
    "    df = pd.read_csv(local_download_path)\n",
    "    print(\"File read.\")\n",
    "    \n",
    "    # Rename headers.\n",
    "    df = df.rename(columns={'Unnamed: 0': 'oid', 'geometry': 'geom', 'huc':'huc8'})\n",
    "        \n",
    "    # Convert all field names to lowercase (needed for ArcGIS Pro).\n",
    "    df.columns= df.columns.str.lower()\n",
    "\n",
    "    # Remove sites that are in derived.ahps_restricted_sites\n",
    "    restricted_sites_df = get_db_values(\"derived.ahps_restricted_sites\", \"*\")\n",
    "    restricted_dict = restricted_sites_df.to_dict('records')\n",
    "\n",
    "    # Change 'mapped' to 'no' if sites are present in restricted_sites_df\n",
    "    for site in restricted_dict:\n",
    "        nws_lid = site['nws_lid'].lower()\n",
    "        if \"sites\" in file_handle:\n",
    "            df.loc[df.ahps_lid==nws_lid, 'mapped'] = 'no'\n",
    "            df.loc[df.ahps_lid==nws_lid, 'status'] = site['restricted_reason']\n",
    "        else:\n",
    "            df.loc[df.ahps_lid==nws_lid, 'viz'] = 'no'\n",
    "            df = df[df['viz']=='yes']  # Subset df to only sites desired for mapping\n",
    "            \n",
    "    for sea_level_site in ['qutg1', 'augg1', 'baxg1', 'lamf1', 'adlg1', 'hrag1', 'stng1']:\n",
    "        if \"sites\" in file_handle:\n",
    "            df.loc[df.ahps_lid==sea_level_site, 'mapped'] = 'no'\n",
    "            df.loc[df.ahps_lid==sea_level_site, 'status'] = 'Stage thresholds seem to be based on sea level and not channel thalweg'\n",
    "        else:\n",
    "            df.loc[df.ahps_lid==sea_level_site, 'viz'] = 'no'\n",
    "            df = df[df['viz']=='yes']  # Subset df to only sites desired for mapping\n",
    "    \n",
    "    # Enforce data types on df before loading in DB (TODO: need to create special cases for each layer).\n",
    "    df = df.astype({'huc8': 'str'})\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    try:\n",
    "        df = df.astype({'feature_id': 'int'})\n",
    "        df = df.astype({'feature_id': 'str'})\n",
    "    except KeyError:  # If there is no feature_id field\n",
    "        pass\n",
    "    try:\n",
    "        df = df.astype({'nwm_seg': 'int'})\n",
    "        df = df.astype({'nwm_seg': 'str'})\n",
    "    except KeyError:  # If there is no nwm_seg field\n",
    "        pass\n",
    "    try:\n",
    "        df = df.astype({'usgs_gage': 'int'})\n",
    "        df = df.astype({'usgs_gage': 'str'})\n",
    "    except KeyError:  # If there is no usgs_gage field\n",
    "        pass\n",
    "        \n",
    "    # zfill HUC8 field.\n",
    "    df['huc8'] = df['huc8'].apply(lambda x: x.zfill(8))\n",
    "    \n",
    "    if file_handle in ['stage_based_catfim_sites.csv']:\n",
    "        df = df.astype({'nws_data_rfc_forecast_point': 'str'})\n",
    "        df = df.astype({'nws_data_rfc_defined_fcst_point': 'str'})\n",
    "        df = df.astype({'nws_data_riverpoint': 'str'})\n",
    "        \n",
    "    # Upload df to database.\n",
    "    stripped_layer_name = file_handle.replace(\".csv\", \"\")\n",
    "    table_name = \"reference.\" + stripped_layer_name\n",
    "    print(\"Loading data into DB...\")\n",
    "    \n",
    "    print(\"Dataframe shape\")\n",
    "    print(df.shape[0])\n",
    "    \n",
    "    # Chunk load data into DB\n",
    "    if file_handle in ['stage_based_catfim.csv']:\n",
    "        print(\"Chunk loading...\")\n",
    "        # Create list of df chunks\n",
    "        n = 1000  #chunk row size\n",
    "        list_df = [df[i:i+n] for i in range(0,df.shape[0],n)]\n",
    "        \n",
    "        # Load the first chunk into the DB as a new table\n",
    "        first_chunk_df = list_df[0]\n",
    "        print(first_chunk_df.shape[0])\n",
    "        load_df_into_db(table_name, db_engine, first_chunk_df, epsg=3857, drop_first=True)\n",
    "        \n",
    "        # Load remaining chunks into newly created table\n",
    "        for remaining_chunk in list_df[1:]:\n",
    "            print(remaining_chunk.shape[0])\n",
    "            load_df_into_db(table_name, db_engine, remaining_chunk, epsg=3857, drop_first=False)\n",
    "        db_engine.execute(f'CREATE INDEX ON reference.{stripped_layer_name} USING GIST (geom);')\n",
    "        \n",
    "    else:\n",
    "        load_df_into_db(table_name, db_engine, df, epsg=3857, drop_first=True)\n",
    "        db_engine.execute(f'CREATE INDEX ON reference.{stripped_layer_name} USING GIST (geom);')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a07257",
   "metadata": {},
   "source": [
    "<h2>10 - UPDATE FLOW-BASED CATFIM DATA IN DB</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d0553f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from helper_functions.shared_functions import *\n",
    "import boto3\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "# os.environ['EGIS_DB_HOST'] = ''\n",
    "os.environ['EGIS_DB_HOST'] = ''\n",
    "os.environ['EGIS_DB_DATABASE'] = ''\n",
    "os.environ['EGIS_DB_USERNAME'] = ''\n",
    "os.environ['EGIS_DB_PASSWORD'] = ''\n",
    "\n",
    "db_type = \"egis\"\n",
    "db_engine = get_db_engine(db_type)\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Define bucket and parent directories.\n",
    "bucket = \"hydrovis-ti-deployment-us-east-1\"\n",
    "parent_directory = \"qc_fim_data\"\n",
    "local_download_parent_directory = f'brad_data/qc_fim_data'\n",
    "\n",
    "file_handles = ['flow_based_catfim.csv', 'flow_based_catfim_sites.csv']\n",
    "\n",
    "for file_handle in file_handles:\n",
    "    # Define path to file to download and its local download path, the download.\n",
    "    filename = f\"{fim_folder}/qa_datasets/{file_handle}\"\n",
    "    print(filename)\n",
    "    local_download_path = os.path.join(local_download_parent_directory, f'{file_handle}')\n",
    "    print(f\"--> Downloading {fim_bucket}/{filename} to {local_download_path}\")\n",
    "    s3.download_file(fim_bucket, filename, local_download_path)\n",
    "    \n",
    "    #  -- Open file and reformat -- #\n",
    "    print(\"Reading file...\")\n",
    "    df = pd.read_csv(local_download_path)\n",
    "    print(\"File read.\")\n",
    "    # Rename headers.\n",
    "    df = df.rename(columns={'Unnamed: 0': 'oid', 'geometry': 'geom', 'huc':'huc8'})\n",
    "        \n",
    "    # Convert all field names to lowercase (needed for ArcGIS Pro).\n",
    "    df.columns= df.columns.str.lower()\n",
    "\n",
    "    # Remove sites that are in derived.ahps_restricted_sites\n",
    "    restricted_sites_df = get_db_values(\"derived.ahps_restricted_sites\", \"*\")\n",
    "    restricted_dict = restricted_sites_df.to_dict('records')\n",
    "\n",
    "    # Change 'mapped' to 'no' if sites are present in restricted_sites_df\n",
    "    for site in restricted_dict:\n",
    "        nws_lid = site['nws_lid'].lower()\n",
    "        if \"sites\" in file_handle:\n",
    "            #print(True)\n",
    "            #print(nws_lid)\n",
    "            df.loc[df.ahps_lid==nws_lid, 'mapped'] = 'no'\n",
    "            df.loc[df.ahps_lid==nws_lid, 'status'] = site['restricted_reason']\n",
    "            #print(df.loc[df.ahps_lid==nws_lid]['status'])\n",
    "        else:\n",
    "            df.loc[df.ahps_lid==nws_lid, 'viz'] = 'no'\n",
    "            df = df[df['viz']=='yes']\n",
    "    \n",
    "    # Enforce data types on df before loading in DB (TODO: need to create special cases for each layer).\n",
    "    df = df.astype({'huc8': 'str'})\n",
    "    df = df.fillna(0)\n",
    "    try:\n",
    "        df = df.astype({'feature_id': 'int'})\n",
    "        df = df.astype({'feature_id': 'str'})\n",
    "    except KeyError:  # If there is no feature_id field\n",
    "        pass\n",
    "    try:\n",
    "        df = df.astype({'nwm_seg': 'int'})\n",
    "        df = df.astype({'nwm_seg': 'str'})\n",
    "    except KeyError:  # If there is no nwm_seg field\n",
    "        pass\n",
    "    try:\n",
    "        df = df.astype({'usgs_gage': 'int'})\n",
    "        df = df.astype({'usgs_gage': 'str'})\n",
    "    except KeyError:  # If there is no usgs_gage field\n",
    "        pass\n",
    "        \n",
    "    # zfill HUC8 field.\n",
    "    df['huc8'] = df['huc8'].apply(lambda x: x.zfill(8))\n",
    "    \n",
    "    if file_handle in ['flow_based_catfim_sites.csv']:\n",
    "        df = df.astype({'nws_data_rfc_forecast_point': 'str'})\n",
    "        df = df.astype({'nws_data_rfc_defined_fcst_point': 'str'})\n",
    "        df = df.astype({'nws_data_riverpoint': 'str'})\n",
    "        \n",
    "    # Upload df to database.\n",
    "    stripped_layer_name = file_handle.replace(\".csv\", \"\")\n",
    "    table_name = \"reference.\" + stripped_layer_name\n",
    "    print(\"Loading data into DB...\")\n",
    "    \n",
    "    print(\"Dataframe shape\")\n",
    "    print(df.shape[0])\n",
    "    \n",
    "    # Chunk load data into DB\n",
    "    if file_handle in ['flow_based_catfim.csv']:\n",
    "        print(\"Chunk loading...\")\n",
    "        # Create list of df chunks\n",
    "        n = 10000  #chunk row size\n",
    "        list_df = [df[i:i+n] for i in range(0,df.shape[0],n)]\n",
    "        \n",
    "        # Load the first chunk into the DB as a new table\n",
    "        first_chunk_df = list_df[0]\n",
    "        print(first_chunk_df.shape[0])\n",
    "        load_df_into_db(table_name, db_engine, first_chunk_df, epsg=3857, drop_first=True)\n",
    "        \n",
    "        # Load remaining chunks into newly created table\n",
    "        for remaining_chunk in list_df[1:]:\n",
    "            print(remaining_chunk.shape[0])\n",
    "            load_df_into_db(table_name, db_engine, remaining_chunk, epsg=3857, drop_first=False)\n",
    "        db_engine.execute(f'CREATE INDEX ON reference.{stripped_layer_name} USING GIST (geom);')\n",
    "        \n",
    "    else:\n",
    "        load_df_into_db(table_name, db_engine, df, epsg=3857, drop_first=True)\n",
    "        db_engine.execute(f'CREATE INDEX ON reference.{stripped_layer_name} USING GIST (geom);')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c65287",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
