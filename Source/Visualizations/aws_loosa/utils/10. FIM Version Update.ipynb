{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bbe6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions.shared_functions import check_if_s3_key_exists, run_sql_in_db, get_db_engine\n",
    "from helper_functions.viz_classes import database\n",
    "import os\n",
    "import codecs\n",
    "import csv\n",
    "import boto3\n",
    "import json\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from sqlalchemy.exc import DataError\n",
    "from sqlalchemy.types import Text\n",
    "\n",
    "fim_version = \"4.3.3.4\"\n",
    "fim_folder = f\"fim_{fim_version.replace('.', '_')}\"\n",
    "fim_bucket = \"hydrovis-ti-deployment-us-east-1\"\n",
    "fim_crosswalk = os.path.join(fim_folder, \"crosswalk_table.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb95da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['VIZ_DB_USERNAME'] = \"viz_proc_admin_rw_user\"\n",
    "os.environ['VIZ_DB_PASSWORD'] = \"QLQW31ZxL0CuE78Rm1lvjYTWg\"\n",
    "ti_access_key = 'ASIAXVLPZTM2SWJW6OPR'\n",
    "ti_secret_key = '1a41cqiZ8AUh4N/azaShJBwZ1ZxO4QdT5zGf5+hf'\n",
    "ti_token = 'IQoJb3JpZ2luX2VjEAQaCXVzLWVhc3QtMSJIMEYCIQDUAudKy1/GTaH2ne2CgyEh3bN5fzOJsVZwMm+0CdKreAIhALm2Ivqgg7xmkWSq1R0tbfhtlyGBLtnZUuc6DQjuzlXSKp4DCNz//////////wEQARoMNTI2OTA0ODI2Njc3Igx2EexzPyiBhJ1jwxcq8gLiZFxXJJ4w2KrTtDS047atxtGw3VET45DmuRqRTaGytDg8Wtm/iK78E1tXQegz6DxuJNYmZ///cHMdXln1s9Aw5GtMwg/x2MTkHRj+cgXFL8a6M1uXzHuxlDlfNc1ig1XqdSgSHL7qY+7JFdOUTdIQMXcew0TFD7w/wuX5V0JYCy5NIK/BNmsQxgNYquZF6bG2gBevFo7rE83TX6jBY21ICIuvgKter4D4MfX7v+I6R77XtvNqgPLE+wjB1K/1LDa2gSJngySEuD9PD3dW8PL9dviSgZgKsg9UDDEZE2+yBOLlMJz9WbZLp1RmOt/g1QKLR2ItkhQYkT//dTClnW2bBNyuOgsRWdFpkLCZ+Fz9E9ZkbFCLKveUYPzVrfuMXSeD5P2JRSB+gK7en2zTu1gco1kWQsVcAS14DGBom9IoeSYmDOwq42E/un34YD0CiU7/NLpvYSNinbwgFx7hpCN8YUIexXkOCdx57EgxtQZOpGxMMP3MrKEGOqUBZF7Qemwip4p3x9CP87NUOkprQ7y/HZn4LJoh0PCYtCoVt28Wwy8A6aeBE255qDg0y5pA7mMdBa5SpJyZIv0QTBneKo0UCLeXENaZPFF40tfXlNAnUrdPN+J5/u79chVGuEWvSREpid/vyr5AeZOXjbAGbYcN29JbSfPUW3QMAUfdCfvot5WxuVP5WP0VPPZYXy7gtmE4fNnRPaHQg+ND24tMCZV3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6166e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client(\"s3\")\n",
    "viz_engine = get_db_engine('viz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea6f947",
   "metadata": {},
   "source": [
    "<h2>1 - UPDATE VLAB REPO WITH NEW FIM VERSION</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e663177",
   "metadata": {},
   "source": [
    "<h2>2 - UPLOAD FIM4 HYDRO ID/FEATURE ID CROSSWALK</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c3d341",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Getting column name from {fim_crosswalk}\")\n",
    "data = s3_client.get_object(Bucket=fim_bucket, Key=fim_crosswalk)\n",
    "d_reader = csv.DictReader(codecs.getreader(\"utf-8\")(data[\"Body\"]))\n",
    "headers = d_reader.fieldnames\n",
    "\n",
    "header_str = \"(\"\n",
    "for header in headers:\n",
    "    header_str += header\n",
    "    if header in ['HydroID', 'LakeID', 'feature_id']:\n",
    "        header_str += ' integer,'\n",
    "    elif header in ['BranchID']:\n",
    "        header_str += ' bigint,'\n",
    "    else:\n",
    "        header_str += ' TEXT,'\n",
    "header_str = header_str[:-1] + \")\"\n",
    "\n",
    "\n",
    "db = database(db_type=\"viz\")\n",
    "with db.get_db_connection() as conn, conn.cursor() as cur:\n",
    "    print(f\"Deleting/Creating {db_schema}.{db_table} using columns {header_str}\")\n",
    "    sql = f\"DROP TABLE IF EXISTS {db_schema}.{db_table}; CREATE TABLE {db_schema}.{db_table} {header_str};\"\n",
    "    cur.execute(sql)\n",
    "    conn.commit()\n",
    "\n",
    "    print(f\"Importing {fim_crosswalk} to {db_schema}.{db_table}\")\n",
    "    sql = f\"\"\"\n",
    "        SELECT aws_s3.table_import_from_s3(\n",
    "           '{db_schema}.{db_table}',\n",
    "           '', \n",
    "           '(format csv, HEADER true)',\n",
    "           (SELECT aws_commons.create_s3_uri(\n",
    "               '{fim_bucket}',\n",
    "               '{fim_crosswalk}',\n",
    "               'us-east-1'\n",
    "                ) AS s3_uri\n",
    "            ),\n",
    "            aws_commons.create_aws_credentials('{ti_access_key}', '{ti_secret_key}', '{ti_token}')\n",
    "           );\n",
    "        \"\"\"\n",
    "    cur.execute(sql)\n",
    "    conn.commit()\n",
    "\n",
    "    print(f\"Adding fim_version column to {db_schema}.{db_table}\")\n",
    "    sql = f\"ALTER TABLE {db_schema}.{db_table} ADD COLUMN fim_version text DEFAULT '{fim_version}';\"\n",
    "    cur.execute(sql)\n",
    "    conn.commit()\n",
    "\n",
    "    print(f\"Renaming columns in {db_schema}.{db_table}\")\n",
    "    sql = f\"\"\"\n",
    "        ALTER TABLE {db_schema}.{db_table} RENAME COLUMN HydroID TO hydro_id;\n",
    "        ALTER TABLE {db_schema}.{db_table} RENAME COLUMN LakeID TO lake_id;\n",
    "        ALTER TABLE {db_schema}.{db_table} RENAME COLUMN BranchID  TO branch_id;\n",
    "    \"\"\"\n",
    "    cur.execute(sql)\n",
    "    conn.commit()\n",
    "\n",
    "    print(f\"Adding feature id index to {db_schema}.{db_table}\")\n",
    "    sql = f\"CREATE INDEX fim4_crosswalk_feature_id ON {db_schema}.{db_table} USING btree (feature_id)\"\n",
    "    cur.execute(sql)\n",
    "    conn.commit()\n",
    "\n",
    "    print(f\"Adding hydro id index to {db_schema}.{db_table}\")\n",
    "    sql = f\"CREATE INDEX fim4_crosswalk_hydro_id ON {db_schema}.{db_table} USING btree (hydro_id)\"\n",
    "    cur.execute(sql)\n",
    "    conn.commit()\n",
    "\n",
    "print(f\"Successully updated {db_schema}.{db_table}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c5c1eb",
   "metadata": {},
   "source": [
    "<h2>3 - UPDATE FIM HUC PROCESSING LAMBDA WITH NEW FIM VERSION</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677de864",
   "metadata": {},
   "source": [
    "<h2>4 - RUN AEP FIM WORKFLOWS 2 CONFIGS AT A TIME. CHECK FOR STEP FUNCTION FINISHING BEFORE STARTING NEW ONE</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a46757a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = f\"\"\"\n",
    "UPDATE admin.services\n",
    "SET run = false\n",
    "WHERE configuration = 'reference'\n",
    "\"\"\"\n",
    "run_sql_in_db(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a990fa2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fim_configs = ['rf_50_inundation', 'rf_high_water_inundation']\n",
    "\n",
    "sql = f\"\"\"\n",
    "UPDATE admin.services\n",
    "SET \n",
    "    fim_configs = array{str(fim_configs).lower()}::text[], run = true\n",
    "WHERE service = 'nwm_aep_fim'\n",
    "\"\"\"\n",
    "run_sql_in_db(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5a93f6",
   "metadata": {},
   "source": [
    "### event = {\n",
    "  \"configuration\": \"reference\",\n",
    "  \"reference_time\": \"2023-04-03 00:00:00\",\n",
    "  \"bucket\": \"hydrovis-ti-nwm-us-east-1\"\n",
    "}\n",
    "\n",
    "client = boto3.client(\"lambda\")\n",
    "response = client.invoke(\n",
    "    FunctionName='arn:aws:lambda:us-east-1:526904826677:function:viz_initialize_pipeline_ti',\n",
    "    InvocationType='Event',\n",
    "    LogType='None',\n",
    "    ClientContext='string',\n",
    "    Payload=json.dumps(event)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6ac578",
   "metadata": {},
   "outputs": [],
   "source": [
    "fim_configs = ['rf_25_inundation', 'rf_10_inundation']\n",
    "\n",
    "sql = f\"\"\"\n",
    "UPDATE admin.services\n",
    "SET \n",
    "    fim_configs = array{str(fim_configs).lower()}::text[], run = true\n",
    "WHERE service = 'nwm_aep_fim'\n",
    "\"\"\"\n",
    "run_sql_in_db(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555d5896",
   "metadata": {},
   "outputs": [],
   "source": [
    "event = {\n",
    "  \"configuration\": \"reference\",\n",
    "  \"reference_time\": \"2023-04-03 00:00:00\",\n",
    "  \"bucket\": \"hydrovis-ti-nwm-us-east-1\"\n",
    "}\n",
    "\n",
    "client = boto3.client(\"lambda\")\n",
    "response = client.invoke(\n",
    "    FunctionName='arn:aws:lambda:us-east-1:526904826677:function:viz_initialize_pipeline_ti',\n",
    "    InvocationType='Event',\n",
    "    LogType='None',\n",
    "    ClientContext='string',\n",
    "    Payload=json.dumps(event)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe430e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "fim_configs = ['rf_5_inundation', 'rf_2_inundation']\n",
    "\n",
    "sql = f\"\"\"\n",
    "UPDATE admin.services\n",
    "SET \n",
    "    fim_configs = array{str(fim_configs).lower()}::text[], run = true\n",
    "WHERE service = 'nwm_aep_fim'\n",
    "\"\"\"\n",
    "run_sql_in_db(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6c8076",
   "metadata": {},
   "outputs": [],
   "source": [
    "event = {\n",
    "  \"configuration\": \"reference\",\n",
    "  \"reference_time\": \"2023-04-03 00:00:00\",\n",
    "  \"bucket\": \"hydrovis-ti-nwm-us-east-1\"\n",
    "}\n",
    "\n",
    "client = boto3.client(\"lambda\")\n",
    "response = client.invoke(\n",
    "    FunctionName='arn:aws:lambda:us-east-1:526904826677:function:viz_initialize_pipeline_ti',\n",
    "    InvocationType='Event',\n",
    "    LogType='None',\n",
    "    ClientContext='string',\n",
    "    Payload=json.dumps(event)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28c06ea",
   "metadata": {},
   "source": [
    "<h2>5 - RUN CATCHMENT WORKFLOWS 2 CONFIGS AT A TIME. CHECK FOR STEP FUNCTION FINISHING BEFORE STARTING NEW ONE</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b9c98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fim_configs = ['branch_0_catchments']\n",
    "\n",
    "sql = f\"\"\"\n",
    "UPDATE admin.services\n",
    "SET run = false\n",
    "WHERE configuration = 'reference';\n",
    "\n",
    "UPDATE admin.services\n",
    "SET \n",
    "    fim_configs = array{str(fim_configs).lower()}::text[], run = true\n",
    "WHERE service = 'branch_0_catchments'\n",
    "\"\"\"\n",
    "run_sql_in_db(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b819ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "event = {\n",
    "  \"configuration\": \"reference\",\n",
    "  \"reference_time\": \"2023-04-03 00:00:00\",\n",
    "  \"bucket\": \"hydrovis-ti-nwm-us-east-1\"\n",
    "}\n",
    "\n",
    "client = boto3.client(\"lambda\")\n",
    "response = client.invoke(\n",
    "    FunctionName='arn:aws:lambda:us-east-1:526904826677:function:viz_initialize_pipeline_ti',\n",
    "    InvocationType='Event',\n",
    "    LogType='None',\n",
    "    ClientContext='string',\n",
    "    Payload=json.dumps(event)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f380bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fim_configs = ['branch_gms_catchments']\n",
    "\n",
    "sql = f\"\"\"\n",
    "UPDATE admin.services\n",
    "SET run = false\n",
    "WHERE configuration = 'reference';\n",
    "\n",
    "UPDATE admin.services\n",
    "SET \n",
    "    fim_configs = array{str(fim_configs).lower()}::text[], run = true\n",
    "WHERE service = 'branch_gms_catchments'\n",
    "\"\"\"\n",
    "run_sql_in_db(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95058a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "event = {\n",
    "  \"configuration\": \"reference\",\n",
    "  \"reference_time\": \"2023-04-03 00:00:00\",\n",
    "  \"bucket\": \"hydrovis-ti-nwm-us-east-1\"\n",
    "}\n",
    "\n",
    "client = boto3.client(\"lambda\")\n",
    "response = client.invoke(\n",
    "    FunctionName='arn:aws:lambda:us-east-1:526904826677:function:viz_initialize_pipeline_ti',\n",
    "    InvocationType='Event',\n",
    "    LogType='None',\n",
    "    ClientContext='string',\n",
    "    Payload=json.dumps(event)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad9a1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fim_configs = ['branch_0_catchments_hi']\n",
    "\n",
    "sql = f\"\"\"\n",
    "UPDATE admin.services\n",
    "SET run = false\n",
    "WHERE configuration = 'reference';\n",
    "\n",
    "UPDATE admin.services\n",
    "SET \n",
    "    fim_configs = array{str(fim_configs).lower()}::text[], run = true\n",
    "WHERE service = 'branch_0_catchments_hi'\n",
    "\"\"\"\n",
    "run_sql_in_db(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a75f141",
   "metadata": {},
   "outputs": [],
   "source": [
    "event = {\n",
    "  \"configuration\": \"reference\",\n",
    "  \"reference_time\": \"2023-04-03 00:00:00\",\n",
    "  \"bucket\": \"hydrovis-ti-nwm-us-east-1\"\n",
    "}\n",
    "\n",
    "client = boto3.client(\"lambda\")\n",
    "response = client.invoke(\n",
    "    FunctionName='arn:aws:lambda:us-east-1:526904826677:function:viz_initialize_pipeline_ti',\n",
    "    InvocationType='Event',\n",
    "    LogType='None',\n",
    "    ClientContext='string',\n",
    "    Payload=json.dumps(event)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b7a208",
   "metadata": {},
   "outputs": [],
   "source": [
    "fim_configs = ['branch_gms_catchments_hi']\n",
    "\n",
    "sql = f\"\"\"\n",
    "UPDATE admin.services\n",
    "SET run = false\n",
    "WHERE configuration = 'reference';\n",
    "\n",
    "UPDATE admin.services\n",
    "SET \n",
    "    fim_configs = array{str(fim_configs).lower()}::text[], run = true\n",
    "WHERE service = 'branch_gms_catchments_hi'\n",
    "\"\"\"\n",
    "run_sql_in_db(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da296955",
   "metadata": {},
   "outputs": [],
   "source": [
    "event = {\n",
    "  \"configuration\": \"reference\",\n",
    "  \"reference_time\": \"2023-04-03 00:00:00\",\n",
    "  \"bucket\": \"hydrovis-ti-nwm-us-east-1\"\n",
    "}\n",
    "\n",
    "client = boto3.client(\"lambda\")\n",
    "response = client.invoke(\n",
    "    FunctionName='arn:aws:lambda:us-east-1:526904826677:function:viz_initialize_pipeline_ti',\n",
    "    InvocationType='Event',\n",
    "    LogType='None',\n",
    "    ClientContext='string',\n",
    "    Payload=json.dumps(event)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a1931c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fim_configs = ['branch_0_catchments_prvi']\n",
    "\n",
    "sql = f\"\"\"\n",
    "UPDATE admin.services\n",
    "SET run = false\n",
    "WHERE configuration = 'reference';\n",
    "\n",
    "UPDATE admin.services\n",
    "SET \n",
    "    fim_configs = array{str(fim_configs).lower()}::text[], run = true\n",
    "WHERE service = 'branch_0_catchments_prvi'\n",
    "\"\"\"\n",
    "run_sql_in_db(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6d246f",
   "metadata": {},
   "outputs": [],
   "source": [
    "event = {\n",
    "  \"configuration\": \"reference\",\n",
    "  \"reference_time\": \"2023-04-03 00:00:00\",\n",
    "  \"bucket\": \"hydrovis-ti-nwm-us-east-1\"\n",
    "}\n",
    "\n",
    "client = boto3.client(\"lambda\")\n",
    "response = client.invoke(\n",
    "    FunctionName='arn:aws:lambda:us-east-1:526904826677:function:viz_initialize_pipeline_ti',\n",
    "    InvocationType='Event',\n",
    "    LogType='None',\n",
    "    ClientContext='string',\n",
    "    Payload=json.dumps(event)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a26389",
   "metadata": {},
   "outputs": [],
   "source": [
    "fim_configs = ['branch_gms_catchments_prvi']\n",
    "\n",
    "sql = f\"\"\"\n",
    "UPDATE admin.services\n",
    "SET run = false\n",
    "WHERE configuration = 'reference';\n",
    "\n",
    "UPDATE admin.services\n",
    "SET \n",
    "    fim_configs = array{str(fim_configs).lower()}::text[], run = true\n",
    "WHERE service = 'branch_gms_catchments_prvi'\n",
    "\"\"\"\n",
    "run_sql_in_db(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d378ee98",
   "metadata": {},
   "outputs": [],
   "source": [
    "event = {\n",
    "  \"configuration\": \"reference\",\n",
    "  \"reference_time\": \"2023-04-03 00:00:00\",\n",
    "  \"bucket\": \"hydrovis-ti-nwm-us-east-1\"\n",
    "}\n",
    "\n",
    "client = boto3.client(\"lambda\")\n",
    "response = client.invoke(\n",
    "    FunctionName='arn:aws:lambda:us-east-1:526904826677:function:viz_initialize_pipeline_ti',\n",
    "    InvocationType='Event',\n",
    "    LogType='None',\n",
    "    ClientContext='string',\n",
    "    Payload=json.dumps(event)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a462aa83",
   "metadata": {},
   "source": [
    "<h2>6 - UPDATE RATING CURVES IN DB</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc06683",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "DROP TABLE IF EXISTS derived.hydrotable;\n",
    "DROP TABLE IF EXISTS derived.usgs_elev_table;\n",
    "\n",
    "SELECT distinct LPAD(huc8::text, 8, '0') as huc8 FROM derived.featureid_huc_crosswalk WHERE huc8 is not null;\n",
    "'''\n",
    "df = run_sql_in_db(sql)\n",
    "\n",
    "col_names = ['HydroID', 'feature_id', 'NextDownID', 'order_', 'Number of Cells',\n",
    "       'SurfaceArea (m2)', 'BedArea (m2)', 'TopWidth (m)', 'LENGTHKM',\n",
    "       'AREASQKM', 'WettedPerimeter (m)', 'HydraulicRadius (m)',\n",
    "       'WetArea (m2)', 'Volume (m3)', 'SLOPE', 'ManningN', 'stage',\n",
    "       'default_discharge_cms', 'default_Volume (m3)', 'default_WetArea (m2)',\n",
    "       'default_HydraulicRadius (m)', 'default_ManningN',\n",
    "       'precalb_discharge_cms', 'calb_coef_spatial', 'HUC', 'LakeID',\n",
    "       'subdiv_applied', 'channel_n', 'overbank_n', 'subdiv_discharge_cms',\n",
    "       'last_updated', 'submitter', 'calb_coef_usgs', 'obs_source',\n",
    "       'calb_coef_final', 'calb_applied', 'discharge_cms']\n",
    "\n",
    "usecols = ['HydroID', 'feature_id', 'stage', 'discharge_cms']\n",
    "\n",
    "paginator = s3_client.get_paginator('list_objects')\n",
    "operation_parameters = {'Bucket': fim_bucket,\n",
    "                        'Prefix': f'{fim_folder}/',\n",
    "                        'Delimiter': '/'}\n",
    "page_iterator = paginator.paginate(**operation_parameters)\n",
    "page_count = 0\n",
    "for page in page_iterator:\n",
    "    page_count += 1\n",
    "    prefix_objects = page['CommonPrefixes']\n",
    "    for i, prefix_obj in enumerate(prefix_objects):\n",
    "        print(f\"Processing {i+1} of {len(prefix_objects)} on page {page_count}\")\n",
    "        branch_prefix = f'{prefix_obj.get(\"Prefix\")}branches/0/'\n",
    "#         ## UNCOMMENT FOR ALL BRANCHES - NOT JUST 0\n",
    "#         huc_branches_prefix = f'{prefix_obj.get(\"Prefix\")}branches/'\n",
    "#         branches_result = s3_client.list_objects(Bucket=fim_bucket, Prefix=huc_branches_prefix, Delimiter='/')\n",
    "#         branch_prefix_objects = branches_result.get('CommonPrefixes')\n",
    "#         for i, branch_prefix_obj in enumerate(branch_prefix_objects):\n",
    "#             branch_prefix = branch_prefix_obj['Prefix']\n",
    "#         ## END UNCOMMENT\n",
    "        ## [UN]INDENT FROM HERE TO THE END IF [COMMENTED]UNCOMMENTED\n",
    "        branch_files_result = s3_client.list_objects(Bucket=fim_bucket, Prefix=branch_prefix, Delimiter='/')\n",
    "        hydro_table_key = None\n",
    "        usgs_elev_table_key = None\n",
    "        for content_obj in branch_files_result.get('Contents'):\n",
    "            branch_file_prefix = content_obj['Key']\n",
    "            if 'hydroTable' in branch_file_prefix:\n",
    "                hydro_table_key = branch_file_prefix\n",
    "            elif 'usgs_elev_table.csv' in branch_file_prefix:\n",
    "                usgs_elev_table_key = branch_file_prefix\n",
    "\n",
    "        if hydro_table_key and usgs_elev_table_key:\n",
    "#             print(f\"Found usgs_elev_table and hydroTable in {branch_prefix}\")\n",
    "            try:\n",
    "#                 print(\"...Fetching csvs...\")\n",
    "                uet = s3_client.get_object(Bucket=fim_bucket, Key=usgs_elev_table_key)['Body']\n",
    "                ht = s3_client.get_object(Bucket=fim_bucket, Key=hydro_table_key)['Body']\n",
    "#                 print(\"...Reading with pandas...\")\n",
    "                uet_df = pd.read_csv(uet)\n",
    "                ht_df = pd.read_csv(ht, header=0, names=col_names, usecols=usecols)\n",
    "#                 print('...Writing to db...')\n",
    "                uet_df['fim_version'] = fim_version\n",
    "                try:\n",
    "                    drop_indicies = []\n",
    "                    for index, row in uet_df.iterrows():\n",
    "                        if 'HydroID' in row and row['HydroID']:\n",
    "                            iter_hydro_id = row['HydroID']\n",
    "#                             print(f\"Subsetting hydrotable where HydroID == {iter_hydro_id}\")\n",
    "                            ht_df_iter = ht_df[ht_df['HydroID']==iter_hydro_id]\n",
    "                            if ht_df_iter.empty:\n",
    "                                drop_indicies.append(index)\n",
    "                                continue\n",
    "                            ht_df_iter['fim_version'] = fim_version\n",
    "                            ht_df_iter.to_sql(con=viz_engine, schema='derived', name='hydrotable', index=False, if_exists='append')\n",
    "                        else:\n",
    "                            drop_indicies.append(index)\n",
    "                    mod_uet_df = uet_df.drop(drop_indicies)\n",
    "                    mod_uet_df.to_sql(con=viz_engine, dtype={\"location_id\": Text(), \"nws_data_huc\": Text()}, schema='derived', name='usgs_elev_table', index=False, if_exists='append')\n",
    "                except Exception as e:\n",
    "                    print('******************************************')\n",
    "                    print(f'Error encountered on {branch_file_prefix}')\n",
    "                    print(e)\n",
    "                    print('******************************************')       \n",
    "            except Exception as e:\n",
    "                raise e\n",
    "                print(f'Fetch failed: {e}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e356373d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "DROP TABLE IF EXISTS derived.hydrotable_staggered;\n",
    "SELECT\n",
    "    et.location_id,\n",
    "    ht.feature_id,\n",
    "    (stage + et.dem_adj_elevation) * 3.28084 as elevation_ft,\n",
    "    LEAD((stage + et.dem_adj_elevation) * 3.28084) OVER (PARTITION BY ht.feature_id ORDER BY ht.feature_id, stage) as next_elevation_ft,\n",
    "    discharge_cms * 35.3147 as discharge_cfs,\n",
    "    LEAD(discharge_cms * 35.3147) OVER (PARTITION BY ht.feature_id ORDER BY ht.feature_id, stage) as next_discharge_cfs\n",
    "INTO derived.hydrotable_staggered\n",
    "FROM derived.hydrotable AS ht\n",
    "JOIN derived.usgs_elev_table AS et ON ht.\"HydroID\" = et.\"HydroID\" AND et.location_id IS NOT NULL;\n",
    "'''\n",
    "\n",
    "run_sql_in_db(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4013a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "usgs_curves_key = f'{fim_folder}/qa_datasets/usgs_rating_curves.csv'\n",
    "obj_body = s3_client.get_object(Bucket=fim_bucket, Key=usgs_curves_key)['Body']\n",
    "df = pd.read_csv(obj_body)\n",
    "df.to_sql(con=viz_engine, schema='derived', name='usgs_rating_curves', index=False, if_exists='replace', chunksize=150000, method='multi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7033b7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "DROP TABLE IF EXISTS derived.usgs_rating_curves_staggered;\n",
    "SELECT \n",
    "    location_id,\n",
    "    flow as discharge_cfs, \n",
    "    LEAD(flow) OVER (PARTITION BY location_id ORDER BY location_id, stage) as next_discharge_cfs,\n",
    "    stage,\n",
    "    navd88_datum,\n",
    "    elevation_navd88 as elevation_ft,\n",
    "    LEAD(elevation_navd88) OVER (PARTITION BY location_id ORDER BY location_id, stage) as next_elevation_ft\n",
    "INTO derived.usgs_rating_curves_staggered\n",
    "FROM derived.usgs_rating_curves;\n",
    "'''\n",
    "\n",
    "run_sql_in_db(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987374a6",
   "metadata": {},
   "source": [
    "<h2>7 - UPDATE SRC SKILL METRICS IN DB</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a361e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_stats_key = f'{fim_folder}/qa_datasets/agg_nwm_recurr_flow_elev_stats_location_id.csv'\n",
    "obj_body = s3_client.get_object(Bucket=fim_bucket, Key=src_stats_key)['Body']\n",
    "df = pd.read_csv(obj_body)\n",
    "# df['fim_version'] = fim_version\n",
    "print(df.to_dict('records'))\n",
    "# df.to_sql(con=viz_engine, schema='derived', name='src_skill', index=False, if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e270278",
   "metadata": {},
   "source": [
    "<h2>8 - UPDATE FIM PERFORMANCE METRICS IN DB</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382ab31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['EGIS_DB_HOST'] = \"hv-ti-egis-rds-pg-egdb.c4vzypepnkx3.us-east-1.rds.amazonaws.com\"  #TI DB\n",
    "os.environ['EGIS_DB_HOST'] = \"hv-ti-egis-rds-pg-egdb.c4vzypepnkx3.us-east-1.rds.amazonaws.com\"  #Dev DB\n",
    "os.environ['EGIS_DB_DATABASE'] = \"hydrovis\"\n",
    "os.environ['EGIS_DB_USERNAME'] = \"hydrovis\"\n",
    "os.environ['EGIS_DB_PASSWORD'] = \"hardwork123donehere\"\n",
    "\n",
    "db_type = \"egis\"\n",
    "db_engine = get_db_engine(db_type)\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Define bucket and parent directories.\n",
    "bucket = \"hydrovis-ti-deployment-us-east-1\"\n",
    "parent_directory = \"qc_fim_data\"\n",
    "local_download_parent_directory = f'brad_data/qc_fim_data'\n",
    "\n",
    "file_handles = ['fim_performance_points.csv', 'fim_performance_polys.csv', 'fim_performance_catchments.csv']\n",
    "\n",
    "for file_handle in file_handles:\n",
    "    # Define path to file to download and its local download path, the download.\n",
    "    filename = f\"{parent_directory}/{file_handle}\"\n",
    "    print(filename)\n",
    "    local_download_path = os.path.join(local_download_parent_directory, f'{file_handle}')\n",
    "    print(f\"--> Downloading {bucket}/{filename} to {local_download_path}\")\n",
    "    s3.download_file(bucket, filename, local_download_path)\n",
    "    \n",
    "    #  -- Open file and reformat -- #\n",
    "    print(\"Reading file...\")\n",
    "    df = pd.read_csv(local_download_path)\n",
    "    print(\"File read.\")\n",
    "    # Rename headers.\n",
    "    if file_handle == 'fim_performance_points.csv':\n",
    "        df = df.rename(columns={'Unnamed: 0': 'oid', 'geometry': 'geom'})\n",
    "    else:\n",
    "        df = df.rename(columns={'Unnamed: 0': 'oid', 'geometry': 'geom', 'huc':'huc8'})\n",
    "        \n",
    "    # Convert all field names to lowercase (needed for ArcGIS Pro).\n",
    "    df.columns= df.columns.str.lower()\n",
    "\n",
    "    # Enforce data types on df before loading in DB (TODO: need to create special cases for each layer).\n",
    "    if file_handle == 'fim_performance_points.csv':\n",
    "        df = df.astype({'huc': 'str'})\n",
    "    else:\n",
    "        df = df.astype({'huc8': 'str'})\n",
    "    df = df.fillna(0)\n",
    "    try:\n",
    "        df = df.astype({'feature_id': 'int'})\n",
    "        df = df.astype({'feature_id': 'str'})\n",
    "    except KeyError:  # If there is no feature_id field\n",
    "        pass\n",
    "    try:\n",
    "        df = df.astype({'nwm_seg': 'int'})\n",
    "        df = df.astype({'nwm_seg': 'str'})\n",
    "    except KeyError:  # If there is no nwm_seg field\n",
    "        pass\n",
    "    try:\n",
    "        df = df.astype({'usgs_gage': 'int'})\n",
    "        df = df.astype({'usgs_gage': 'str'})\n",
    "    except KeyError:  # If there is no usgs_gage field\n",
    "        pass\n",
    "        \n",
    "    # zfill HUC8 field.\n",
    "    if file_handle == 'fim_performance_points.csv':\n",
    "        df['huc'] = df['huc'].apply(lambda x: x.zfill(8))\n",
    "    else:\n",
    "        df['huc8'] = df['huc8'].apply(lambda x: x.zfill(8))\n",
    " \n",
    "    # Upload df to database.\n",
    "    stripped_layer_name = file_handle.replace(\".csv\", \"\")\n",
    "    table_name = \"reference.\" + stripped_layer_name\n",
    "    print(\"Loading data into DB...\")\n",
    "    \n",
    "    # Chunk load data into DB\n",
    "    if file_handle in ['fim_performance_catchments.csv']:\n",
    "        print(\"Chunk loading...\")\n",
    "        # Create list of df chunks\n",
    "        n = 100000  #chunk row size\n",
    "        list_df = [df[i:i+n] for i in range(0,df.shape[0],n)]\n",
    "        \n",
    "        # Load the first chunk into the DB as a new table\n",
    "        first_chunk_df = list_df[0]\n",
    "        print(first_chunk_df.shape[0])\n",
    "        load_df_into_db(table_name, db_engine, first_chunk_df, epsg=3857, drop_first=True)\n",
    "        \n",
    "        # Load remaining chunks into newly created table\n",
    "        for remaining_chunk in list_df[1:]:\n",
    "            print(remaining_chunk.shape[0])\n",
    "            load_df_into_db(table_name, db_engine, remaining_chunk, epsg=3857, drop_first=False)\n",
    "        db_engine.execute(f'CREATE INDEX ON reference.{stripped_layer_name} USING GIST (geom);')\n",
    "        \n",
    "    else:\n",
    "        load_df_into_db(table_name, db_engine, df, epsg=3857, drop_first=True)\n",
    "        db_engine.execute(f'CREATE INDEX ON reference.{stripped_layer_name} USING GIST (geom);')\n",
    "    \n",
    "    if file_handle == 'fim_performance_catchments.csv':\n",
    "        print(\"Making valid...\")\n",
    "        make_valid_sql = f\"\"\"\n",
    "\n",
    "        ALTER TABLE reference.fim_performance_catchments\n",
    "          ADD COLUMN geom_invalid geometry\n",
    "          DEFAULT NULL;\n",
    "\n",
    "        UPDATE reference.fim_performance_catchments\n",
    "          SET geom = ST_MakeValid(geom),\n",
    "              geom_invalid = geom\n",
    "          WHERE NOT ST_IsValid(geom);\n",
    "          \"\"\"\n",
    "\n",
    "        db_engine.execute(make_valid_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64eee04",
   "metadata": {},
   "source": [
    "<h2>9 - UPDATE STAGE-BASED CATFIM DATA IN DB</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc74d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from helper_functions.shared_functions import *\n",
    "import boto3\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "# os.environ['EGIS_DB_HOST'] = \"hv-ti-egis-rds-pg-egdb.c4vzypepnkx3.us-east-1.rds.amazonaws.com\"  #TI DB\n",
    "os.environ['EGIS_DB_HOST'] = \"hv-ti-egis-rds-pg-egdb.c4vzypepnkx3.us-east-1.rds.amazonaws.com\"  #Dev DB\n",
    "os.environ['EGIS_DB_DATABASE'] = \"hydrovis\"\n",
    "os.environ['EGIS_DB_USERNAME'] = \"hydrovis\"\n",
    "os.environ['EGIS_DB_PASSWORD'] = \"hardwork123donehere\"\n",
    "\n",
    "db_type = \"egis\"\n",
    "db_engine = get_db_engine(db_type)\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Define bucket and parent directories.\n",
    "bucket = \"hydrovis-ti-deployment-us-east-1\"\n",
    "parent_directory = \"qc_fim_data\"\n",
    "local_download_parent_directory = f'brad_data/qc_fim_data'\n",
    "\n",
    "file_handles = ['stage_based_catfim.csv', 'stage_based_catfim_sites.csv']\n",
    "\n",
    "for file_handle in file_handles:\n",
    "    # Define path to file to download and its local download path, the download.\n",
    "    filename = f\"{parent_directory}/{file_handle}\"\n",
    "    print(filename)\n",
    "    local_download_path = os.path.join(local_download_parent_directory, f'{file_handle}')\n",
    "    print(f\"--> Downloading {bucket}/{filename} to {local_download_path}\")\n",
    "    #s3.download_file(bucket, filename, local_download_path)\n",
    "    \n",
    "    #  -- Open file and reformat -- #\n",
    "    print(\"Reading file...\")\n",
    "    df = pd.read_csv(local_download_path)\n",
    "    print(\"File read.\")\n",
    "    \n",
    "    # Rename headers.\n",
    "    df = df.rename(columns={'Unnamed: 0': 'oid', 'geometry': 'geom', 'huc':'huc8'})\n",
    "        \n",
    "    # Convert all field names to lowercase (needed for ArcGIS Pro).\n",
    "    df.columns= df.columns.str.lower()\n",
    "\n",
    "    # Remove sites that are in derived.ahps_restricted_sites\n",
    "    restricted_sites_df = get_db_values(\"derived.ahps_restricted_sites\", \"*\")\n",
    "    restricted_dict = restricted_sites_df.to_dict('records')\n",
    "\n",
    "    # Change 'mapped' to 'no' if sites are present in restricted_sites_df\n",
    "    for site in restricted_dict:\n",
    "        nws_lid = site['nws_lid'].lower()\n",
    "        if \"sites\" in file_handle:\n",
    "            df.loc[df.ahps_lid==nws_lid, 'mapped'] = 'no'\n",
    "            df.loc[df.ahps_lid==nws_lid, 'status'] = site['restricted_reason']\n",
    "        else:\n",
    "            df.loc[df.ahps_lid==nws_lid, 'viz'] = 'no'\n",
    "            df = df[df['viz']=='yes']  # Subset df to only sites desired for mapping\n",
    "            \n",
    "    for sea_level_site in ['qutg1', 'augg1', 'baxg1', 'lamf1', 'adlg1', 'hrag1', 'stng1']:\n",
    "        if \"sites\" in file_handle:\n",
    "            df.loc[df.ahps_lid==sea_level_site, 'mapped'] = 'no'\n",
    "            df.loc[df.ahps_lid==sea_level_site, 'status'] = 'Stage thresholds seem to be based on sea level and not channel thalweg'\n",
    "        else:\n",
    "            df.loc[df.ahps_lid==sea_level_site, 'viz'] = 'no'\n",
    "            df = df[df['viz']=='yes']  # Subset df to only sites desired for mapping\n",
    "    \n",
    "    # Enforce data types on df before loading in DB (TODO: need to create special cases for each layer).\n",
    "    df = df.astype({'huc8': 'str'})\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    try:\n",
    "        df = df.astype({'feature_id': 'int'})\n",
    "        df = df.astype({'feature_id': 'str'})\n",
    "    except KeyError:  # If there is no feature_id field\n",
    "        pass\n",
    "    try:\n",
    "        df = df.astype({'nwm_seg': 'int'})\n",
    "        df = df.astype({'nwm_seg': 'str'})\n",
    "    except KeyError:  # If there is no nwm_seg field\n",
    "        pass\n",
    "    try:\n",
    "        df = df.astype({'usgs_gage': 'int'})\n",
    "        df = df.astype({'usgs_gage': 'str'})\n",
    "    except KeyError:  # If there is no usgs_gage field\n",
    "        pass\n",
    "        \n",
    "    # zfill HUC8 field.\n",
    "    df['huc8'] = df['huc8'].apply(lambda x: x.zfill(8))\n",
    "    \n",
    "    if file_handle in ['stage_based_catfim_sites.csv']:\n",
    "        df = df.astype({'nws_data_rfc_forecast_point': 'str'})\n",
    "        df = df.astype({'nws_data_rfc_defined_fcst_point': 'str'})\n",
    "        df = df.astype({'nws_data_riverpoint': 'str'})\n",
    "        \n",
    "    # Upload df to database.\n",
    "    stripped_layer_name = file_handle.replace(\".csv\", \"\")\n",
    "    table_name = \"reference.\" + stripped_layer_name\n",
    "    print(\"Loading data into DB...\")\n",
    "    \n",
    "    print(\"Dataframe shape\")\n",
    "    print(df.shape[0])\n",
    "    \n",
    "    # Chunk load data into DB\n",
    "    if file_handle in ['stage_based_catfim.csv']:\n",
    "        print(\"Chunk loading...\")\n",
    "        # Create list of df chunks\n",
    "        n = 10000  #chunk row size\n",
    "        list_df = [df[i:i+n] for i in range(0,df.shape[0],n)]\n",
    "        \n",
    "        # Load the first chunk into the DB as a new table\n",
    "        first_chunk_df = list_df[0]\n",
    "        print(first_chunk_df.shape[0])\n",
    "        load_df_into_db(table_name, db_engine, first_chunk_df, epsg=3857, drop_first=True)\n",
    "        \n",
    "        # Load remaining chunks into newly created table\n",
    "        for remaining_chunk in list_df[1:]:\n",
    "            print(remaining_chunk.shape[0])\n",
    "            load_df_into_db(table_name, db_engine, remaining_chunk, epsg=3857, drop_first=False)\n",
    "        db_engine.execute(f'CREATE INDEX ON reference.{stripped_layer_name} USING GIST (geom);')\n",
    "        \n",
    "    else:\n",
    "        load_df_into_db(table_name, db_engine, df, epsg=3857, drop_first=True)\n",
    "        db_engine.execute(f'CREATE INDEX ON reference.{stripped_layer_name} USING GIST (geom);')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c641ecf2",
   "metadata": {},
   "source": [
    "<h2>10 - UPDATE FLOW-BASED CATFIM DATA IN DB</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5f007a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from helper_functions.shared_functions import *\n",
    "import boto3\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "# os.environ['EGIS_DB_HOST'] = \"hv-ti-egis-rds-pg-egdb.c4vzypepnkx3.us-east-1.rds.amazonaws.com\"  #TI DB\n",
    "os.environ['EGIS_DB_HOST'] = \"hv-ti-egis-rds-pg-egdb.c4vzypepnkx3.us-east-1.rds.amazonaws.com\"  #Dev DB\n",
    "os.environ['EGIS_DB_DATABASE'] = \"hydrovis\"\n",
    "os.environ['EGIS_DB_USERNAME'] = \"hydrovis\"\n",
    "os.environ['EGIS_DB_PASSWORD'] = \"hardwork123donehere\"\n",
    "\n",
    "db_type = \"egis\"\n",
    "db_engine = get_db_engine(db_type)\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Define bucket and parent directories.\n",
    "bucket = \"hydrovis-ti-deployment-us-east-1\"\n",
    "parent_directory = \"qc_fim_data\"\n",
    "local_download_parent_directory = f'brad_data/qc_fim_data'\n",
    "\n",
    "file_handles = ['flow_based_catfim.csv', 'flow_based_catfim_sites.csv']\n",
    "\n",
    "for file_handle in file_handles:\n",
    "    # Define path to file to download and its local download path, the download.\n",
    "    filename = f\"{parent_directory}/{file_handle}\"\n",
    "    print(filename)\n",
    "    local_download_path = os.path.join(local_download_parent_directory, f'{file_handle}')\n",
    "    print(f\"--> Downloading {bucket}/{filename} to {local_download_path}\")\n",
    "    s3.download_file(bucket, filename, local_download_path)\n",
    "    \n",
    "    #  -- Open file and reformat -- #\n",
    "    print(\"Reading file...\")\n",
    "    df = pd.read_csv(local_download_path)\n",
    "    print(\"File read.\")\n",
    "    # Rename headers.\n",
    "    df = df.rename(columns={'Unnamed: 0': 'oid', 'geometry': 'geom', 'huc':'huc8'})\n",
    "        \n",
    "    # Convert all field names to lowercase (needed for ArcGIS Pro).\n",
    "    df.columns= df.columns.str.lower()\n",
    "\n",
    "    # Remove sites that are in derived.ahps_restricted_sites\n",
    "    restricted_sites_df = get_db_values(\"derived.ahps_restricted_sites\", \"*\")\n",
    "    restricted_dict = restricted_sites_df.to_dict('records')\n",
    "\n",
    "    # Change 'mapped' to 'no' if sites are present in restricted_sites_df\n",
    "    for site in restricted_dict:\n",
    "        nws_lid = site['nws_lid'].lower()\n",
    "        if \"sites\" in file_handle:\n",
    "            #print(True)\n",
    "            #print(nws_lid)\n",
    "            df.loc[df.ahps_lid==nws_lid, 'mapped'] = 'no'\n",
    "            df.loc[df.ahps_lid==nws_lid, 'status'] = site['restricted_reason']\n",
    "            #print(df.loc[df.ahps_lid==nws_lid]['status'])\n",
    "        else:\n",
    "            df.loc[df.ahps_lid==nws_lid, 'viz'] = 'no'\n",
    "            df = df[df['viz']=='yes']\n",
    "    \n",
    "    # Enforce data types on df before loading in DB (TODO: need to create special cases for each layer).\n",
    "    df = df.astype({'huc8': 'str'})\n",
    "    df = df.fillna(0)\n",
    "    try:\n",
    "        df = df.astype({'feature_id': 'int'})\n",
    "        df = df.astype({'feature_id': 'str'})\n",
    "    except KeyError:  # If there is no feature_id field\n",
    "        pass\n",
    "    try:\n",
    "        df = df.astype({'nwm_seg': 'int'})\n",
    "        df = df.astype({'nwm_seg': 'str'})\n",
    "    except KeyError:  # If there is no nwm_seg field\n",
    "        pass\n",
    "    try:\n",
    "        df = df.astype({'usgs_gage': 'int'})\n",
    "        df = df.astype({'usgs_gage': 'str'})\n",
    "    except KeyError:  # If there is no usgs_gage field\n",
    "        pass\n",
    "        \n",
    "    # zfill HUC8 field.\n",
    "    df['huc8'] = df['huc8'].apply(lambda x: x.zfill(8))\n",
    "    \n",
    "    if file_handle in ['flow_based_catfim_sites.csv']:\n",
    "        df = df.astype({'nws_data_rfc_forecast_point': 'str'})\n",
    "        df = df.astype({'nws_data_rfc_defined_fcst_point': 'str'})\n",
    "        df = df.astype({'nws_data_riverpoint': 'str'})\n",
    "        \n",
    "    # Upload df to database.\n",
    "    stripped_layer_name = file_handle.replace(\".csv\", \"\")\n",
    "    table_name = \"reference.\" + stripped_layer_name\n",
    "    print(\"Loading data into DB...\")\n",
    "    \n",
    "    print(\"Dataframe shape\")\n",
    "    print(df.shape[0])\n",
    "    \n",
    "    # Chunk load data into DB\n",
    "    if file_handle in ['flow_based_catfim.csv']:\n",
    "        print(\"Chunk loading...\")\n",
    "        # Create list of df chunks\n",
    "        n = 10000  #chunk row size\n",
    "        list_df = [df[i:i+n] for i in range(0,df.shape[0],n)]\n",
    "        \n",
    "        # Load the first chunk into the DB as a new table\n",
    "        first_chunk_df = list_df[0]\n",
    "        print(first_chunk_df.shape[0])\n",
    "        load_df_into_db(table_name, db_engine, first_chunk_df, epsg=3857, drop_first=True)\n",
    "        \n",
    "        # Load remaining chunks into newly created table\n",
    "        for remaining_chunk in list_df[1:]:\n",
    "            print(remaining_chunk.shape[0])\n",
    "            load_df_into_db(table_name, db_engine, remaining_chunk, epsg=3857, drop_first=False)\n",
    "        db_engine.execute(f'CREATE INDEX ON reference.{stripped_layer_name} USING GIST (geom);')\n",
    "        \n",
    "    else:\n",
    "        load_df_into_db(table_name, db_engine, df, epsg=3857, drop_first=True)\n",
    "        db_engine.execute(f'CREATE INDEX ON reference.{stripped_layer_name} USING GIST (geom);')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
