{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f15f66a5-1f40-4933-8601-b56e2e8134a9",
   "metadata": {},
   "source": [
    "### Notes - Oct 21, 2024 ###\n",
    "This is a copy from 10.FIM Version 4.5.2.11 which included hand data loads plus ras2fim data. \n",
    "We will remove all ras2fim stuff here knowing that sometimes ras2fim will be uploaded on its own.\n",
    "However.. when ras2fim is loaded, some steps here will need to be re-run. Those steps will be\n",
    "duplicated when we build our next ras2fim load. This hand release does not have a ras2fim update so\n",
    "we will keep the one in place.\n",
    "</br></br>\n",
    "All code in here will be reviewed and adjusted as the loads progress. Consider each step to be\n",
    "WIP until you see a load date below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae54700-98da-48c9-95d3-97dde219b8d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load Status for hand 4.5.11.1 - Started Oct 21, 2024\n",
    "\n",
    "#### Add dates to each line as they have been loaded\n",
    "\n",
    "1. `Crosswalk` :  \n",
    "2. `Lambda FIM_PREFIX` :   \n",
    "3. `Lambda FIM_VERSION and Memory` :   \n",
    "4. `ras2fim` :  No update in this release. Keep the last load.\n",
    "5. `ras2fim Boundaries`: No update required for this release.\n",
    "6. `AEP`\n",
    "    - `2 year` :  -- \n",
    "    - `5 year` :  -- \n",
    "    - `10 year` :  -- \n",
    "    - `25 year` :  -- \n",
    "    - `50 year` :  -- \n",
    "    - `HW / High Water` :  -- \n",
    "    - `Change the hv-vpp-ti-viz-fim-data-prep Lambda memory back to 2048mb` :  -- \n",
    "7. `Catchments`\n",
    "    - `Branch 0` :  -- \n",
    "    - `GMS` :  -- \n",
    "8. `usgs_elev_table` :  -- \n",
    "9. `hydrotable / hydrotable_staggered` : -- \n",
    "10. `usgs_rating_curve / usgs_rating_curves staggered` : -- \n",
    "11. `Skills Metrics` :  -- \n",
    "12. `FIM Performance` :  -- \n",
    "13. `CatFIM`\n",
    "    - `Stage Based CatFIM` :  -- \n",
    "    - `Flow Based CatFIM` :   -- \n",
    "    - `CatFIM FIM 30` : Stage based only? flow not needed but confirm this.\n",
    "14. `Clear HAND cache` :\n",
    "15. `GIT1 and `Terraform ??` : TBD\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1ee2b5-b109-49e8-8255-f06449b44ee6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell to manually pip reload a packages that the Jupyter engine not retained\n",
    "# !pip install numpy\n",
    "# !pip install geopandas\n",
    "# !pip install pyarrow\n",
    "# !pip install xarray\n",
    "# !pip install geoalchemy2\n",
    "# !pip install contextily\n",
    "# !pip install rioxarray\n",
    "print(\"All loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103a0d12-b2a0-4586-ba0f-0ed6b2c4eb89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pd.set_option(\"max_info_rows\", 100000) # override  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b656259",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import codecs\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import boto3\n",
    "import geopandas as gpd\n",
    "import json\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "import sqlalchemy\n",
    "import xarray as xr\n",
    "\n",
    "from geopandas import GeoDataFrame\n",
    "from io import StringIO\n",
    "from geoalchemy2 import Geometry\n",
    "from shapely import wkt\n",
    "from shapely.geometry import Polygon\n",
    "from sqlalchemy.exc import DataError   # yes, reduntant, fix it later\n",
    "from sqlalchemy.types import Text    # yes, reduntant, fix it later\n",
    "\n",
    "sys.path.append(os.path.join(os.path.abspath(''), '..'))\n",
    "\n",
    "import helper_functions.s3_shared_functions as s3_sf\n",
    "import helper_functions.shared_functions as sf\n",
    "# import helper_functions.viz_classes\n",
    "\n",
    "from helper_functions.viz_classes import database\n",
    "from helper_functions.viz_db_ingest import lambda_handler as execute_db_ingest\n",
    "\n",
    "print(\"imports loaded\")\n",
    "\n",
    "# Note: Aug 2024: Sometimes if you need to do the pip install above, you need to reload this twice.. must be a circular dependency ?? or forced pkg reload\n",
    "# Not sure why yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e944eff3-6023-48f4-9f57-27304a240447",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Aug 5, 2024: # The variable named FIM_VERSION will continue to be the field that joins all data together.\n",
    "# But we need two new public display fields in the UI. We will no longer show a UI field which previously would have been\n",
    "# \"FIM_4_5_2_11\". That won't be displayed anymore. The public field of:\n",
    "#     public_fim_version, for this edition, becomes \"FIM 5_0_0\"  (yes.. three segs)\n",
    "#     public_model_version, for this edition, becomes \"HAND 4_5_2_11\". \n",
    "# When we add ras2fim into the system, it's public_fim_version continues to be FIM 5_0_0,\n",
    "# but ras2fim public_model_version, becomes \"ras2fim 2_0_3_0\"\n",
    "\n",
    "# NOTE: sep 19, 2024: creaing the 4.4.0.0 was just for learning purposes. Now Rob has access to UAT db's so I can compare against\n",
    "#   those next time if needed. We might remove the references to 4.4.0.0 next time\n",
    "\n",
    "OLD_FIM_VERSION = \"4.4.0.0\"\n",
    "NEW_FIM_VERSION = \"4.5.2.11\"\n",
    "PUBLIC_FIM_VERSION = \"FIM 5.0.0\" \n",
    "FIM_MODEL_VERSION = \"HAND 4.5.2.11\"  # on next major build (after Aug 2024, change this to space and dots. ie) HAND 4.5.2.11)\n",
    "OLD_FIM_TAG = OLD_FIM_VERSION.replace('.', '_')\n",
    "\n",
    "FIM_ROOT_DPATH = f\"fim/fim_{NEW_FIM_VERSION.replace('.', '_')}\"\n",
    "HAND_DATASETS_DPATH = f\"{FIM_ROOT_DPATH}/hand_datasets\"\n",
    "QA_DATASETS_DPATH = f\"{FIM_ROOT_DPATH}/qa_datasets\"\n",
    "\n",
    "FIM_BUCKET = \"hydrovis-ti-deployment-us-east-1\"\n",
    "FIM_CROSSWALK_FPATH = os.path.join(HAND_DATASETS_DPATH, \"crosswalk_table.csv\")\n",
    "PIPELINE_ARN = 'arn:aws:states:us-east-1:526904826677:stateMachine:hv-vpp-ti-viz-pipeline'\n",
    "\n",
    "COLUMN_NAME_FIM_VERSION = \"fim_version\"\n",
    "COLUMN_NAME_MODEL_VERSION = \"model_version\"\n",
    "\n",
    "# Sometimes these credential values get updated. To find the latest correct values, go to your AWS Console log page and click on the \"Access Key\"\n",
    "# link to get the latest valid set. Using the \"AWS environment variables\" values.\n",
    "# If this is not set correctly, you will get an HTTP error 400 when you call S3 lower.\n",
    "# You might also see an error of 'An error occurred (NoSuchKey) when calling the GetObject operation: The specified key does not exist.\" the creds are not correct\"\n",
    "\n",
    "# Helps us get to the keys. Note: This was added Oct 16, 2024 and is untested\n",
    "sys.path.append(os.path.join(os.path.abspath(''), '../../../../AWS_Secret_keys'))\n",
    "import AWS_Keys\n",
    "\n",
    "\n",
    "S3_CLIENT = boto3.client(\"s3\")\n",
    "STEPFUNCTION_CLIENT = boto3.client('stepfunctions')\n",
    "VIZ_DB_ENGINE = sf.get_db_engine('viz')\n",
    "\n",
    "print(\"Global Variables loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5827eb-3ca3-4b29-8ee7-f7c4f3c2c736",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2>1 - UPLOAD FIM4 HYDRO ID/FEATURE ID CROSSWALK</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28195858-0cb6-4ad3-8966-db1371a8452a",
   "metadata": {
    "tags": []
   },
   "source": [
    "February 2024 Update from Tyler: This code will need to be updated to handle a new hand_id unique integer that the fim team (Rob Hanna and Matt Luck) has added to the crosswalk, and is now important to fim runs. They also changed the field names / format to match our schema, so this chunk of code should be able to be simplified significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007c59f4-72ff-4e3e-9f99-36a8e830c426",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    " # This was already done for 4.4.0.0, so we can skip it jump from 4.5.2.0 to 4.2.5.11\n",
    "   \n",
    "\n",
    "'''\n",
    "Be Very Careful to just rename tables. If they have indexes, the index will now point to the new\n",
    "table names but maintain the original index name. Those index names can really mess stuff up.\n",
    "Best to never rename unless you rename indexes as well. This particular on is ok. \n",
    "Note: When various '\"to_sql\" tools are run which have GIST indexes, this index column name issue\n",
    "will be the problem.\n",
    "\n",
    "Why Drop instead of Truncate? if the schema changes for the incoming, truncate will have column\n",
    "missmatches.\n",
    "\n",
    "We really should be backing up indexes and constraints as well.\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "# TODO: Aug 2024: Change this to a backup without indexes and not rename, it affects indexes that we might need\n",
    "# tbl_name = \"derived.fim4_featureid_crosswalk\"\n",
    "# new_table_name = f\"{tbl_name}_{OLD_FIM_TAG}\"\n",
    "# sql = f'''\n",
    "#     CREATE TABLE IF NOT EXISTS {new_table_name} AS TABLE {tbl_name};\n",
    "# '''\n",
    "# sf.execute_sql(sql)\n",
    "# print(f\"{tbl_name} copied to {new_table_name} if it does not already exists\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96a49f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "print(f\"Getting column name from {FIM_CROSSWALK_FPATH}\")\n",
    "\n",
    "data = S3_CLIENT.get_object(Bucket=FIM_BUCKET, Key=FIM_CROSSWALK_FPATH)\n",
    "d_reader = csv.DictReader(codecs.getreader(\"utf-8\")(data[\"Body\"]))\n",
    "headers = d_reader.fieldnames\n",
    "\n",
    "# Aug 5, 2024 - Updated column names for 4.5.2.11\n",
    "header_str = \"(\"\n",
    "for header in headers:\n",
    "    header_str += header\n",
    "    if header in ['hand_id', 'hydro_id', 'lake_id']:\n",
    "        header_str += ' integer,'\n",
    "    elif header in ['branch_id', 'feature_id']:\n",
    "        header_str += ' bigint,'\n",
    "    else:\n",
    "        header_str += ' TEXT,'\n",
    "header_str = header_str[:-1] + \")\"\n",
    "print(header_str)\n",
    "\n",
    "db = database(db_type=\"viz\")\n",
    "with db.get_db_connection() as conn, conn.cursor() as cur:\n",
    "    \n",
    "    print(f\"Deleting/Creating derived.fim4_featureid_crosswalk using columns {header_str}\")\n",
    "    sql = f\"DROP TABLE IF EXISTS derived.fim4_featureid_crosswalk; CREATE TABLE derived.fim4_featureid_crosswalk {header_str};\"\n",
    "    cur.execute(sql)\n",
    "    conn.commit()\n",
    "\n",
    "    print(f\"Importing {FIM_CROSSWALK_FPATH} to derived.fim4_featureid_crosswalk\")\n",
    "    sql = f\"\"\"\n",
    "        SELECT aws_s3.table_import_from_s3(\n",
    "           'derived.fim4_featureid_crosswalk',\n",
    "           '', \n",
    "           '(format csv, HEADER true)',\n",
    "           (SELECT aws_commons.create_s3_uri(\n",
    "               '{FIM_BUCKET}',\n",
    "               '{FIM_CROSSWALK_FPATH}',\n",
    "               'us-east-1'\n",
    "                ) AS s3_uri\n",
    "            ),\n",
    "            aws_commons.create_aws_credentials('{TI_ACCESS_KEY}', '{TI_SECRET_KEY}', '{TI_TOKEN}')\n",
    "           );\n",
    "        \"\"\"\n",
    "    cur.execute(sql)\n",
    "    conn.commit()\n",
    "\n",
    "    \n",
    "    # Aug 5, 2024: see notes at the top about the new FIM 5.0.0 system \n",
    "    # We will manually add a couple of new columns for public display\n",
    "    # New columns names are public_fim_version  (FIM_5_0_0) and public model version (FIM_4_5_2_11)\n",
    "    print(f\"Adding {COLUMN_NAME_FIM_VERSION} column to derived.fim4_featureid_crosswalk\")\n",
    "    sql = f\"ALTER TABLE derived.fim4_featureid_crosswalk ADD COLUMN IF NOT EXISTS {COLUMN_NAME_FIM_VERSION} text DEFAULT '{PUBLIC_FIM_VERSION}';\"\n",
    "    cur.execute(sql)\n",
    "    conn.commit()\n",
    "    \n",
    "    print(f\"Adding {COLUMN_NAME_MODEL_VERSION} column to derived.fim4_featureid_crosswalk\")\n",
    "    sql = f\"ALTER TABLE derived.fim4_featureid_crosswalk ADD COLUMN IF NOT EXISTS {COLUMN_NAME_MODEL_VERSION} text DEFAULT '{FIM_MODEL_VERSION}';\"\n",
    "    cur.execute(sql)\n",
    "    conn.commit()\n",
    "\n",
    "    print(\"Adding feature id index to derived.fim4_featureid_crosswalk\")\n",
    "    # Drop it already exists\n",
    "    sql = \"DROP INDEX IF EXISTS derived.fim4_crosswalk_feature_id\"\n",
    "    cur.execute(sql)\n",
    "    conn.commit()    \n",
    "    sql = \"CREATE INDEX fim4_crosswalk_feature_id ON derived.fim4_featureid_crosswalk USING btree (feature_id)\"\n",
    "    cur.execute(sql)\n",
    "    conn.commit()\n",
    "\n",
    "    print(\"Adding hydro id index to derived.fim4_featureid_crosswalk\")\n",
    "    # Drop it already exists\n",
    "    sql = \"DROP INDEX IF EXISTS derived.fim4_crosswalk_hydro_id\"\n",
    "    cur.execute(sql)\n",
    "    conn.commit()    \n",
    "    sql = \"CREATE INDEX fim4_crosswalk_hydro_id ON derived.fim4_featureid_crosswalk USING btree (hydro_id)\"\n",
    "    cur.execute(sql)\n",
    "    conn.commit()\n",
    "\n",
    "print(\"\")\n",
    "print(f\"Successully loaded derived.fim4_featureid_crosswalk and updated it\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23500c40-b088-4b62-aa10-fe2f88c52ef9",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2>2 - UPDATE FIM HAND PROCESSING LAMBDA ENV VARIABLE WITH NEW FIM PREFIX</h2>\n",
    "\n",
    "https://us-east-1.console.aws.amazon.com/lambda/home?region=us-east-1#/functions/hv-vpp-ti-viz-hand-fim-processing?tab=configure\n",
    "\n",
    "Lambda name: hv-vpp-ti-viz-hand-fim-processing\n",
    "\n",
    "In the Configuration Tab, click on the `Environment variables` (left menu), then change the `FIX_PREFIX` to location of the latest hand_dataset you are working on. Referencial to S3 Bucket name.\n",
    "<br>\n",
    "ie) fim/fim_4_5_2_11/hand_datasets\n",
    "\n",
    "Aug 5, 2024: changed my FIM_PREFIX:\n",
    "<br>   from:  fim/fim_4_5_2_0/hand_datasets\n",
    "<br>   to:  fim/fim_4_5_2_11/hand_datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040cf1f3-cdfd-467a-9ac0-5478a285f032",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2>3 - UPDATE FIM DATA PREP LAMBDA ENV VARIABLE WITH NEW FIM VERSION AND MEMORY</h2>\n",
    "\n",
    "https://us-east-1.console.aws.amazon.com/lambda/home?region=us-east-1#/functions/hv-vpp-ti-viz-fim-data-prep?tab=code\n",
    "\n",
    "Lambda name: hv-vpp-ti-viz-hand-fim-processing\n",
    "\n",
    "In the `Configuration` Tab, click on the `Environment variables` (left menu), then change the `FIM_VERSION` to the latest fim model version. \n",
    "<br>\n",
    "ie) 4.5.2.11\n",
    "<br><br>\n",
    "Aug 5, 2024: changed my FIM_VERSION:\n",
    "<br>   from:  4.5.2.0\n",
    "<br>   to:    4.5.2.11\n",
    "<br><br>\n",
    "<b>Then:</b> Still in the Configuration Tab, now click on the `General Configuration` (left menu), followed \n",
    "by the `edit` button on the far right side, to get into the `General Configuration` page details.\n",
    "<br>Change (if they are not already there)\n",
    "<br>Memory (text field) to 4096 (MB)  and\n",
    "<br>Emphermeral Storage tp 1024 (MB)\n",
    "<br>\n",
    "#### Note: Later in these steps we will change the Memory and Emphermal Storage back to default values, see below ####\n",
    "\n",
    "Aug 5, 2024: changed my Memory (4096) and Emphermal (1024):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef11c15-c1c1-46a8-a65a-fa2a0c1dc97b",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2>4 - LOAD AND UPDATE RAS2FIM DATA IN DB</h2>\n",
    "\n",
    "<h3>Note about ras2fim domain extents</h3>\n",
    "As of Aug 2024, a new service came online for a new layer for ras2fim domain extents. Don took care of it.\n",
    "The new extent data was loaded as part of different tools and processes, but we will likly want to consolidate\n",
    "it to here.\n",
    "\n",
    "When ras2fim datasets are released, they come with a \"release\" package that has all of the ras2fim models and geocurves\n",
    "needed here, but also has domain extents for each HUC in the release package. That entire thing is loaded to S3\n",
    "for HV to load. As we will upload a new ras2fim data / geocurves and domain extents at the same time, those load\n",
    "scripts should all stay together (here for now). We can add that next time.\n",
    "\n",
    "However.. ras2fim will likely do releases a lot more regularily than FIM, so it should get it's own independeant load scripts\n",
    "which this script can optionally reference if it likes (well.. future versions of this script, ie FIM 4.8.x.x or whatever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc0a44c-8b46-4db5-a9a0-c65e0bf1e20b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Already done for 4.4.0.0 (4.5.2.11)\n",
    "\n",
    "\n",
    "# TODO: Aug 2024: Change this to a backup without indexes and not rename\n",
    "\n",
    "\n",
    "# By doing a backup, we are leaving the original tables with the indexes and we want to keep them with\n",
    "# ras2fim as it loads geometry and without those pre-existing indexes, loading can fail\n",
    "# tbl_name = \"ras2fim.geocurves\"\n",
    "# new_table_name = f\"{tbl_name}_{OLD_FIM_TAG}\"\n",
    "# sql = f\"CREATE TABLE IF NOT EXISTS {new_table_name} AS TABLE {tbl_name};\"\n",
    "# sf.execute_sql(sql)\n",
    "# print(f\"{tbl_name} copied to {new_table_name} if it does not already exists\")\n",
    "\n",
    "\n",
    "# tbl_name = \"ras2fim.max_geocurves\"\n",
    "# new_table_name = f\"{tbl_name}_{OLD_FIM_TAG}\"\n",
    "# sql = f\"CREATE TABLE IF NOT EXISTS {new_table_name} AS TABLE {tbl_name};\"\n",
    "# sf.execute_sql(sql)\n",
    "# print(f\"{tbl_name} copied to {new_table_name} if it does not already exists\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8c3939-f2fc-42de-921d-423d69e023f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# NOTE: This can be removed in future ras2fim loads.\n",
    "\n",
    "\n",
    "# Temp Aug 2024: We originally just did a table rename for the {table name} to add _4_4_0_0 on it.\n",
    "# Then discovered that renaming it means the indexes are now with the new renamed tables\n",
    "# When we load the ras2fim tables, they can't have some of the indexes in place.\n",
    "# So.. for now, we are going to rename the _4_4_0_0 tables back to their original name, the do the backup\n",
    "# above.\n",
    "#sf.execute_sql(f'ALTER TABLE IF EXISTS ras2fim.geocurves_4_4_0_0 RENAME TO geocurves;')\n",
    "#sf.execute_sql(f'ALTER TABLE IF EXISTS ras2fim.max_geocurves_4_4_0_0 RENAME TO max_geocurves')\n",
    "#print(\"Done renaming them back\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f910bf-4da7-4e62-b9b6-e981f21a23af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# This function is not efficient, but as ras2fim has a built in geometry columns, it loads it as a string and not a \"geometry\" object.\n",
    "# we have to add the records one at a time.\n",
    "\n",
    "# Aug 2024: Maybe eventually I can make this more generic, but for now it is ras2fim specific\n",
    "# see the new one for catfim as it can likely be rolled into one function\n",
    "\n",
    "#  UPDATE: Sep 19, 2024: We had to remove the chunking portion as we discovered that each csv being loaded might have \n",
    "# different crs's. You have to know the incoming crs in order to reproject as the incoming csv's can not be used\n",
    "# to auto detect the crs. We put in cards for the ras2fim team to have all final csv's come out as a standard\n",
    "# projection (perferraly 3857). Going back to chunking will slow down our DB writes and speed it back up again\n",
    "\n",
    "# Most of the temp comment code is still in place for chunking.\n",
    "\n",
    "\n",
    "def load_ras2fim_files_into_db(csv_file_list, s3_source_parent_prefix, schema_name, db_name):\n",
    "\n",
    "    # TODO: change these to params and make more generic\n",
    "    #   also tell that this is only if you have a geometry column (for now)\n",
    "\n",
    "    print(f\"Loading data to database {schema_name}.{db_name}\")\n",
    "    print(\"\")\n",
    "\n",
    "    if len(csv_file_list) == 0:\n",
    "        raise Exception(\"csv file list is empty\")\n",
    "\n",
    "\n",
    "    # source_crs = \"epsg:2277\" # (it is coming in as 5070) but we are changing it to 3857 as loading\n",
    "\n",
    "    # The server has limited memory but it is faster to load as many csv's in at a time\n",
    "    # as resonablty possible. We are going to try it at chunks of 50 (50 csv files) which for ras2fim\n",
    "    # shouls be appx 2,000 records, but for ras2fim V2, we have 750 (ish) files.\n",
    "\n",
    "    # We can leave this open the entire times as well.\n",
    "    s3_client = boto3.client(\"s3\")\n",
    "    default_kwargs = {\"Bucket\": FIM_BUCKET, \"Prefix\": s3_source_parent_prefix}\n",
    "\n",
    "\n",
    "    # chunk_size = 25  # number of csv's to load per set\n",
    "    total_row_count = 0  # all csv row counts combined. You should see this as a record count in the db when done\n",
    "    r2f_df = None  # a re-used concatenating pd dataframe loading up sets of 20 csvs\n",
    "    # is_new_df = True  # After we db load a set, we reset this to start a new set\n",
    "    is_first_db_set = True  # Very first db load\n",
    "\n",
    "    num_recs = len(csv_file_list)\n",
    "    print(f\"Total number of files to process are {num_recs}\")\n",
    "\n",
    "    # We are going ot keep the db connection open the entire time. \n",
    "    # It is slow to open/close connections\n",
    "    # It \"should not\" ?? block any other scripts / services from usign it\n",
    "    # most Sql servers allow for more than one connection at a time.\n",
    "    db = database(db_type=\"viz\")\n",
    "\n",
    "    for idx, full_file_url in enumerate(csv_file_list):\n",
    "\n",
    "        print(f\"Dowloading {idx + 1} of {num_recs} files: {full_file_url}\")\n",
    "\n",
    "        is_first_db_set = idx == 0\n",
    "        # if idx > 4:\n",
    "        #    return  # stub test\n",
    "\n",
    "#        if is_new_df is True:\n",
    "        s3_client = boto3.client(\"s3\")\n",
    "        default_kwargs = {\"Bucket\": FIM_BUCKET, \"Prefix\": s3_source_parent_prefix}\n",
    "\n",
    "\n",
    "        # is_new_df = False\n",
    "        # else:\n",
    "        #     temp_df = pd.read_csv(full_file_url)\n",
    "        #     total_row_count += len(temp_df)\n",
    "        #     r2f_df = pd.concat([r2f_df, temp_df])\n",
    "\n",
    "        # we want it merge into the db on each xth (chunk size) record or the last record\n",
    "        # if ((idx + 1) == num_recs) or ((idx+1) % chunk_size == 0):\n",
    "\n",
    "        # download the csv via pandas into a dataframe\n",
    "        r2f_df = pd.read_csv(full_file_url)\n",
    "\n",
    "        total_row_count += len(r2f_df)\n",
    "        r2f_df = r2f_df.fillna(0)\n",
    "\n",
    "        cur_csv = r2f_df.loc[0, 'crs']\n",
    "        # print(f\"Original crs = {cur_csv}\")\n",
    "\n",
    "        # Create a new source_unit_id which traces back to the folder and code to create\n",
    "        # this specific huc and model in ras2fim\n",
    "        r2f_df['source_unit_id'] = r2f_df.apply(lambda row: row.unit_name + \"_\" + \n",
    "                                                   str(row.unit_version), axis=1)\n",
    "        r2f_df.rename(columns={'source_code': 'feature_id_source_code', 'geometry': 'geom'}, inplace=True)\n",
    "        r2f_df['geom'] = r2f_df['geom'].apply(wkt.loads)\n",
    "\n",
    "        # print(f\"... Next set of downloads and adjustments complete, now to db load - Last Idx: {idx + 1} \")\n",
    "\n",
    "        r2f_geodf = gpd.GeoDataFrame(data=r2f_df, geometry='geom', crs=cur_csv)\n",
    "        # print(r2f_geodf)\n",
    "        # print(\"\")\n",
    "        r2f_reproj = r2f_geodf.to_crs(\"epsg:3857\")\n",
    "\n",
    "        # If this is the first load, the type must be the value of \"replace\", else \"append\"\n",
    "        load_type = 'replace' if is_first_db_set is True else 'append'\n",
    "\n",
    "        r2f_reproj.to_postgis(\n",
    "                name=db_name,\n",
    "                con=VIZ_DB_ENGINE,\n",
    "                schema=schema_name,\n",
    "                if_exists=load_type,\n",
    "                index=False,\n",
    "        )\n",
    "        print(\"... db load complete\")\n",
    "\n",
    "        # Sanity check on crs\n",
    "        # if is_first_db_set:\n",
    "        # print(sf.run_sql_in_db(f\"SELECT ST_SRID(geom) FROM {schema_name}.{db_name} LIMIT 1\"))\n",
    "\n",
    "        r2f_df = None\n",
    "        r2f_geodf = None\n",
    "        r2f_reproj = None\n",
    "        # is_new_df = True  # reset it for the next set\n",
    "        s3_client = None  # resets it so it is not open so long. It timse out if open too long\n",
    "        is_first_db_set = False\n",
    "\n",
    "        # break\n",
    "\n",
    "        # else don't write to db tu continue on to the next record\n",
    "\n",
    "        # end for\n",
    "    print(\"\")\n",
    "    print(\"--------------------------------------------------------------\")\n",
    "    print(\"All records now loaded to the database\")\n",
    "\n",
    "    with db.get_db_connection() as conn, conn.cursor() as cur:\n",
    "        # after all records are loaded to the db.\n",
    "        print(f\"Adding {COLUMN_NAME_FIM_VERSION} column to {schema_name}.{db_name}\")\n",
    "        sql = f\"ALTER TABLE {schema_name}.{db_name} ADD COLUMN IF NOT EXISTS {COLUMN_NAME_FIM_VERSION} text DEFAULT '{PUBLIC_FIM_VERSION}';\"\n",
    "        cur.execute(sql)\n",
    "        conn.commit()\n",
    "\n",
    "        print(f\"Adding {COLUMN_NAME_MODEL_VERSION} column to {schema_name}.{db_name}\")\n",
    "        sql = f\"ALTER TABLE {schema_name}.{db_name} ADD COLUMN IF NOT EXISTS {COLUMN_NAME_MODEL_VERSION} text DEFAULT '{model_version}';\"\n",
    "        cur.execute(sql)\n",
    "        conn.commit()\n",
    "\n",
    "        print(\"Dropping un-necessary columns from DB ...\")\n",
    "        drop_col_names = [\"profile_num\", \"model_id\", \"xs_us\", \"xs_ds\", \"unit_name\", \"unit_version\", \"version\", \"crs\"]\n",
    "        # print(drop_col_names)\n",
    "        # print(\"\")\n",
    "\n",
    "        sql = f\"ALTER TABLE {schema_name}.{db_name} \"\n",
    "        for col_name in drop_col_names:\n",
    "            sql += f\" DROP COLUMN {col_name},\"\n",
    "\n",
    "        # the last char is a comma and we need to change it to be \" CASCASE;\"\n",
    "        sql = sql[0:-1] + \" CASCADE;\"\n",
    "        print(sql)\n",
    "        cur.execute(sql)\n",
    "        conn.commit()\n",
    "        print(f\"Total Rows loaded to DB is {total_row_count}\")\n",
    "        # end of def\n",
    "\n",
    "print(\"Download and db load ras2fim function loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57d7bd1-c852-416e-b259-715b023bf743",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the ras2fim.geocurves\n",
    "\n",
    "# Note: For Aug 2024 (ras 2.0.1.0 with appx 11 hucs, this took appx 1 hr 15 mins to run\n",
    "\n",
    "model_version = \"ras2fim 2.0.1.0\"\n",
    "new_s3_version_folder = \"v2_0\"\n",
    "s3_source_parent_prefix = f\"ras2fim/{new_s3_version_folder}\"\n",
    "\n",
    "start_dt = datetime.now()\n",
    "print(\"\")\n",
    "print(\"Starting loading of ras2fim.geocurves\")\n",
    "\n",
    "\n",
    "# Aug 21 2024, AWS Creds expired and died just after loading rec 2475 of 7948.\n",
    "# Commented out truncate, reset csv_list to be recs 2476 and higher and restarted.\n",
    "# All over exact time lost, but can esimate it.\n",
    "sql = '''\n",
    " TRUNCATE TABLE ras2fim.geocurves;\n",
    " TRUNCATE TABLE ras2fim.max_geocurves;\n",
    "'''\n",
    "print(sf.execute_sql(sql))\n",
    "print(\"geocurves and max_geocurves tables truncated to start clean\")\n",
    "print(\"\")\n",
    "\n",
    "# Now download the s3 geocurves\n",
    "# Overloaded the server as the memory couldn't handle it.\n",
    "# r2f_df = s3_sf.download_S3_csv_files_to_df(FIM_BUCKET, s3_source_parent_prefix, True)\n",
    "\n",
    "# lets just get a list of files, then iterate over them to load each to the db one at a time.\n",
    "r2f_file_names = s3_sf.get_s3_subfolder_file_names(FIM_BUCKET, s3_source_parent_prefix, False)\n",
    "\n",
    "if len(r2f_file_names) == 0:\n",
    "    raise Exception(\"No file names found\")\n",
    "\n",
    "\n",
    "csv_file_list = list(filter(lambda x: (x.endswith(\".csv\") == True), r2f_file_names))\n",
    "if len(csv_file_list) == 0:\n",
    "    raise Exception(\"No csv file names found\")\n",
    "\n",
    "# print(csv_file_list)\n",
    "\n",
    "# Test against just 20 records for a timing test\n",
    "# test_list = csv_file_list[:20]\n",
    "# print(test_list)\n",
    "\n",
    "print(\"Loading df into ras2fim geocurve db\")\n",
    "\n",
    "load_ras2fim_files_into_db(csv_file_list, s3_source_parent_prefix, 'ras2fim', 'geocurves')\n",
    "\n",
    "# See note above about having to restart at rec num 2475 (our index displays were 1 based and not zero based\n",
    "# restart_list = csv_file_list[974:]\n",
    "# load_ras2fim_files_into_db(restart_list, s3_source_parent_prefix, 'ras2fim', 'geocurves')\n",
    "\n",
    "end_dt = datetime.now()\n",
    "time_duration = end_dt - start_dt\n",
    "print(\".... ras2fim files now loaded to ras2fim.geocurves\")\n",
    "print(f\"... duration was  {str(time_duration).split('.')[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded5372c-d94e-4276-be77-b13a94db55cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ras2fim \"previous\" columns loading\n",
    "\n",
    "print(\"... Starting ras2fim previous stage adding and max_geocurves creating\")\n",
    "start_dt = datetime.now()\n",
    "\n",
    "sql = \"\"\"\n",
    "ALTER TABLE IF EXISTS ras2fim.geocurves ADD COLUMN IF NOT EXISTS previous_stage_ft double precision;\n",
    "ALTER TABLE IF EXISTS ras2fim.geocurves ADD COLUMN IF NOT EXISTS previous_stage_m double precision;\n",
    "ALTER TABLE IF EXISTS ras2fim.geocurves ADD COLUMN IF NOT EXISTS previous_discharge_cfs double precision;\n",
    "ALTER TABLE IF EXISTS ras2fim.geocurves ADD COLUMN IF NOT EXISTS previous_discharge_cms double precision;\n",
    "ALTER TABLE IF EXISTS ras2fim.geocurves ADD COLUMN IF NOT EXISTS oid INTEGER PRIMARY KEY GENERATED ALWAYS AS IDENTITY;\n",
    "\"\"\" \n",
    "print(sf.execute_sql(sql))\n",
    "\n",
    "sql = '''DROP TABLE IF EXISTS ras2fim.temp_ras2fim_lagged;'''\n",
    "print(sf.execute_sql(sql))\n",
    "\n",
    "\n",
    "db = database(db_type=\"viz\")\n",
    "with db.get_db_connection() as conn, conn.cursor() as cur:\n",
    "\n",
    "    # PS. It is ok that there are some nulls in the four \"previous\" columns\n",
    "    sql = \"\"\"\n",
    "    CREATE TABLE ras2fim.temp_ras2fim_lagged as (SELECT\n",
    "        feature_id,\n",
    "        stage_ft,\n",
    "        (lag(stage_m, 1) OVER (PARTITION BY feature_id ORDER by stage_m)) as previous_stage_m,\n",
    "        (lag(stage_ft, 1) OVER (PARTITION BY feature_id ORDER by stage_ft)) as previous_stage_ft,\n",
    "        (lag(discharge_cfs, 1) OVER (PARTITION BY feature_id ORDER by discharge_cfs)) as previous_discharge_cfs,\n",
    "        (lag(discharge_cms, 1) OVER (PARTITION BY feature_id ORDER by discharge_cms)) as previous_discharge_cms\n",
    "    FROM ras2fim.geocurves)\n",
    "    \"\"\"\n",
    "    cur.execute(sql)\n",
    "    conn.commit()\n",
    "\n",
    "    sql = \"\"\"\n",
    "        UPDATE ras2fim.geocurves gc\n",
    "        SET previous_stage_ft = lagged.previous_stage_ft,\n",
    "            previous_stage_m = lagged.previous_stage_m,\n",
    "            previous_discharge_cfs = lagged.previous_discharge_cfs,\n",
    "            previous_discharge_cms = lagged.previous_discharge_cms\n",
    "        FROM ras2fim.temp_ras2fim_lagged as lagged\n",
    "        WHERE gc.feature_id = lagged.feature_id\n",
    "            and gc.stage_ft = lagged.stage_ft;\n",
    "    \"\"\"\n",
    "    cur.execute(sql)\n",
    "    conn.commit()\n",
    "\n",
    "\n",
    "print(\"Removing ras2fim.temp_ras2fim_lagged table\")\n",
    "sql = \"DROP TABLE IF EXISTS ras2fim.temp_ras2fim_lagged;\"\n",
    "print(sf.execute_sql(sql))\n",
    "\n",
    "\n",
    "print(\"Adding indexes if required\")\n",
    "\n",
    "sql = \"ALTER TABLE IF EXISTS ras2fim.geocurves OWNER to viz_proc_admin_rw_user;\"\n",
    "print(sf.execute_sql(sql))\n",
    "\n",
    "sql = \"ALTER TABLE IF EXISTS ras2fim.geocurves OWNER to viz_proc_admin_rw_user;\"\n",
    "print(sf.execute_sql(sql))\n",
    "\n",
    "sql = \"DROP INDEX IF EXISTS ras2fim.geocurves_discharge_cms_index;\"\n",
    "print(sf.execute_sql(sql))\n",
    "\n",
    "\n",
    "sql = \"\"\"\n",
    "CREATE INDEX IF NOT EXISTS geocurves_discharge_cms_index ON ras2fim.geocurves USING btree (discharge_cms ASC NULLS LAST)\n",
    "\"\"\"\n",
    "print(sf.execute_sql(sql))\n",
    "\n",
    "\n",
    "sql = \"DROP INDEX IF EXISTS ras2fim.geocurves_feature_id_index;\"\n",
    "print(sf.execute_sql(sql))\n",
    "\n",
    "\n",
    "sql = \"\"\"\n",
    "CREATE INDEX IF NOT EXISTS geocurves_feature_id_index ON ras2fim.geocurves USING btree (feature_id ASC NULLS LAST)\n",
    "\"\"\"\n",
    "print(sf.execute_sql(sql))\n",
    "\n",
    "\n",
    "sql = \"DROP INDEX IF EXISTS ras2fim.geocurves_previous_discharge_cms_index;\"\n",
    "print(sf.execute_sql(sql))\n",
    "\n",
    "\n",
    "sql = \"\"\"\n",
    "CREATE INDEX IF NOT EXISTS geocurves_previous_discharge_cms_index\n",
    "   ON ras2fim.geocurves USING btree (previous_discharge_cms ASC NULLS LAST)\n",
    "\"\"\"\n",
    "print(sf.execute_sql(sql))\n",
    "\n",
    "\n",
    "# Skip for now.. not sure if it will be needed in the next set\n",
    "# -- DROP INDEX IF EXISTS ras2fim.idx_geocurves_geom;\n",
    "# CREATE INDEX IF NOT EXISTS idx_geocurves_geom\n",
    "#     ON ras2fim.geocurves USING gist\n",
    "#     (geom)\n",
    "#     TABLESPACE pg_default;\n",
    "\n",
    "\n",
    "end_dt = datetime.now()\n",
    "time_duration = end_dt - start_dt\n",
    "print(\".... Done - ras2fim previous stage columns added\")\n",
    "print(f\"... duration was  {str(time_duration).split('.')[0]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bd997c-f813-49dd-94e0-adf062f3b74b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ras2fim max_geocurve loading\n",
    "\n",
    "start_dt = datetime.now()\n",
    "\n",
    "print(\"Start of creating and loading max_geocurves table\")\n",
    "\n",
    "# Table can't have any indexes as nothing in unique enough\n",
    "# We shoudl have an oid column though\n",
    "sql = \"DROP TABLE IF EXISTS ras2fim.max_geocurves;\"\n",
    "print(sf.execute_sql(sql))\n",
    "\n",
    "sql = \"\"\"\n",
    "CREATE TABLE ras2fim.max_geocurves as (\n",
    "    SELECT\n",
    "        feature_id,\n",
    "        max(discharge_cfs) as max_rc_discharge_cfs,\n",
    "        max(stage_ft) as max_rc_stage_ft,\n",
    "        max(discharge_cms) as max_rc_discharge_cms,\n",
    "        max(stage_m) as max_rc_stage_m\n",
    "    FROM ras2fim.geocurves\n",
    "    GROUP BY feature_id )\n",
    "\"\"\"\n",
    "print(sf.execute_sql(sql))\n",
    "\n",
    "sql = \"DROP INDEX IF EXISTS ras2fim.max_geocurves_feature_id_index;\"\n",
    "print(sf.execute_sql(sql))\n",
    "\n",
    "sql = \"\"\"\n",
    "CREATE INDEX IF NOT EXISTS max_geocurves_feature_id_index ON \n",
    "     ras2fim.max_geocurves USING btree (feature_id ASC NULLS LAST);\n",
    "\"\"\"\n",
    "print(sf.execute_sql(sql))\n",
    "\n",
    "print(\"max_geocurves table created and filled\")\n",
    "\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "end_dt = datetime.now()\n",
    "time_duration = end_dt - start_dt\n",
    "print(f\"... duration was  {str(time_duration).split('.')[0]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec1b8bf-e1dc-4525-9066-afef7273e289",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2>5 - Load the Ras2Fim boundaries into egis</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251905aa-259a-4ef4-9590-dcd31375c624",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Moves data from local shapefile to EGIS\n",
    "\n",
    "# Importing Modules\n",
    "import os\n",
    "import sys\n",
    "import helper_functions.shared_functions as sf\n",
    "import geopandas as gpd\n",
    "\n",
    "# sys path if needed\n",
    "sys.path.append(os.path.join(os.path.abspath(''), '..'))\n",
    "\n",
    "# Dir location of your data\n",
    "DATA_DPATH = r\"/home/ec2-user/SageMaker/Don - Campground/Don - Store\"\n",
    "# File location of your shapefile data\n",
    "DATASET_DPATH = f\"{DATA_DPATH}/main_huc8.shp\"\n",
    "# Check path by printing\n",
    "print(DATASET_DPATH)\n",
    "\n",
    "# Only use when you want to create something new\n",
    "gdf = gpd.read_file(DATASET_DPATH, columns='geometry')\n",
    "gdf.to_postgis(name=\"boundaries2\", con=sf.get_db_engine(db_type=\"viz\"),\n",
    "               schema=\"ras2fim\", if_exists=\"replace\")\n",
    "\n",
    "# add an oid field to your data\n",
    "sql = \"\"\"\n",
    "ALTER TABLE ras2fim.boundaries ADD COLUMN oid SERIAL PRIMARY KEY;\n",
    "\"\"\"\n",
    "sf.execute_sql(sql)\n",
    "\n",
    "# delete a table in egis if you made a mistake or such\n",
    "sql = \"\"\"\n",
    "DROP TABLE IF EXISTS reference.ras2fim_boundaries;\n",
    "\"\"\"\n",
    "sf.execute_sql(sql, db_type=\"egis\")\n",
    "\n",
    "# The function move_data_from_viz_to_egis is called because the data needs to \n",
    "# be in the egis before publishing to uat or prd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316b1321-f570-4f53-aeee-f8798330642f",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2>6 - Run AEP FIM Pipelines.</h2>\n",
    "Updated Documentation from Tyler Early 2024: This can be done in a couple of diferent ways.\n",
    "\n",
    "1) One option is to use the pipeline_input code created below by Corey to start the AEP pipelines directly from this notebook.<br>\n",
    "   However, those pipeline_input dictionaries may very well be be out of date, pending more recent updates to the pipelines.<br?\n",
    "\n",
    "\n",
    "2) The other option, which I prefer, is to setup a manual test event in the initialize_pipeline lambda function to trigger an AEP pipeline like this:</b>\n",
    "{\n",
    "  \"configuration\": \"reference\",\n",
    "  \"products_to_run\": \"static_nwm_aep_inundation_extent_library\",\n",
    "  \"invoke_step_function\": false\n",
    "}\n",
    "\n",
    "Using this test event will produce the pipeline instructions, printing any errors that come up, and you can simply change the invoke_step_function flag to True when you're ready to actually invoke a pipeline run (which you can monitor/manage in the step function gui). You will need to manually update the static_nwm_aep_inundation_extent_library.yml product config file to only run 1 aep configuration at a time, and work through the configs as the pipelines finish (takes about an hour each). I've also found that the fim_data_prep lambda function needs to be temporarilly increased to ~4,500mb of memory to run these pipelines. It's also worth noting that these are very resource intesive pipelines, as FIM is calculated for every reach in the nation. AWS costs can amount to hundreds or even thousands of dollars by running these pipelines, so use responsibly.\n",
    "\n",
    "A couple other important notes:\n",
    "- These AEP configurations write data directly to the aep_fim schema in the egis RDS database, instead of the viz database.\n",
    "- <b>You'll need to dump the aep_fim schema after that is complete for backup / deployment into other environments.</b>\n",
    "- This process has not been tested with new NWM 3.0 Recurrence Flows, and a good thorough audit / QC check of output data is warranted, given those changes and the recent updates to the pipelines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a698067",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Aug 6, 2024: Note: This was created after all intervals were created, so only HW was tested against\n",
    "\n",
    "def get_aep_pipeline_input(stage_interval):\n",
    "    pipeline_input = {\n",
    "      \"configuration\": \"reference\",\n",
    "      \"job_type\": \"auto\",\n",
    "      \"data_type\": \"channel\",\n",
    "      \"keep_raw\": False,\n",
    "      \"reference_time\": datetime.now().strftime('%Y-%m-%d 00:00:00'),\n",
    "      \"configuration_data_flow\": {\n",
    "        \"db_max_flows\": [],\n",
    "        \"db_ingest_groups\": [],\n",
    "        \"python_preprocessing\": []\n",
    "      },\n",
    "      \"pipeline_products\": [\n",
    "        {\n",
    "          \"product\": \"static_nwm_aep_inundation_extent_library\",\n",
    "          \"configuration\": \"reference\",\n",
    "          \"product_type\": \"fim\",\n",
    "          \"run\": True,\n",
    "          \"fim_configs\": [\n",
    "            {\n",
    "              \"name\": f\"rf_{stage_interval}_inundation\",\n",
    "              \"target_table\": f\"aep_fim.rf_{stage_interval}_inundation\",\n",
    "              \"fim_type\": \"hand\",\n",
    "              \"sql_file\": f\"rf_{stage_interval}_inundation\"\n",
    "            }\n",
    "          ],\n",
    "          \"services\": [\n",
    "            \"static_nwm_aep_inundation_extent_library_noaa\"\n",
    "          ],\n",
    "          \"raster_outputs\": {\n",
    "            \"output_bucket\": \"\",\n",
    "            \"output_raster_workspaces\": []\n",
    "          },\n",
    "          \"postprocess_sql\": [],\n",
    "          \"product_summaries\": [],\n",
    "          \"python_preprocesing_dependent\": False\n",
    "        }\n",
    "      ],\n",
    "      \"sql_rename_dict\": {},\n",
    "      \"logging_info\": {\n",
    "          \"Timestamp\": int(datetime.now().timestamp())\n",
    "      }\n",
    "    }\n",
    "\n",
    "    return pipeline_input\n",
    "\n",
    "print(\"function: get_aep_pipeline_input loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d6ee69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#### 2 Year Flow\n",
    "pipeline_input = get_aep_pipeline_input(\"2\")\n",
    "\n",
    "# notice, slightly different object name\n",
    "pipeline_name = f\"sagemaker_aep_2_{datetime.now().strftime('%Y%m%dT%H%M')}\"\n",
    "\n",
    "STEPFUNCTION_CLIENT.start_execution(\n",
    "    stateMachineArn = PIPELINE_ARN,\n",
    "    name = pipeline_name,\n",
    "    input= json.dumps(pipeline_input)\n",
    ")\n",
    "\n",
    "print(f\"AEP : 2 year flows ie: rf_2_inundation kicked off. Can take 45 mins. Pipeline : hv-vpp-ti-viz-pipeline - {pipeline_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f89d9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#### 5 Year Flow\n",
    "pipeline_input = get_aep_pipeline_input(\"5\")\n",
    "\n",
    "# notice, slightly different object name\n",
    "pipeline_name = f\"sagemaker_aep_5_{datetime.now().strftime('%Y%m%dT%H%M')}\"\n",
    "\n",
    "STEPFUNCTION_CLIENT.start_execution(\n",
    "    stateMachineArn = PIPELINE_ARN,\n",
    "    name = pipeline_name,\n",
    "    input= json.dumps(pipeline_input)\n",
    ")\n",
    "\n",
    "print(f\"AEP : 5 year flows ie: rf_5_inundation kicked off. Can take 45 mins. Pipeline : hv-vpp-ti-viz-pipeline  - {pipeline_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791d1a8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#### 10 Year Flow\n",
    "pipeline_input = get_aep_pipeline_input(\"10\")\n",
    "\n",
    "# notice, slightly different object name\n",
    "pipeline_name = f\"sagemaker_aep_10_{datetime.now().strftime('%Y%m%dT%H%M')}\"\n",
    "\n",
    "STEPFUNCTION_CLIENT.start_execution(\n",
    "    stateMachineArn = PIPELINE_ARN,\n",
    "    name = pipeline_name,\n",
    "    input= json.dumps(pipeline_input)\n",
    ")\n",
    "\n",
    "print(f\"AEP : 10 year flows ie: rf_10_inundation kicked off. Can take 45 mins. Pipeline : hv-vpp-ti-viz-pipeline  - {pipeline_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb87128",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#### 25 Year Flow\n",
    "pipeline_input = get_aep_pipeline_input(\"25\")\n",
    "\n",
    "# notice, slightly different object name\n",
    "pipeline_name = f\"sagemaker_aep_25_{datetime.now().strftime('%Y%m%dT%H%M')}\"\n",
    "\n",
    "STEPFUNCTION_CLIENT.start_execution(\n",
    "    stateMachineArn = PIPELINE_ARN,\n",
    "    name = pipeline_name,\n",
    "    input= json.dumps(pipeline_input)\n",
    ")\n",
    "\n",
    "print(f\"AEP : 25 year flows ie: rf_25_inundation kicked off. Can take 45 mins. Pipeline : hv-vpp-ti-viz-pipeline  - {pipeline_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4832e4e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#### 50 Year Flow\n",
    "pipeline_input = get_aep_pipeline_input(\"50\")\n",
    "\n",
    "# notice, slightly different object name\n",
    "pipeline_name = f\"sagemaker_aep_50_{datetime.now().strftime('%Y%m%dT%H%M')}\"\n",
    "\n",
    "STEPFUNCTION_CLIENT.start_execution(\n",
    "    stateMachineArn = PIPELINE_ARN,\n",
    "    name = pipeline_name,\n",
    "    input= json.dumps(pipeline_input)\n",
    ")\n",
    "\n",
    "print(f\"AEP : 50 year flows ie: rf_50_inundation kicked off. Can take 45 mins. Pipeline : hv-vpp-ti-viz-pipeline  - {pipeline_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187e83bc-ebbe-4615-a046-e0ef7b09ad3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#### HW (High Water) Flow\n",
    "pipeline_input = get_aep_pipeline_input(\"high_water\")\n",
    "\n",
    "# notice, slightly different object name\n",
    "pipeline_name = f\"sagemaker_aep_hw_{datetime.now().strftime('%Y%m%dT%H%M')}\"\n",
    "\n",
    "STEPFUNCTION_CLIENT.start_execution(\n",
    "     stateMachineArn = PIPELINE_ARN,\n",
    "     name = pipeline_name,\n",
    "     input= json.dumps(pipeline_input)\n",
    ")\n",
    "\n",
    "print(f\"AEP : High Water year flows ie: rf_hw_inundation kicked off. Can take 45 mins. Pipeline : hv-vpp-ti-viz-pipeline  - {pipeline_name}\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c810767-2f5d-46b2-860c-5d5c549f2e2a",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h3>IMPORTANT: Return hv-vpp-ti-viz-fim-data-prep Lambda memory to 2048mb</h3>\n",
    "\n",
    "https://us-east-1.console.aws.amazon.com/lambda/home?region=us-east-1#/functions/hv-vpp-ti-viz-fim-data-prep?tab=code\n",
    "\n",
    "Lambda name: hv-vpp-ti-viz-hand-fim-processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48074dbe-ead7-467f-ae68-16c746115efc",
   "metadata": {},
   "source": [
    "<h2>7 - RUN CATCHMENT WORKFLOWS 2 CONFIGS AT A TIME. CHECK FOR STEP FUNCTION FINISHING BEFORE STARTING NEW ONE</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b33a346-ee44-4844-9687-627ccd601263",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 7a - Branch 0 Catchments. Wait until it is done before kicking off the next GMS (Level Path) catchments load a bit lower. ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77d6e5b-f5fb-422f-a6c3-0f53743cd631",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Add backups to these (4.4.0.0)  (already not available for 4.4.0.0)\n",
    "\n",
    "\n",
    "# TODO: We likely need to keep the schema, so trun is fine for now, but eventually, get a lsit of the indexes and re-build \n",
    "# indexes each time as/if needed. Granted these tables are loaded via Lambdas, so I am not sure how indexes will play into that\n",
    "\n",
    "sf.execute_sql('''\n",
    "TRUNCATE \n",
    "    fim_catchments.branch_0_catchments, \n",
    "    fim_catchments.branch_0_catchments_hi, \n",
    "    fim_catchments.branch_0_catchments_prvi;\n",
    "''', db_type=\"egis\")\n",
    "\n",
    "print(\"Catchment Truncation for Branch 0 Done\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7495759",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline_input = {\n",
    "  \"configuration\": \"reference\",\n",
    "  \"job_type\": \"auto\",\n",
    "  \"data_type\": \"channel\",\n",
    "  \"keep_raw\": False,\n",
    "  \"reference_time\": datetime.now().strftime('%Y-%m-%d 00:00:00'),\n",
    "  \"configuration_data_flow\": {\n",
    "    \"db_max_flows\": [],\n",
    "    \"db_ingest_groups\": [],\n",
    "    \"python_preprocessing\": []\n",
    "  },\n",
    "  \"pipeline_products\": [\n",
    "    {\n",
    "      \"product\": \"static_hand_catchments_0_branches\",\n",
    "      \"configuration\": \"reference\",\n",
    "      \"product_type\": \"fim\",\n",
    "      \"run\": True,\n",
    "      \"fim_configs\": [\n",
    "        {\n",
    "          \"name\": \"catchments_0_branches\",\n",
    "          \"target_table\": \"fim_catchments.branch_0_catchments\",\n",
    "          \"fim_type\": \"hand\",\n",
    "          \"sql_file\": \"catchments_0_branches\"\n",
    "        }\n",
    "      ],\n",
    "      \"services\": [\n",
    "        \"static_hand_catchments_0_branches_noaa\"\n",
    "      ],\n",
    "      \"raster_outputs\": {\n",
    "        \"output_bucket\": \"\",\n",
    "        \"output_raster_workspaces\": []\n",
    "      },\n",
    "      \"postprocess_sql\": [],\n",
    "      \"product_summaries\": [],\n",
    "      \"python_preprocesing_dependent\": False\n",
    "    },\n",
    "    {\n",
    "      \"product\": \"static_hand_catchments_0_branches_hi\",\n",
    "      \"configuration\": \"reference\",\n",
    "      \"product_type\": \"fim\",\n",
    "      \"run\": True,\n",
    "      \"fim_configs\": [\n",
    "        {\n",
    "          \"name\": \"catchments_0_branches_hi\",\n",
    "          \"target_table\": \"fim_catchments.branch_0_catchments_hi\",\n",
    "          \"fim_type\": \"hand\",\n",
    "          \"sql_file\": \"catchments_0_branches_hi\"\n",
    "        }\n",
    "      ],\n",
    "      \"services\": [\n",
    "        \"static_hand_catchments_0_branches_hi_noaa\"\n",
    "      ],\n",
    "      \"raster_outputs\": {\n",
    "        \"output_bucket\": \"\",\n",
    "        \"output_raster_workspaces\": []\n",
    "      },\n",
    "      \"postprocess_sql\": [],\n",
    "      \"product_summaries\": [],\n",
    "      \"python_preprocesing_dependent\": False\n",
    "    },\n",
    "    {\n",
    "      \"product\": \"static_hand_catchments_0_branches_prvi\",\n",
    "      \"configuration\": \"reference\",\n",
    "      \"product_type\": \"fim\",\n",
    "      \"run\": True,\n",
    "      \"fim_configs\": [\n",
    "        {\n",
    "          \"name\": \"catchments_0_branches_prvi\",\n",
    "          \"target_table\": \"fim_catchments.branch_0_catchments_prvi\",\n",
    "          \"fim_type\": \"hand\",\n",
    "          \"sql_file\": \"catchments_0_branches_prvi\"\n",
    "        }\n",
    "      ],\n",
    "      \"services\": [\n",
    "        \"static_hand_catchments_0_branches_prvi_noaa\"\n",
    "      ],\n",
    "      \"raster_outputs\": {\n",
    "        \"output_bucket\": \"\",\n",
    "        \"output_raster_workspaces\": []\n",
    "      },\n",
    "      \"postprocess_sql\": [],\n",
    "      \"product_summaries\": [],\n",
    "      \"python_preprocesing_dependent\": False\n",
    "    }\n",
    "  ],\n",
    "  \"sql_rename_dict\": {},\n",
    "  \"logging_info\": {\n",
    "      \"Timestamp\": int(datetime.now().timestamp())\n",
    "  }\n",
    "}\n",
    "\n",
    "pipeline_name = f\"sagemaker_0_catchments_{datetime.now().strftime('%Y%m%dT%H%M')}\"\n",
    "\n",
    "# TODO: For later... fix fim_version value and add model_version column. current fim_version vlaue is showing 4.5.2.11\n",
    "\n",
    "STEPFUNCTION_CLIENT.start_execution(\n",
    "    stateMachineArn = PIPELINE_ARN,\n",
    "    name = pipeline_name,\n",
    "    input= json.dumps(pipeline_input)\n",
    ")\n",
    "\n",
    "print(\"Catchments Branch 0 load kicked off. Last runtime: 23:38.019. \"\n",
    "      f\"Pipeline : hv-vpp-ti-viz-pipeline  - {pipeline_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d792a88e-51fd-4e28-9360-b90abd99ddaf",
   "metadata": {},
   "source": [
    "### 7b - GMS (Level Paths / non branch 0) catchments ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea2a5be-9936-4abe-832a-8582f3c3af64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Add backups to these\n",
    "\n",
    "sf.execute_sql('''\n",
    "TRUNCATE\n",
    "    fim_catchments.branch_gms_catchments,\n",
    "    fim_catchments.branch_gms_catchments_hi,\n",
    "    fim_catchments.branch_gms_catchments_prvi;\n",
    "''', db_type=\"egis\")\n",
    "\n",
    "print(\"Catchment Truncation for GMS (Level Path) Branchs Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae8917f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline_input = {\n",
    "  \"configuration\": \"reference\",\n",
    "  \"job_type\": \"auto\",\n",
    "  \"data_type\": \"channel\",\n",
    "  \"keep_raw\": False,\n",
    "  \"reference_time\": datetime.now().strftime('%Y-%m-%d 00:00:00'),\n",
    "  \"configuration_data_flow\": {\n",
    "    \"db_max_flows\": [],\n",
    "    \"db_ingest_groups\": [],\n",
    "    \"python_preprocessing\": []\n",
    "  },\n",
    "  \"pipeline_products\": [\n",
    "    {\n",
    "      \"product\": \"static_hand_catchments_gms_branches\",\n",
    "      \"configuration\": \"reference\",\n",
    "      \"product_type\": \"fim\",\n",
    "      \"run\": True,\n",
    "      \"fim_configs\": [\n",
    "        {\n",
    "          \"name\": \"catchments_gms_branches\",\n",
    "          \"target_table\": \"fim_catchments.branch_gms_catchments\",\n",
    "          \"fim_type\": \"hand\",\n",
    "          \"sql_file\": \"catchments_gms_branches\"\n",
    "        }\n",
    "      ],\n",
    "      \"services\": [\n",
    "        \"static_hand_catchments_gms_branches_noaa\"\n",
    "      ],\n",
    "      \"raster_outputs\": {\n",
    "        \"output_bucket\": \"\",\n",
    "        \"output_raster_workspaces\": []\n",
    "      },\n",
    "      \"postprocess_sql\": [],\n",
    "      \"product_summaries\": [],\n",
    "      \"python_preprocesing_dependent\": False\n",
    "    },\n",
    "    {\n",
    "      \"product\": \"static_hand_catchments_gms_branches_hi\",\n",
    "      \"configuration\": \"reference\",\n",
    "      \"product_type\": \"fim\",\n",
    "      \"run\": True,\n",
    "      \"fim_configs\": [\n",
    "        {\n",
    "          \"name\": \"catchments_gms_branches_hi\",\n",
    "          \"target_table\": \"fim_catchments.branch_gms_catchments_hi\",\n",
    "          \"fim_type\": \"hand\",\n",
    "          \"sql_file\": \"catchments_gms_branches_hi\"\n",
    "        }\n",
    "      ],\n",
    "      \"services\": [\n",
    "        \"static_hand_catchments_gms_branches_hi_noaa\"\n",
    "      ],\n",
    "      \"raster_outputs\": {\n",
    "        \"output_bucket\": \"\",\n",
    "        \"output_raster_workspaces\": []\n",
    "      },\n",
    "      \"postprocess_sql\": [],\n",
    "      \"product_summaries\": [],\n",
    "      \"python_preprocesing_dependent\": False\n",
    "    },\n",
    "    {\n",
    "      \"product\": \"static_hand_catchments_gms_branches_prvi\",\n",
    "      \"configuration\": \"reference\",\n",
    "      \"product_type\": \"fim\",\n",
    "      \"run\": True,\n",
    "      \"fim_configs\": [\n",
    "        {\n",
    "          \"name\": \"catchments_gms_branches_prvi\",\n",
    "          \"target_table\": \"fim_catchments.branch_gms_catchments_prvi\",\n",
    "          \"fim_type\": \"hand\",\n",
    "          \"sql_file\": \"catchments_gms_branches_prvi\"\n",
    "        }\n",
    "      ],\n",
    "      \"services\": [\n",
    "        \"static_hand_catchments_gms_branches_prvi_noaa\"\n",
    "      ],\n",
    "      \"raster_outputs\": {\n",
    "        \"output_bucket\": \"\",\n",
    "        \"output_raster_workspaces\": []\n",
    "      },\n",
    "      \"postprocess_sql\": [],\n",
    "      \"product_summaries\": [],\n",
    "      \"python_preprocesing_dependent\": False\n",
    "    }\n",
    "  ],\n",
    "  \"sql_rename_dict\": {},\n",
    "  \"logging_info\": {\n",
    "      \"Timestamp\": int(datetime.now().timestamp())\n",
    "  }\n",
    "}\n",
    "\n",
    "pipeline_name = f\"sagemaker_gms_catchments_{datetime.now().strftime('%Y%m%dT%H%M')}\"\n",
    "\n",
    "# TODO: For later... fix fim_version value and add model_version column. current fim_version vlaue is showing 4.5.2.11\n",
    "\n",
    "STEPFUNCTION_CLIENT.start_execution(\n",
    "    stateMachineArn = PIPELINE_ARN,\n",
    "    name = pipeline_name,\n",
    "    input= json.dumps(pipeline_input)\n",
    ")\n",
    "\n",
    "print(\"Catchments GMS Branches (Level Paths / non branch 0) load kicked off.\"\n",
    "      f\" Last runtime: 24:45.150. Pipeline : hv-vpp-ti-viz-pipeline  - {pipeline_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff4dc6b-c41b-412c-9d9f-d812c5c72115",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2>8 - Recreate derived.usgs_elev_table</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4eb95e-e147-4789-a76b-d8bda33bc3cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Already run for 4.4.0.0 (4.5.2.11)\n",
    "\n",
    "# TODO: Aug 2024: Change this to a backup without indexes and not rename\n",
    "\n",
    "# sf.execute_sql(f'ALTER TABLE IF EXISTS derived.usgs_elev_table RENAME TO usgs_elev_table_fim_{OLD_FIM_TAG};')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd0c144-5a20-4899-b946-a6dd9ae49b53",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# NOTE: Aug 6, 2024 - There is already a 4.4.0.0 backup. How did it get made?\n",
    "# Has appx 2,150 HUCs to process, but this section goes quickly.\n",
    "\n",
    "sf.execute_sql('DROP TABLE IF EXISTS derived.usgs_elev_table;')\n",
    "\n",
    "uet_usecols = ['location_id', 'HydroID', 'dem_adj_elevation', 'nws_lid', 'levpa_id']\n",
    "\n",
    "paginator = S3_CLIENT.get_paginator('list_objects')\n",
    "operation_parameters = {'Bucket': FIM_BUCKET,\n",
    "                        'Prefix': f'{HAND_DATASETS_DPATH}/',\n",
    "                        'Delimiter': '/'}\n",
    "page_iterator = paginator.paginate(**operation_parameters)\n",
    "page_count = 0\n",
    "for page in page_iterator:\n",
    "    page_count += 1\n",
    "    prefix_objects = page['CommonPrefixes']\n",
    "    for i, prefix_obj in enumerate(prefix_objects):\n",
    "        print(f\"Processing {i+1} of {len(prefix_objects)} on page {page_count}\")\n",
    "        huc_prefix = prefix_obj.get(\"Prefix\")\n",
    "        usgs_elev_table_key = f'{huc_prefix}usgs_elev_table.csv'\n",
    "        try:\n",
    "            uet = S3_CLIENT.get_object(\n",
    "                Bucket=FIM_BUCKET, \n",
    "                Key=usgs_elev_table_key\n",
    "            )['Body']\n",
    "            uet_df = pd.read_csv(uet, header=0, usecols=uet_usecols)\n",
    "            uet_df['fim_version'] = PUBLIC_FIM_VERSION\n",
    "            uet_df[COLUMN_NAME_MODEL_VERSION] = FIM_MODEL_VERSION\n",
    "            uet_df.to_sql(\n",
    "                con=VIZ_DB_ENGINE,\n",
    "                dtype={\n",
    "                    \"location_id\": Text(),\n",
    "                    \"nws_data_huc\": Text()\n",
    "                },\n",
    "                schema='derived',\n",
    "                name='usgs_elev_table',\n",
    "                index=False, \n",
    "                if_exists='append'\n",
    "            )\n",
    "        except Exception as e:\n",
    "            if \"NoSuchKey\" in str(e):\n",
    "                pass\n",
    "            else:\n",
    "                raise e\n",
    "                \n",
    "print(\"usgs_elev_tables load completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a02c35-16a9-4cd8-9fd6-61d20b3e6feb",
   "metadata": {},
   "source": [
    "<h2>9 - Recreate derived.hydrotable_staggered</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a2a7c2-7499-4ebc-a878-769e62dfbd6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Already run for 4.4.0.0  (4.5.2.11)\n",
    "\n",
    "# TODO: Aug 2024: Change this to a backup without indexes and not rename\n",
    "\n",
    "# sf.execute_sql(f'ALTER TABLE IF EXISTS derived.hydrotable_staggered RENAME TO hydrotable_staggered_{OLD_FIM_TAG};')\n",
    "#print(\"derived.hydrotable_staggered renamed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33026eb6-fbc7-47f7-a44b-f9cea8f0911b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Takes appx 5.75 to 6 hrs to run\n",
    "\n",
    "print(\"hydrotable reloaded - started\")\n",
    "start_dt = datetime.now()\n",
    "\n",
    "sf.execute_sql('DROP TABLE IF EXISTS derived.hydrotable;')\n",
    "sql = '''\n",
    "SELECT distinct LPAD(huc8::text, 8, '0') as huc8 FROM derived.featureid_huc_crosswalk WHERE huc8 is not null;\n",
    "'''\n",
    "df = sf.sql_to_dataframe(sql)\n",
    "ht_usecols = ['HydroID', 'feature_id', 'stage', 'discharge_cms']\n",
    "\n",
    "paginator = S3_CLIENT.get_paginator('list_objects')\n",
    "operation_parameters = {'Bucket': FIM_BUCKET,\n",
    "                        'Prefix': f'{HAND_DATASETS_DPATH}/',\n",
    "                        'Delimiter': '/'}\n",
    "page_iterator = paginator.paginate(**operation_parameters)\n",
    "page_count = 0\n",
    "for page in page_iterator:\n",
    "    page_count += 1\n",
    "    prefix_objects = page['CommonPrefixes']\n",
    "    for i, prefix_obj in enumerate(prefix_objects):\n",
    "        print(f\"Processing {i+1} of {len(prefix_objects)} on page {page_count}\")\n",
    "        branch_prefix = f'{prefix_obj.get(\"Prefix\")}branches/0/'\n",
    "        branch_files_result = S3_CLIENT.list_objects(\n",
    "            Bucket=FIM_BUCKET, \n",
    "            Prefix=branch_prefix, \n",
    "            Delimiter='/'\n",
    "        )\n",
    "        hydro_table_key = None\n",
    "        for content_obj in branch_files_result.get('Contents'):\n",
    "            branch_file_prefix = content_obj['Key']\n",
    "            if 'hydroTable' in branch_file_prefix:\n",
    "                hydro_table_key = branch_file_prefix\n",
    "\n",
    "        if hydro_table_key:\n",
    "            # print(f\"Found usgs_elev_table and hydroTable in {branch_prefix}\")\n",
    "            try:\n",
    "                # print(\"...Fetching csvs...\")\n",
    "                ht = S3_CLIENT.get_object(\n",
    "                    Bucket=FIM_BUCKET,\n",
    "                    Key=hydro_table_key\n",
    "                )['Body']\n",
    "                # print(\"...Reading with pandas...\")\n",
    "                ht_df = pd.read_csv(ht, header=0, usecols=ht_usecols)\n",
    "                # print('...Writing to db...')\n",
    "                ht_df['fim_version'] = PUBLIC_FIM_VERSION\n",
    "                ht_df[COLUMN_NAME_MODEL_VERSION] = FIM_MODEL_VERSION\n",
    "                ht_df.to_sql(\n",
    "                    con=VIZ_DB_ENGINE, \n",
    "                    schema='derived',\n",
    "                    name='hydrotable',\n",
    "                    index=False,\n",
    "                    if_exists='append'\n",
    "                )\n",
    "            except Exception as e:\n",
    "                raise e\n",
    "                print(f'Fetch failed: {e}')\n",
    "                \n",
    "end_dt = datetime.now()\n",
    "time_duration = end_dt - start_dt\n",
    "print(\"hydrotable reload done\")\n",
    "print(f\"... duration was  {str(time_duration).split('.')[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc764e4-7e67-4b51-a100-6021e31416cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"hydrotable_staggered started\")\n",
    "\n",
    "start_dt = datetime.now()\n",
    "\n",
    "sql = '''\n",
    "DROP TABLE IF EXISTS derived.hydrotable_staggered;\n",
    "SELECT\n",
    "    et.location_id,\n",
    "    ht.feature_id,\n",
    "    (stage + et.dem_adj_elevation) * 3.28084 as elevation_ft,\n",
    "    LEAD((stage + et.dem_adj_elevation) * 3.28084) OVER (PARTITION BY ht.feature_id ORDER BY ht.feature_id, stage) as next_elevation_ft,\n",
    "    discharge_cms * 35.3147 as discharge_cfs,\n",
    "    LEAD(discharge_cms * 35.3147) OVER (PARTITION BY ht.feature_id ORDER BY ht.feature_id, stage) as next_discharge_cfs\n",
    "INTO derived.hydrotable_staggered\n",
    "FROM derived.hydrotable AS ht\n",
    "JOIN derived.usgs_elev_table AS et ON ht.\"HydroID\" = et.\"HydroID\" AND et.location_id IS NOT NULL;\n",
    "'''\n",
    "sf.execute_sql(sql)\n",
    "\n",
    "print(\"hydrotable_staggered reload done\")\n",
    "end_dt = datetime.now()\n",
    "time_duration = end_dt - start_dt\n",
    "print(f\"... duration was  {str(time_duration).split('.')[0]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f21c03-ca36-402c-a69b-396162720f66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# we don't need the hydrotable anymore as it has been reloaded and adjusted above in hydrotable_staggered\n",
    "sf.execute_sql('DROP TABLE IF EXISTS derived.hydrotable;')\n",
    "print(\"Done dropping derived.hydrotable, post hydrotable_staggered load\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0237f374-5829-4050-b503-958c9d4c3703",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2>10 - Recreate derived.usgs_rating_curves_staggered</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461f20b4-8442-4c0e-9114-ea62764cd789",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Aug 16, 2024 - done for 4.4.0.0 (4.5.2.11)\n",
    "\n",
    "# TODO: Aug 2024: Change this to a backup without indexes and not rename\n",
    "# Aug 27, 2024: This needs to be redone so we don't rename tables, it messes up indexes and index names when we use _to_sql commands later\n",
    "\n",
    "# sf.execute_sql(f'ALTER TABLE IF EXISTS derived.usgs_rating_curves RENAME TO usgs_rating_curves_{OLD_FIM_TAG};')\n",
    "# sf.execute_sql(f'ALTER TABLE IF EXISTS derived.usgs_rating_curves_staggered RENAME TO usgs_rating_curves_staggered_{OLD_FIM_TAG};')\n",
    "# print(\"usgs rating curve tables renamed and cleaned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b752fce5-c8d7-4064-92d7-842c22d1723e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "sql = '''\n",
    "    DROP TABLE IF EXISTS derived.usgs_rating_curves;\n",
    "    DROP TABLE IF EXISTS derived.usgs_rating_curves_staggered;\n",
    "'''\n",
    "sf.execute_sql(sql)\n",
    "\n",
    "print(\"Done dropping usgs_rating_curves and usgs_rating_curves_staggered\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a0cc18-de82-4bd8-bd4d-c3ad021a1dec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run the script to load the usgs_rating_curve.csv. Exact duration not yet known. Appx 30 min (??)\n",
    "\n",
    "start_dt = datetime.now()\n",
    "event = {\n",
    "    'target_table': 'derived.usgs_rating_curves',\n",
    "    'target_cols': ['location_id', 'flow', 'stage', 'navd88_datum', 'elevation_navd88'],\n",
    "    'file': f'{QA_DATASETS_DPATH}/usgs_rating_curves.csv',\n",
    "    'bucket': FIM_BUCKET,\n",
    "    'reference_time': '2023-08-23 00:00:00',\n",
    "    'keep_flows_at_or_above': 0,\n",
    "    'iteration_index': 0\n",
    "}\n",
    "\n",
    "sf.execute_db_ingest(event, None)\n",
    "\n",
    "print(\"done loading usgs_rating_curves\")\n",
    "end_dt = datetime.now()\n",
    "time_duration = end_dt - start_dt\n",
    "print(f\"... duration was  {str(time_duration).split('.')[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4548d5e8-303e-44e0-a8a5-7b1947214b4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Takes under a minute\n",
    "print(\"Starting usgs_rating_curves_staggered build based on usgs_rating_curve table\")\n",
    "\n",
    "sql = '''\n",
    "SELECT \n",
    "    location_id,\n",
    "    flow as discharge_cfs, \n",
    "    LEAD(flow) OVER (PARTITION BY location_id ORDER BY location_id, stage) as next_discharge_cfs,\n",
    "    stage,\n",
    "    navd88_datum,\n",
    "    elevation_navd88 as elevation_ft,\n",
    "    LEAD(elevation_navd88) OVER (PARTITION BY location_id ORDER BY location_id, stage) as next_elevation_ft\n",
    "INTO derived.usgs_rating_curves_staggered\n",
    "FROM derived.usgs_rating_curves;\n",
    "'''\n",
    "\n",
    "sf.execute_sql(sql)\n",
    "\n",
    "print(\"Done loading usgs_rating_curves_staggered\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a77875-f569-4410-888c-b132714a7c9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# usgs_rating_curves is a temp table and is loaded with some changes into the usgs_rating_curves_staggered\n",
    "sf.execute_sql('DROP TABLE IF EXISTS derived.usgs_rating_curves;')\n",
    "print(\"Done dropping derived.usgs_rating_curves, post loading usgs_rating_curves_staggered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee640076-be6f-47f5-b091-f60192502bd0",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2>11 - UPDATE SRC SKILL METRICS IN DB</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c472f8-0557-4864-8536-1814f3ac4286",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Already run for 4.4.0.0 (4.5.2.11)\n",
    "\n",
    "'''\n",
    "Be Very Careful to just rename tables. If they have indexes, the index will now point to the new\n",
    "table names but maintain the original index name. Those index names can really mess stuff up.\n",
    "Best to never rename unless you rename indexes as well. This particular on is ok. \n",
    "Note: When various '\"to_sql\" tools are run which have GIST indexes, this index column name issue\n",
    "will be the problem.\n",
    "\n",
    "Why Drop instead of Truncate? if the schema changes for the incoming, truncate will have column\n",
    "missmatches.\n",
    "\n",
    "We really should be backing up indexes and constraints as well.\n",
    "\n",
    "'''\n",
    "\n",
    "# TODO: Aug 2024: Change this away from \"rename\" to copy / drop. \n",
    "# sf.execute_sql(f'ALTER TABLE IF EXISTS derived.src_skill_temp RENAME TO src_skill_temp_{OLD_FIM_TAG};')\n",
    "# sf.execute_sql(f'ALTER TABLE IF EXISTS derived.src_skill RENAME TO src_skill_{OLD_FIM_TAG};')\n",
    "\n",
    "# print(\"src_skill and src_skill_temps db renamed\")\n",
    "\n",
    "\n",
    "# TODO: Rob Aug 2024: change this to backup of table and not rename as it messses with indexes\n",
    "# Don't need a copy of the reference src_skill table , so just drop it.\n",
    "new_table_name = f\"derived.src_skill_temp_{OLD_FIM_TAG}\"\n",
    "sql = f'''\n",
    "   CREATE TABLE IF NOT EXISTS {new_table_name} AS TABLE derived.src_skill_temp;\n",
    "'''\n",
    "\n",
    "\n",
    "#print(\"src_skill and src_skill_temps db renamed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a249854-b9cd-4ccb-afea-96c35708515d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prep the dbs for the new load\n",
    "#sf.execute_sql('DROP TABLE IF EXISTS derived.src_skill_temp;')\n",
    "#sf.execute_sql('DROP TABLE IF EXISTS reference.src_skill;', db_type='egis')\n",
    "#print(\"Done dropping src_skill and src_skill_temp tables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24f509a-90df-492a-a0d4-aa9eab8e7041",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the src_skill_temp table\n",
    "start_dt = datetime.now()\n",
    "\n",
    "event = {\n",
    "    'target_table': 'derived.src_skill_temp',\n",
    "    'target_cols': None,  # This means \"all\"\n",
    "    'file': f'{QA_DATASETS_DPATH}/agg_nwm_recurr_flow_elev_stats_location_id.csv',\n",
    "    'bucket': FIM_BUCKET,\n",
    "    'reference_time': '2023-08-23 00:00:00',\n",
    "    'keep_flows_at_or_above': 0,\n",
    "    'iteration_index': 0,\n",
    "    'db_type': 'viz'\n",
    "}\n",
    "\n",
    "execute_db_ingest(event, None)\n",
    "print(\"Done loading derived.src_skill_temp table\")\n",
    "end_dt = datetime.now()\n",
    "time_duration = end_dt - start_dt\n",
    "print(f\"... duration was  {str(time_duration).split('.')[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5639447-9a5a-4add-9c36-ebd945a5a8d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load into src_skill table adding geometry to it from external.usgs_gage. Yes.. more/less straight from WRDS tables\n",
    "# Some recs appear to be in error in the csv. location id = 394220106431500 (those are dropped below)\n",
    "\n",
    "start_dt = datetime.now()\n",
    "\n",
    "sf.execute_sql('DROP TABLE IF EXISTS derived.src_skill;')\n",
    "\n",
    "sql = f\"\"\"\n",
    "SELECT\n",
    "\t(row_number() OVER ())::int as oid,\n",
    "\tgage.name,\n",
    "\tLPAD(skill.location_id::text, 8, '0') as location_id,\n",
    "\tskill.nrmse,\n",
    "\tskill.mean_abs_y_diff_ft,\n",
    "\tskill.mean_y_diff_ft,\n",
    "\tskill.percent_bias,\n",
    "    '{PUBLIC_FIM_VERSION}' as {COLUMN_NAME_FIM_VERSION},\n",
    "    '{FIM_MODEL_VERSION}' as {COLUMN_NAME_MODEL_VERSION},\n",
    "\tgage.geo_point as geom\n",
    "INTO derived.src_skill\n",
    "FROM derived.src_skill_temp skill\n",
    "JOIN external.usgs_gage AS gage ON LPAD(gage.usgs_gage_id::text, 8, '0') = LPAD(skill.location_id::text, 8, '0')\n",
    "\"\"\"\n",
    "\n",
    "sf.execute_sql(sql)\n",
    "\n",
    "print(\"Done loading derived.src_skill table\")\n",
    "end_dt = datetime.now()\n",
    "time_duration = end_dt - start_dt\n",
    "print(f\"... duration was  {str(time_duration).split('.')[0]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa32cd0-6edc-4441-9be3-f5bcba499ad0",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h4>Then export the derived.src_skill table and import it into the EGIS reference.src_skill table</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df23345-47a4-490a-b10f-2020e3a2a68e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "sf.move_data_from_viz_to_egis(\"derived.src_skill\", \"reference.src_skill\")\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf47b262-d5aa-41e3-a12e-99432f52fbfc",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2>12 - UPDATE FIM PERFORMANCE METRICS IN DB</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b291a461-e36f-4049-84b6-dd59cd1c4ad2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Make copies of current dbs for 4.4.0.0 (4.5.2.11)\n",
    "# DONE: for 4.4.0.0 (4.5.2.11)\n",
    "\n",
    "# NOTE: Aug 2024: The problem with not droppign them and rebuilding them with indexes, is that if the table schema\n",
    "# changes it is not reflected\n",
    "\n",
    "\n",
    "# Points\n",
    "new_table_name = f\"reference.fim_performance_points_{OLD_FIM_TAG}\"\n",
    "sql = f'''\n",
    "    CREATE TABLE IF NOT EXISTS {new_table_name} AS TABLE reference.fim_performance_points;\n",
    "'''\n",
    "sf.execute_sql(sql, db_type='egis')\n",
    "print(f\"fim_performance_points copied to {new_table_name} if it does not already exists\")\n",
    "\n",
    "\n",
    "# Catchments\n",
    "new_table_name = f\"reference.fim_performance_catchments_{OLD_FIM_TAG}\"\n",
    "sql = f'''\n",
    "   CREATE TABLE IF NOT EXISTS {new_table_name} AS TABLE reference.fim_performance_catchments;\n",
    "'''\n",
    "sf.execute_sql(sql, db_type='egis')\n",
    "print(f\"fim_performance_catchments copied to {new_table_name} if it does not already exists\")\n",
    "\n",
    "\n",
    "# Polys\n",
    "new_table_name = f\"reference.fim_performance_polys_{OLD_FIM_TAG}\"\n",
    "sql = f'''\n",
    "   CREATE TABLE IF NOT EXISTS {new_table_name} AS TABLE reference.fim_performance_polys;\n",
    "'''\n",
    "sf.execute_sql(sql, db_type='egis')\n",
    "print(f\"fim_performance_polys copied to {new_table_name} if it does not already exists\")\n",
    "\n",
    "print(\"Done making backups of the FIM performance tables\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fcdfab-3936-4a0a-8ab9-605e9a64f9bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# clean up tables for new load\n",
    "\n",
    "# TODO: Aug 2024: Add postgresql if / else. Truncate \"if exists\" doesn't exist. :)\n",
    "\n",
    "table_names = [\n",
    "    \"reference.fim_performance_points\",\n",
    "    \"reference.fim_performance_polys\",\n",
    "    \"reference.fim_performance_catchments\"\n",
    "]\n",
    "\n",
    "for tb_name in table_names:\n",
    "    sql = f\"TRUNCATE TABLE {tb_name}\"\n",
    "#    print(sql)\n",
    "    sf.execute_sql(sql,db_type='egis')\n",
    "\n",
    "\n",
    "print(f\"All fim_performance tables trunated if they exist\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ca5581",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the new fim performance tables\n",
    "\n",
    "start_dt = datetime.now()\n",
    "\n",
    "# os.environ['EGIS_DB_HOST'] =''  #TI DB\n",
    "\n",
    "db_type = \"egis\"\n",
    "db_engine = sf.get_db_engine(db_type)\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Define bucket and parent directories.\n",
    "bucket = \"hydrovis-ti-deployment-us-east-1\"\n",
    "\n",
    "# file_handles = ['fim_performance_points.csv']\n",
    "# file_handles = ['fim_performance_points.csv', 'fim_performance_polys.csv', 'fim_performance_catchments_dissolved.csv']\n",
    "# file_handles = ['fim_performance_points.csv', 'fim_performance_polys.csv']\n",
    "file_handles = ['fim_performance_catchments.csv']\n",
    "\n",
    "for file_handle in file_handles:\n",
    "\n",
    "    print(\"Reading file...\")\n",
    "    # df = pd.read_csv(local_download_path)\n",
    "    file_to_download = f\"{QA_DATASETS_DPATH}/{file_handle}\"\n",
    "    df = s3_sf.download_S3_csv_files_to_df_from_list(FIM_BUCKET, [file_to_download], True)\n",
    "    print(\"File read.\")\n",
    "\n",
    "    # Rename headers.\n",
    "\n",
    "    if file_handle == 'fim_performance_points.csv':\n",
    "        df = df.rename(columns={'Unnamed: 0': 'oid', 'geometry': 'geom'})\n",
    "    else:\n",
    "        df = df.rename(columns={'Unnamed: 0': 'oid', 'geometry': 'geom', 'huc':'huc8'})\n",
    "\n",
    "    print(df.dtypes)\n",
    "    # Convert all field names to lowercase (needed for ArcGIS Pro).\n",
    "    df.columns = df.columns.str.lower()\n",
    "\n",
    "    # Enforce data types on df before loading in DB (TODO: need to create special cases for each layer).\n",
    "    if file_handle == 'fim_performance_points.csv':\n",
    "        df = df.astype({'huc': 'str'})\n",
    "    else:\n",
    "        df = df.astype({'huc8': 'str'})\n",
    "    df = df.fillna(0)\n",
    "    try:\n",
    "        df = df.astype({'feature_id': 'int'})\n",
    "        df = df.astype({'feature_id': 'str'})\n",
    "        df = df.astype({'oid': 'int'})\n",
    "    except KeyError:  # If there is no feature_id field\n",
    "        pass\n",
    "    try:\n",
    "        df = df.astype({'nwm_seg': 'int'})\n",
    "        df = df.astype({'nwm_seg': 'str'})\n",
    "    except KeyError:  # If there is no nwm_seg field\n",
    "        pass\n",
    "    try:\n",
    "        df = df.astype({'usgs_gage': 'int'})\n",
    "        df = df.astype({'usgs_gage': 'str'})\n",
    "    except KeyError:  # If there is no usgs_gage field\n",
    "        pass\n",
    "\n",
    "    # zfill HUC8 field.\n",
    "    if file_handle == 'fim_performance_points.csv':\n",
    "        df['huc'] = df['huc'].apply(lambda x: x.zfill(8))\n",
    "    else:\n",
    "        df['huc8'] = df['huc8'].apply(lambda x: x.zfill(8))\n",
    "\n",
    "    df['version'] = PUBLIC_FIM_VERSION\n",
    "    df[COLUMN_NAME_MODEL_VERSION] = FIM_MODEL_VERSION\n",
    "\n",
    "    # Upload df to database.\n",
    "    stripped_layer_name = file_handle.replace(\".csv\", \"\")\n",
    "    table_name = \"reference.\" + stripped_layer_name\n",
    "    print(\"Loading data into DB...\")\n",
    "\n",
    "    # Chunk load data into DB\n",
    "\n",
    "    if file_handle in ['fim_performance_catchments.csv']:\n",
    "\n",
    "        print(\"Chunk loading...\")\n",
    "        # Create list of df chunks\n",
    "        n = 10000  # chunk row size\n",
    "        list_df = [df[i:i+n] for i in range(0,df.shape[0],n)]\n",
    "        # geometry = 'MULTIPOLYGON'\n",
    "        # Load the first chunk into the DB as a new table\n",
    "        first_chunk_df = list_df[0]\n",
    "        print(first_chunk_df.shape[0])\n",
    "\n",
    "        first_chunk_df.to_sql(\n",
    "            name=stripped_layer_name, \n",
    "            con=db_engine, \n",
    "            schema='reference',\n",
    "            if_exists='replace', \n",
    "            index=False,\n",
    "            dtype={'oid': sqlalchemy.types.Integer(),\n",
    "                   'version': sqlalchemy.types.String(),\n",
    "                   'geom': Geometry('MULTIPOLYGON', srid=3857)\n",
    "                  }\n",
    "        )\n",
    "        # Load remaining chunks into newly created table\n",
    "\n",
    "        for remaining_chunk_df in list_df[1:]:\n",
    "            print(remaining_chunk_df.shape[0])\n",
    "            remaining_chunk_df.to_sql(\n",
    "                name=stripped_layer_name,\n",
    "                con=db_engine,\n",
    "                schema='reference',\n",
    "                if_exists='append',\n",
    "                index=False,\n",
    "                dtype={'oid': sqlalchemy.types.Integer(),\n",
    "                       'version': sqlalchemy.types.String(),\n",
    "                       'geom': Geometry('MULTIPOLYGON', srid=3857)\n",
    "                      }\n",
    "            )\n",
    "    else:\n",
    "        if 'points' in stripped_layer_name: geometry = 'POINT'\n",
    "        if 'polys' in stripped_layer_name: geometry = 'POLYGON'\n",
    "        # print(\"GEOMETRY\")\n",
    "        # print(geometry)\n",
    "        df.to_sql(\n",
    "            name=stripped_layer_name,\n",
    "            con=db_engine,\n",
    "            schema='reference',\n",
    "            if_exists='replace',\n",
    "            index=False,\n",
    "            dtype={'oid': sqlalchemy.types.Integer(),\n",
    "                   'version': sqlalchemy.types.String(),\n",
    "                   'geom': Geometry(geometry, srid=3857)\n",
    "                  }\n",
    "        )\n",
    "\n",
    "    print(f\">>> {file_handle} downloaded and loaded\")\n",
    "\n",
    "    # deleted the downloaded file that was just processed.\n",
    "    # if os.path.exists(local_download_path):\n",
    "\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "end_dt = datetime.now()\n",
    "time_duration = end_dt - start_dt\n",
    "# print(\"All FIM Performance files loaded\")\n",
    "print(f\"... duration was  {str(time_duration).split('.')[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52c1563-3c91-4810-96d0-48568b60cbef",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2>13 - CatFIM (Stage-Based and Flow-Based)</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7938fcb2-313c-4cf8-8274-d34d80bdf7bf",
   "metadata": {},
   "source": [
    "<h4>Function to load CatFIM Data (Non Public)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f37c81-d105-4c38-aa49-b4aef40a7543",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "''' Function to load CatFIM data (for any flow / stage / library / sites but non public)'''\n",
    "\n",
    "\n",
    "def load_catfim_table(catfim_type):\n",
    "\n",
    "    '''\n",
    "    Inputs:\n",
    "        - catfim_type: name identififer for the set, such as \"flow_based_catfim\" or \"flow_based_catfim_sites\", etc\n",
    "              Sometimes the file_handle name can be the name of the s3 file (without extension) and/or the table\n",
    "              name.\n",
    "              Options: flow_based_catfim, flow_based_catfim_sites, stage_based_catfim, stage_based_catfim_sites\n",
    "    '''\n",
    "\n",
    "    db_type = \"egis\"\n",
    "    db_engine = sf.get_db_engine(db_type)\n",
    "    src_crs = \"3857\"\n",
    "\n",
    "    # --------------------------------------\n",
    "    # Drop the original Db if already in place\n",
    "    table_name = catfim_type  # yes, dup variable for now\n",
    "\n",
    "    sf.execute_sql(f\"DROP TABLE IF EXISTS reference.{table_name};\", db_type=db_type)\n",
    "    print(f\"Dropping reference.{table_name} table if it existed\")\n",
    "    print(\"\")\n",
    "\n",
    "    # --------------------------------------\n",
    "    # Get the data from S3 and load it into a df\n",
    "    if catfim_type in ['flow_based_catfim', 'stage_based_catfim']:\n",
    "        file_to_download = f\"{QA_DATASETS_DPATH}/{catfim_type}_library.csv\"\n",
    "    else:\n",
    "        file_to_download = f\"{QA_DATASETS_DPATH}/{catfim_type}.csv\"\n",
    "\n",
    "    # print(f\"Downloading {file_to_download} ... \")\n",
    "\n",
    "    df = s3_sf.download_S3_csv_files_to_df_from_list(FIM_BUCKET, [file_to_download], True)\n",
    "    num_recs = len(df)\n",
    "    print(f\"File read. {num_recs} records to load\")\n",
    "\n",
    "    # --------------------------------------\n",
    "    # Adjusting Columns and data\n",
    "    # Rename headers. All files this name\n",
    "    df = df.rename(columns={'Unnamed: 0': 'oid',\n",
    "                            'geometry': 'geom',\n",
    "                            'huc': 'huc8'})\n",
    "\n",
    "    # 4.5.2.11, fixing a column name bug\n",
    "    if catfim_type == 'stage_based_catfim_sites':\n",
    "        df = df.rename(columns={'nws_lid': 'ahps_lid'})\n",
    "\n",
    "    # Convert all field names to lowercase (needed for ArcGIS Pro).\n",
    "    df.columns = df.columns.str.lower()\n",
    "\n",
    "    # Remove sites that are in derived.ahps_restricted_sites\n",
    "    # TODO: Aug 2024: Need to see if this list needs to be updated. Submitted card.\n",
    "    restricted_sites_df = sf.get_db_values(\"derived.ahps_restricted_sites\", [\"*\"])\n",
    "    restricted_dict = restricted_sites_df.to_dict('records')\n",
    "\n",
    "    for site in restricted_dict:\n",
    "        nws_lid = site['nws_lid'].lower()\n",
    "        #print(nws_lid)\n",
    "        if \"sites\" in catfim_type:\n",
    "            # print(True)\n",
    "            # print(nws_lid)\n",
    "            df.loc[df.ahps_lid == nws_lid, 'mapped'] = 'no'\n",
    "            df.loc[df.ahps_lid == nws_lid, 'status'] = site['restricted_reason']\n",
    "            # print(df.loc[df.ahps_lid==nws_lid]['status'])\n",
    "        else:\n",
    "            df.loc[df.ahps_lid == nws_lid, 'viz'] = 'no'\n",
    "            df = df[df['viz'] == 'yes']\n",
    "\n",
    "    # TODO: Aug 2024: This may be a bug or very outdated. It was in the code to load stage for 4.4.0.0\n",
    "    # and I left it here for 4.5.2.11, but made a card with the FIM team to review and fix it in there code\n",
    "    # so we can drop this.\n",
    "    if 'stage_based' in catfim_type:\n",
    "        for sea_level_site in ['qutg1', 'augg1', 'baxg1', 'lamf1', 'adlg1', 'hrag1', 'stng1']:\n",
    "            if \"sites\" in catfim_type:\n",
    "                df.loc[df.ahps_lid==sea_level_site, 'mapped'] = 'no'\n",
    "                df.loc[df.ahps_lid==sea_level_site, 'status'] = 'Stage thresholds seem to be based on sea level and not channel thalweg'\n",
    "            else:\n",
    "                df.loc[df.ahps_lid==sea_level_site, 'viz'] = 'no'\n",
    "                df = df[df['viz']=='yes']  # Subset df to only sites desired for mapping\n",
    "    # end if\n",
    "\n",
    "    # Enforce data types on df before loading in DB (TODO: need to create special cases for each layer).\n",
    "    df = df.astype({'huc8': 'str'})\n",
    "    df = df.fillna(0)\n",
    "    try:\n",
    "        df = df.astype({'feature_id': 'int'})\n",
    "        df = df.astype({'feature_id': 'str'})\n",
    "    except KeyError:  # If there is no feature_id field\n",
    "        pass\n",
    "    try:\n",
    "        df = df.astype({'nwm_seg': 'int'})\n",
    "        df = df.astype({'nwm_seg': 'str'})\n",
    "    except KeyError:  # If there is no nwm_seg field\n",
    "        pass\n",
    "    try:\n",
    "        df = df.astype({'usgs_gage': 'int'})\n",
    "        df = df.astype({'usgs_gage': 'str'})\n",
    "    except KeyError:  # If there is no usgs_gage field\n",
    "        pass\n",
    "\n",
    "    # zfill HUC8 field.\n",
    "    df['huc8'] = df['huc8'].apply(lambda x: x.zfill(8))\n",
    "\n",
    "    if '_sites' in catfim_type:\n",
    "        df = df.astype({'nws_data_rfc_forecast_point': 'str'})\n",
    "        df = df.astype({'nws_data_rfc_defined_fcst_point': 'str'})\n",
    "        df = df.astype({'nws_data_riverpoint': 'str'})\n",
    "\n",
    "    # TODO: Aug 27, 2024: For now, let's jsut override the \"version\" column and fix it when we\n",
    "    # reconsile the fim_version and model_version columns\n",
    "    df['version'] = PUBLIC_FIM_VERSION\n",
    "    df[COLUMN_NAME_FIM_VERSION] = PUBLIC_FIM_VERSION\n",
    "    df[COLUMN_NAME_MODEL_VERSION] = FIM_MODEL_VERSION\n",
    "\n",
    "    # --------------------------------------\n",
    "    # Load to DB\n",
    "    # Chunk load data into DB\n",
    "    if catfim_type in ['flow_based_catfim', 'stage_based_catfim']:\n",
    "\n",
    "        # Create list of df chunks\n",
    "        n = 1000  # chunk row size\n",
    "        print(f\"Chunk loading... into {table_name} -- {n} records at a time\")\n",
    "        print(\"\")\n",
    "        chunk_df = [df[i:i+n] for i in range(0, df.shape[0], n)]\n",
    "\n",
    "        # Load the first chunk into the DB as a new table\n",
    "        first_chunk_df = chunk_df[0]\n",
    "        num_chunks = len(chunk_df)\n",
    "\n",
    "        print(f\" ... loading chunk 1 of {num_chunks}\")\n",
    "\n",
    "        first_chunk_df.to_sql(\n",
    "            name=table_name,\n",
    "            con=db_engine,\n",
    "            schema='reference',\n",
    "            if_exists='replace',\n",
    "            index=False,\n",
    "            dtype={'oid': sqlalchemy.types.Integer(),\n",
    "                   'geom': Geometry('MULTIPOLYGON', srid=src_crs)}\n",
    "        )\n",
    "\n",
    "        # Load remaining chunks into newly created table\n",
    "        ctr = 1  # Already loaded one\n",
    "        for remaining_chunk in chunk_df[1:]:\n",
    "            # print(remaining_chunk.shape[0])\n",
    "            ctr += 1\n",
    "            print(f\" ... loading chunk {ctr} of {num_chunks}\")\n",
    "            remaining_chunk.to_sql(\n",
    "                        name=table_name,\n",
    "                        con=db_engine,\n",
    "                        schema='reference',\n",
    "                        if_exists='append',\n",
    "                        index=False,\n",
    "                        dtype={'oid': sqlalchemy.types.Integer(),\n",
    "                               'geom': Geometry('MULTIPOLYGON', srid=src_crs)\n",
    "                              }\n",
    "                    )\n",
    "        # end for\n",
    "    else:  # sites tables\n",
    "        print(f\"Loading data into {table_name} ...\")\n",
    "\n",
    "        df.to_sql(\n",
    "            name=table_name,\n",
    "            con=db_engine,\n",
    "            schema='reference',\n",
    "            if_exists='replace',\n",
    "            index=False,\n",
    "            dtype={'oid': sqlalchemy.types.Integer(),\n",
    "                   'geom': Geometry('POINT', srid=src_crs)}\n",
    "        )\n",
    "\n",
    "    # This should auto create a gist index against the geometry column\n",
    "    # if that index name already exists, the upload will fail, the index can not pre-exist\n",
    "    # Best to drop the table before loading.\n",
    "\n",
    "    # return\n",
    "\n",
    "print(\"load_catfim_table function loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b43cdc-8591-47f4-8db7-1058af5863f5",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h3>13.a - Backup old DBs and prepare new databases (but not the \"public\" FIM 10/30 db's)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9577edb-4aa6-423b-819e-df8c922c7ec2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This covers both Stage Based and Flow Based (but not the \"public\" catfim db's)\n",
    "\n",
    "# The \"Public\" db backups ana loads are in cells lower (12.d and higher)\n",
    "\n",
    "# DONE for 4.4.0.0.  (4.5.2.11)\n",
    "\n",
    "# # print(\"Starting Data Backups and table drops for stage and flow based catfim\")\n",
    "# db_names = [\"stage_based_catfim\", \"stage_based_catfim_sites\",\n",
    "#             \"flow_based_catfim\", \"flow_based_catfim_sites\"]\n",
    "\n",
    "# for db_name in db_names:\n",
    "#     new_table_name = f\"reference.{db_name}_{OLD_FIM_TAG}\"\n",
    "#     sql = f'''\n",
    "#         CREATE TABLE IF NOT EXISTS {new_table_name} AS TABLE reference.{db_name};\n",
    "#     '''\n",
    "#     sf.execute_sql(sql, db_type='egis')\n",
    "#     print(f\"{db_name} copied to {new_table_name} if it does not already exist\")\n",
    "\n",
    "\n",
    "# Aug 2024: Now we can drop the tables as we don't have any indexes on them at this time other than the gist geom index.\n",
    "# By dropping them, we can auto adjust the tables schema. (don't truncate)\n",
    "\n",
    "# for db_name in db_names:\n",
    "#     sf.execute_sql(f\"DROP TABLE IF EXISTS reference.{db_name};\", db_type='egis')\n",
    "#     print(f\"reference.{db_name} table dropped if it existed\")\n",
    "\n",
    "\n",
    "# print(\"Data Backups of flow based catfim are complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4377dc64-963e-447e-a553-1a59c7cb1781",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h3>13.b - Updated Flow and Stage Based CatFIM Data (Non Public)</h3>\n",
    "\n",
    "<h3>AUG 2024: IMPORTANT NOTE:</h3>\n",
    "The stage based catfim (library) csv has grown to appx 10 GiB. Our current notebook, hv-vpp-ti-viz-notebook only has 15 GiB memory.\n",
    "Running tool can easily overwhelm the notebook server and freeze it up forcing a reboot.\n",
    "Sometimes when the notebook instance comes back up, it no longer has ths swap system in place. You will need most of the memory\n",
    "and some swap to load it.  Keep an eye a \"terminal\" windows and keep entering `free -h` to keep an eye on it's usage.\n",
    "</br>\n",
    "We will need to review to see if we want to:\n",
    "\n",
    "1. Upgrade this notebook server with more memory (and harddrive space would be good)\n",
    "\n",
    "2. Change the load of the catfim library (non sites) data to another system. Maybe we can load it via a lambda to an EC2 or something?\n",
    "\n",
    "3. Get the FIM Team to break it to smaller pieces, but watch carefully for the OID system (unique id for all records)\n",
    "\n",
    "**When you are done running this script, Please restart this kernal as it does not appear to be releasing all memory. (memory leak?)**\n",
    "\n",
    "\n",
    "Also looks like Tyler has some notebooks where he was moving this into a lambda load? We need to look into that\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c55cd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"Starting of CatFIM data\")\n",
    "\n",
    "# catfim_types = ['flow_based_catfim', 'flow_based_catfim_sites']\n",
    "# catfim_types =  ['stage_based_catfim', 'stage_based_catfim_sites']\n",
    "catfim_types = ['stage_based_catfim_sites']\n",
    "# catfim_types = ['stage_based_catfim']\n",
    "\n",
    "start_dt = datetime.now()\n",
    "\n",
    "for catfim_type in catfim_types:\n",
    "    print(f\"Loading {catfim_type} data\")\n",
    "    load_catfim_table(catfim_type)\n",
    "\n",
    "print(\"\")\n",
    "end_dt = datetime.now()\n",
    "time_duration = end_dt - start_dt\n",
    "print(f\"... duration was  {str(time_duration).split('.')[0]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3eef2c-8764-4c0a-9e4d-07d9bb6d4dfe",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h3>13.c - CatFIM Backup old \"public\" FIM 10 / 30 DBs and prepare new databases</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cf108d-7360-48a9-a4a2-81b23b9e51b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This covers ONLY Catfim public FIM 10/30 for both flow based and stage based\n",
    "'''\n",
    "\n",
    "''' DONE for 4.4.0.0.  (4.5.2.11)'''\n",
    "\n",
    "# db_name_appendix = f\"{OLD_FIM_TAG}_fim_10\"\n",
    "\n",
    "# print(\"Starting Data Backups and table drops for stage and flow based PUBLIC catfim\")\n",
    "# # db_names = [\"stage_based_catfim_public\", \"stage_based_catfim_sites_public\",\n",
    "# #              \"flow_based_catfim_public\", \"flow_based_catfim_sites_public\"]\n",
    "\n",
    "# # stage_based_catfim_sites_public didn't exist for fim 10 but should have in TI (does in other enviros likely)\n",
    "# db_names = [\"stage_based_catfim_public\", \n",
    "#              \"flow_based_catfim_public\", \"flow_based_catfim_sites_public\"]\n",
    "\n",
    "# for db_name in db_names:\n",
    "#     new_table_name = f\"reference.{db_name}_{db_name_appendix}\"\n",
    "#     sql = f\"CREATE TABLE IF NOT EXISTS {new_table_name} AS TABLE reference.{db_name}\"\n",
    "#     sf.execute_sql(sql, db_type='egis')\n",
    "#     print(f\"{db_name} copied to {new_table_name} if it does not already exist\")\n",
    "\n",
    "    \n",
    "# # Aug 2024: Now we can drop the tables as we don't have any indexes on them at this time other than the gist geom index.\n",
    "# # By dropping them, we can auto adjust the tables schema. (don't truncate)\n",
    "\n",
    "# for db_name in db_names:\n",
    "#     sf.execute_sql(f\"DROP TABLE IF EXISTS reference.{db_name};\", db_type='egis')\n",
    "#     print(f\"reference.{db_name} table dropped if it existed\")\n",
    "\n",
    "# print(\"Data Backups of flow based catfim are complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e451ed4-148f-4ccd-83bf-dff900915efb",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h3>13.d - Load CatFIM \"public\" FIM 30 DBs</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd38c00d-22ad-476e-ab05-cf293e5bbc15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"Loading CatFIM Public datasets (FIM 30)\")\n",
    "\n",
    "catfim_types = [\"stage_based_catfim\", \"stage_based_catfim_sites\",\n",
    "                \"flow_based_catfim\", \"flow_based_catfim_sites\"]\n",
    "\n",
    "__public_fim_release = \"fim_30\"  # The new fim public release being loaded (ie. fim_10, fim_30, fim_60..)\n",
    "\n",
    "start_dt = datetime.now()\n",
    "\n",
    "for catfim_type in catfim_types:\n",
    "    print(\"\")\n",
    "    sql = f'''\n",
    "    DROP TABLE IF EXISTS reference.{catfim_type}_public;\n",
    "\n",
    "    SELECT\n",
    "        catfim.*,\n",
    "        '{__public_fim_release}' as public_fim_release\n",
    "    INTO reference.{catfim_type}_public\n",
    "    FROM reference.{catfim_type} as catfim\n",
    "    JOIN reference.public_fim_domain as fim_domain ON ST_Intersects(catfim.geom, fim_domain.geom)\n",
    "    '''\n",
    "    print(sf.execute_sql(sql, db_type='egis'))\n",
    "    print(f\"public {__public_fim_release} data load for {catfim_type} is complete\")\n",
    "\n",
    "# what about indexes again?\n",
    "\n",
    "# for db_name in db_names:\n",
    "#     new_table_name = f\"reference.{db_name}_{db_name_appendix}\"\n",
    "#     sql = f\"CREATE TABLE IF NOT EXISTS {new_table_name} AS TABLE reference.{db_name}\"\n",
    "#     sf.execute_sql(sql, db_type='egis')\n",
    "#     print(f\"{db_name} copied to {new_table_name} if it does not already exist\")\n",
    "\n",
    "print(\"\")\n",
    "end_dt = datetime.now()\n",
    "time_duration = end_dt - start_dt\n",
    "print(f\"... duration was  {str(time_duration).split('.')[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65578391-f29e-4de4-8d98-ab48a0375603",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2>14 - Clear the HAND Cache</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d1ab20-4148-4d1d-bbb6-a2c3983ba145",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "TRUNCATE TABLE fim_cache.hand_hydrotable_cached;\n",
    "TRUNCATE TABLE fim_cache.hand_hydrotable_cached_max;\n",
    "TRUNCATE TABLE fim_cache.hand_hydrotable_cached_geo;\n",
    "TRUNCATE TABLE fim_cache.hand_hydrotable_cached_zero_stage;\n",
    "\"\"\"\n",
    "sf.execute_sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af909c1-d6a6-4ce7-85b5-080d0a556c90",
   "metadata": {},
   "source": [
    "<h2>15 - SAVE TO REPO (AND REDEPLOY TO TI WITH NEW VERSION VARIABLE IN TERRAFORM ??)</h2>\n",
    "\n",
    "TODO: Aug 2024: Come up with system to save changes to this script and related scripts\n",
    "Note from Rob: While, un-elegant, there so much quick evolution here that I recommend we even keep seperate named load scripts in GIT\n",
    "ie) one for FIM Version 4.4.0.0 and one for 4.5.2.11, etc. So many changes for each edition and very fast script changes WIP may \n",
    "make it smarter to keep each script seperately (ie. 4.4.0.0, 4.5.2.11, etc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
