{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68fce701-8ba2-400d-b662-dca6439ef9b0",
   "metadata": {},
   "source": [
    "### Notes - Oct 21, 2024 ###\n",
    "This is a copy from 10.FIM Version 4.5.2.11 which included hand data loads plus ras2fim data. \n",
    "We will remove all ras2fim stuff here knowing that sometimes ras2fim will be uploaded on its own.\n",
    "However.. when ras2fim is loaded, some steps here will need to be re-run. Those steps will be\n",
    "duplicated when we build our next ras2fim load. This hand release does not have a ras2fim update so\n",
    "we will keep the one in place.\n",
    "</br></br>\n",
    "All code in here will be reviewed and adjusted as the loads progress. Consider each step to be\n",
    "WIP until you see a load date below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e1ee2b5-b109-49e8-8255-f06449b44ee6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (1.0.1)\n",
      "All loaded\n"
     ]
    }
   ],
   "source": [
    "# Cell to manually pip reload a packages that the Jupyter engine not retained\n",
    "# !pip install numpy\n",
    "# !pip install geopandas\n",
    "# !pip install pyarrow\n",
    "# !pip install xarray\n",
    "# !pip install geoalchemy2\n",
    "# !pip install contextily\n",
    "# !pip install rioxarray\n",
    "\n",
    "!pip install python-dotenv\n",
    "print(\"All loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103a0d12-b2a0-4586-ba0f-0ed6b2c4eb89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pd.set_option(\"max_info_rows\", 100000) # override  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b656259",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imports loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import codecs\n",
    "import csv\n",
    "\n",
    "import sys\n",
    "\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import boto3\n",
    "import geopandas as gpd\n",
    "import json\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "import sqlalchemy\n",
    "import xarray as xr\n",
    "\n",
    "from geopandas import GeoDataFrame\n",
    "from io import StringIO\n",
    "from geoalchemy2 import Geometry\n",
    "from shapely import wkt\n",
    "from shapely.geometry import Polygon\n",
    "from sqlalchemy.exc import DataError   # yes, reduntant, fix it later\n",
    "from sqlalchemy.types import Text    # yes, reduntant, fix it later\n",
    "\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "import helper_functions.shared_functions as sf\n",
    "import helper_functions.s3_shared_functions as s3_sf\n",
    "\n",
    "from helper_functions.viz_classes import database\n",
    "\n",
    "print(\"imports loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fae5ca0f-e582-4ff2-89b6-087285433a97",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aws_keys are at /home/ec2-user/SageMaker/AWS_keys.env\n",
      "aws_keys loaded\n"
     ]
    }
   ],
   "source": [
    "# Load AWS Keys\n",
    "from dotenv import load_dotenv\n",
    "aws_keys_path = os.path.join(Path.home(),\"SageMaker\", \"AWS_keys.env\")\n",
    "print(f\"aws_keys are at {aws_keys_path}\")\n",
    "load_dotenv(aws_keys_path)\n",
    "\n",
    "TI_ACCESS_KEY = os.environ['WF_TI_ACCESS_KEY']\n",
    "TI_SECRET_KEY = os.environ['WF_TI_SECRET_KEY']\n",
    "TI_TOKEN = os.environ['WF_TI_TOKEN']\n",
    "\n",
    "# I updated the file but it is not being honored in the enviro values\n",
    "\n",
    "# print(TI_ACCESS_KEY)\n",
    "# print(TI_SECRET_KEY)\n",
    "# print(TI_TOKEN)\n",
    "\n",
    "print(\"aws_keys loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e944eff3-6023-48f4-9f57-27304a240447",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Variables loaded\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# we won't load this into any tables at this time\n",
    "# The phrase of FIM 5.1.0 will be embedded in config files\n",
    "#PU0LIC_FIM_VERSION = \"FIM 5.1.0\"\n",
    "HAND_MODEL_VERSION = \"4.5.11.1\"\n",
    "\n",
    "HAND_ROOT_DPATH = \"fim/hand_4_5_11_1\"\n",
    "HAND_DATASETS_DPATH = f\"{HAND_ROOT_DPATH}/hand_datasets\"\n",
    "QA_DATASETS_DPATH = f\"{HAND_ROOT_DPATH}/qa_datasets\"\n",
    "\n",
    "FIM_BUCKET = \"hydrovis-ti-deployment-us-east-1\"\n",
    "FIM_CROSSWALK_FPATH = os.path.join(HAND_DATASETS_DPATH, \"crosswalk_table.csv\")\n",
    "PIPELINE_ARN = 'arn:aws:states:us-east-1:526904826677:stateMachine:hv-vpp-ti-viz-pipeline'\n",
    "\n",
    "COLUMN_NAME_MODEL_VERSION = \"model_version\"\n",
    "\n",
    "# Sometimes these credential values get updated. To find the latest correct values, go to your AWS Console log page and click on the \"Access Key\"\n",
    "# link to get the latest valid set. Using the \"AWS environment variables\" values.\n",
    "# If this is not set correctly, you will get an HTTP error 400 when you call S3 lower.\n",
    "# You might also see an error of 'An error occurred (NoSuchKey) when calling the GetObject operation:\n",
    "# The specified key does not exist.\" the creds are not correct\"\n",
    "\n",
    "S3_CLIENT = boto3.client(\"s3\")\n",
    "STEPFUNCTION_CLIENT = boto3.client('stepfunctions')\n",
    "VIZ_DB_ENGINE = sf.get_db_engine('viz')\n",
    "\n",
    "print(\"Global Variables loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5827eb-3ca3-4b29-8ee7-f7c4f3c2c736",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2>1 - UPLOAD FIM4 HYDRO ID/FEATURE ID CROSSWALK</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c96a49f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting column name from fim/hand_4_5_11_1/hand_datasets/crosswalk_table.csv\n",
      "(hand_id integer,feature_id bigint,huc8 TEXT,branch_id bigint,hydro_id integer,lake_id integer)\n",
      "***> Established db connection to: hv-vpp-ti-viz-processing.c4vzypepnkx3.us-east-1.rds.amazonaws.com from <module>()\n",
      "Deleting/Creating derived.fim4_featureid_crosswalk using columns (hand_id integer,feature_id bigint,huc8 TEXT,branch_id bigint,hydro_id integer,lake_id integer)\n",
      "Importing fim/hand_4_5_11_1/hand_datasets/crosswalk_table.csv to derived.fim4_featureid_crosswalk\n",
      "Adding model_version column to derived.fim4_featureid_crosswalk\n",
      "Adding feature id index to derived.fim4_featureid_crosswalk\n",
      "Adding hydro id index to derived.fim4_featureid_crosswalk\n",
      "\n",
      "... Estimated time to completion is just a few mins\n",
      "Successully loaded derived.fim4_featureid_crosswalk and updated it\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Getting column name from {FIM_CROSSWALK_FPATH}\")\n",
    "\n",
    "data = S3_CLIENT.get_object(Bucket=FIM_BUCKET, Key=FIM_CROSSWALK_FPATH)\n",
    "d_reader = csv.DictReader(codecs.getreader(\"utf-8\")(data[\"Body\"]))\n",
    "headers = d_reader.fieldnames\n",
    "\n",
    "\n",
    "header_str = \"(\"\n",
    "for header in headers:\n",
    "    header_str += header\n",
    "    if header in ['hand_id', 'hydro_id', 'lake_id']:\n",
    "        header_str += ' integer,'\n",
    "    elif header in ['branch_id', 'feature_id']:\n",
    "        header_str += ' bigint,'\n",
    "    else:\n",
    "        header_str += ' TEXT,'\n",
    "header_str = header_str[:-1] + \")\"\n",
    "print(header_str)\n",
    "\n",
    "db = database(db_type=\"viz\")\n",
    "with db.get_db_connection() as conn, conn.cursor() as cur:\n",
    "    \n",
    "    print(f\"Deleting/Creating derived.fim4_featureid_crosswalk using columns {header_str}\")\n",
    "    sql = f\"DROP TABLE IF EXISTS derived.fim4_featureid_crosswalk; CREATE TABLE derived.fim4_featureid_crosswalk {header_str};\"\n",
    "    cur.execute(sql)\n",
    "    conn.commit()\n",
    "    \n",
    "    # TODO: Nov: Drop the other 2 tables? No. ignore featureid_huc_crosswalk and featureid_huc_crosswalk_ak (not ours)\n",
    "    \n",
    "\n",
    "    print(f\"Importing {FIM_CROSSWALK_FPATH} to derived.fim4_featureid_crosswalk\")\n",
    "    sql = f\"\"\"\n",
    "        SELECT aws_s3.table_import_from_s3(\n",
    "           'derived.fim4_featureid_crosswalk',\n",
    "           '', \n",
    "           '(format csv, HEADER true)',\n",
    "           (SELECT aws_commons.create_s3_uri(\n",
    "               '{FIM_BUCKET}',\n",
    "               '{FIM_CROSSWALK_FPATH}',\n",
    "               'us-east-1'\n",
    "                ) AS s3_uri\n",
    "            ),\n",
    "            aws_commons.create_aws_credentials('{TI_ACCESS_KEY}', '{TI_SECRET_KEY}', '{TI_TOKEN}')\n",
    "           );\n",
    "        \"\"\"\n",
    "    cur.execute(sql)\n",
    "    conn.commit()\n",
    "\n",
    "    \n",
    "    print(f\"Adding {COLUMN_NAME_MODEL_VERSION} column to derived.fim4_featureid_crosswalk\")\n",
    "    sql = f\"ALTER TABLE derived.fim4_featureid_crosswalk ADD COLUMN IF NOT EXISTS {COLUMN_NAME_MODEL_VERSION} text DEFAULT '{HAND_MODEL_VERSION}';\"\n",
    "    cur.execute(sql)\n",
    "    conn.commit()\n",
    "    \n",
    "    print(\"Adding feature id index to derived.fim4_featureid_crosswalk\")\n",
    "    # Drop it already exists\n",
    "    sql = \"DROP INDEX IF EXISTS derived.fim4_crosswalk_feature_id\"\n",
    "    cur.execute(sql)\n",
    "    conn.commit()    \n",
    "    sql = \"CREATE INDEX fim4_crosswalk_feature_id ON derived.fim4_featureid_crosswalk USING btree (feature_id)\"\n",
    "    cur.execute(sql)\n",
    "    conn.commit()\n",
    "\n",
    "    print(\"Adding hydro id index to derived.fim4_featureid_crosswalk\")\n",
    "    # Drop it already exists\n",
    "    sql = \"DROP INDEX IF EXISTS derived.fim4_crosswalk_hydro_id\"\n",
    "    cur.execute(sql)\n",
    "    conn.commit()    \n",
    "    sql = \"CREATE INDEX fim4_crosswalk_hydro_id ON derived.fim4_featureid_crosswalk USING btree (hydro_id)\"\n",
    "    cur.execute(sql)\n",
    "    conn.commit()\n",
    "\n",
    "print(\"\")\n",
    "print(\"... Estimated time to completion is just a few mins\")\n",
    "print(\"Successully loaded derived.fim4_featureid_crosswalk and updated it\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c7d50b-0885-4bec-8094-7efe3d762da4",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2>2 - UPDATE FIM HAND PROCESSING LAMBDA ENV VARIABLE WITH NEW FIM PREFIX</h2>\n",
    "\n",
    "https://us-east-1.console.aws.amazon.com/lambda/home?region=us-east-1#/functions/hv-vpp-ti-viz-hand-fim-processing?tab=configure\n",
    "\n",
    "Lambda name: hv-vpp-ti-viz-hand-fim-processing\n",
    "\n",
    "In the Configuration Tab, click on the `Environment variables` (left menu)\n",
    "- change the `FIM_VERSION` to latest publie version being used. (numerics only): ie: 5.1.0\n",
    "- change the `HAND_VERSION` to latest HAND model version being used. (numerics only): ie: 4.5.11.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d71e761-9421-49fc-aaed-aafb33b7de0a",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2>3 - UPDATE FIM DATA PREP LAMBDA ENV VARIABLE WITH NEW FIM VERSION AND MEMORY</h2>\n",
    "\n",
    "https://us-east-1.console.aws.amazon.com/lambda/home?region=us-east-1#/functions/hv-vpp-ti-viz-fim-data-prep?tab=code\n",
    "\n",
    "Lambda name: hv-vpp-ti-viz-hand-fim-processing\n",
    "\n",
    "In the `Configuration` Tab, click on the `Environment variables` (left menu):\n",
    "- change the `FIM_VERSION` to the latest fim model version. \n",
    "<br>\n",
    "ie) 4.5.11.1\n",
    "<br><br>\n",
    "<b>Then:</b> Still in the Configuration Tab, now click on the `General Configuration` (left menu), followed \n",
    "by the `edit` button on the far right side, to get into the `General Configuration` page details.\n",
    "<br>Change (if they are not already there)\n",
    "<br>Memory (text field) to 4096 (MB)  and\n",
    "<br>Emphermeral Storage tp 1024 (MB)\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052f15eb-c883-4c27-9dd0-eb9c77ae7594",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2>4 - UPDATE RAS2FIM DATA (inc ras2fim boundaries) IN DB</h2>\n",
    "\n",
    "As of Oct 2024, we have a new fim (hand) release covered in this file, but ras2fim does not have a new\n",
    "release. ras2fim will likely be loaded as new datasets become available. \n",
    "\n",
    "***The code for ras2fim is removed here from the 4.5.2.11 set and will rebuilt as it''s own new separate load script when that happens.***\n",
    "\n",
    "However, we will have a few modifications for ras2fim data (not a reload) to help bring in the new\n",
    "fim_version and model_version format of just \"2.0\" columns. Those changes are included here.\n",
    "\n",
    "The Fim Version field needs to be deleted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc0a44c-8b46-4db5-a9a0-c65e0bf1e20b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Update \"geocurves\" to update the \"fim_version\" field to \"FIM 5.1.0:\n",
    "\n",
    "print(\"Updating geocurves table to model_version of 2.0 to follow the new versioning system and drop fim_version\")\n",
    "\n",
    "sf.execute_sql(\"UPDATE ras2fim.geocurves SET model_version = '2.0'\", db_type=\"viz\")\n",
    "\n",
    "# Drop the \"fim_version\" field.\n",
    "sf.execute_sql(\"ALTER TABLE ras2fim.geocurves DROP COLUMN fim_version\", db_type=\"viz\")\n",
    "\n",
    "print(\"Updating done for geocurves\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6d68b8-ecf2-406c-aa5d-43fe53707862",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2>5 - Run AEP FIM Pipelines.</h2>\n",
    "Updated Documentation from Tyler Early 2024: This can be done in a couple of diferent ways.\n",
    "\n",
    "1) One option is to use the pipeline_input code created below by Corey to start the AEP pipelines directly from this notebook.<br>\n",
    "   However, those pipeline_input dictionaries may very well be be out of date, pending more recent updates to the pipelines.<br?\n",
    "\n",
    "\n",
    "2) The other option, which I prefer, is to setup a manual test event in the initialize_pipeline lambda function to trigger an AEP pipeline like this:</b>\n",
    "{\n",
    "  \"configuration\": \"reference\",\n",
    "  \"products_to_run\": \"static_nwm_aep_inundation_extent_library\",\n",
    "  \"invoke_step_function\": false\n",
    "}\n",
    "\n",
    "Using this test event will produce the pipeline instructions, printing any errors that come up, and you can simply change the invoke_step_function flag to True when you're ready to actually invoke a pipeline run (which you can monitor/manage in the step function gui). You will need to manually update the static_nwm_aep_inundation_extent_library.yml product config file to only run 1 aep configuration at a time, and work through the configs as the pipelines finish (takes about an hour each). I've also found that the fim_data_prep lambda function needs to be temporarilly increased to ~4,500mb of memory to run these pipelines. It's also worth noting that these are very resource intesive pipelines, as FIM is calculated for every reach in the nation. AWS costs can amount to hundreds or even thousands of dollars by running these pipelines, so use responsibly.\n",
    "\n",
    "A couple other important notes:\n",
    "- These AEP configurations write data directly to the aep_fim schema in the egis RDS database, instead of the viz database.\n",
    "- <b>You'll need to dump the aep_fim schema after that is complete for backup / deployment into other environments.</b>\n",
    "- This process has not been tested with new NWM 3.0 Recurrence Flows, and a good thorough audit / QC check of output data is warranted, given those changes and the recent updates to the pipelines.\n",
    "\n",
    "***Note: You can start each of these 6 one right after the other. Maybe someday we can create a block that just starts all 6 at once.***\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a698067",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function: get_aep_pipeline_input loaded\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Aug 6, 2024: Note: This was created after all intervals were created, so only HW was tested against\n",
    "\n",
    "def get_aep_pipeline_input(stage_interval):\n",
    "    pipeline_input = {\n",
    "      \"configuration\": \"reference\",\n",
    "      \"job_type\": \"auto\",\n",
    "      \"data_type\": \"channel\",\n",
    "      \"keep_raw\": False,\n",
    "      \"reference_time\": datetime.now().strftime('%Y-%m-%d 00:00:00'),\n",
    "      \"configuration_data_flow\": {\n",
    "        \"db_max_flows\": [],\n",
    "        \"db_ingest_groups\": [],\n",
    "        \"python_preprocessing\": []\n",
    "      },\n",
    "      \"pipeline_products\": [\n",
    "        {\n",
    "          \"product\": \"static_nwm_aep_inundation_extent_library\",\n",
    "          \"configuration\": \"reference\",\n",
    "          \"product_type\": \"fim\",\n",
    "          \"run\": True,\n",
    "          \"fim_configs\": [\n",
    "            {\n",
    "              \"name\": f\"rf_{stage_interval}_inundation\",\n",
    "              \"target_table\": f\"aep_fim.rf_{stage_interval}_inundation\",\n",
    "              \"fim_type\": \"hand\",\n",
    "              \"sql_file\": f\"rf_{stage_interval}_inundation\"\n",
    "            }\n",
    "          ],\n",
    "          \"services\": [\n",
    "            \"static_nwm_aep_inundation_extent_library_noaa\"\n",
    "          ],\n",
    "          \"raster_outputs\": {\n",
    "            \"output_bucket\": \"\",\n",
    "            \"output_raster_workspaces\": []\n",
    "          },\n",
    "          \"postprocess_sql\": [],\n",
    "          \"product_summaries\": [],\n",
    "          \"python_preprocesing_dependent\": False\n",
    "        }\n",
    "      ],\n",
    "      \"sql_rename_dict\": {},\n",
    "      \"logging_info\": {\n",
    "          \"Timestamp\": int(datetime.now().timestamp())\n",
    "      }\n",
    "    }\n",
    "\n",
    "    return pipeline_input\n",
    "\n",
    "print(\"function: get_aep_pipeline_input loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6d6ee69",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AEP : 2 year flows ie: rf_2_inundation kicked off. Can take 15 - 45 mins.\n",
      "Step Function Pipeline : hv-vpp-ti-viz-pipeline : Pipeline run name started: sagemaker_aep_2_20241116T1902\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#### 2 Year Flow\n",
    "pipeline_input = get_aep_pipeline_input(\"2\")\n",
    "\n",
    "# notice, slightly different object name\n",
    "pipeline_name = f\"hv_ti_data_loads_sage_aep_2_{datetime.now().strftime('%Y%m%dT%H%M')}\"\n",
    "\n",
    "STEPFUNCTION_CLIENT.start_execution(\n",
    "    stateMachineArn = PIPELINE_ARN,\n",
    "    name = pipeline_name,\n",
    "    input= json.dumps(pipeline_input)\n",
    ")\n",
    "\n",
    "print(\"AEP : 2 year flows ie: rf_2_inundation kicked off. Can take 15 - 45 mins.\")\n",
    "print(f\"Step Function Pipeline : hv-vpp-ti-viz-pipeline : Pipeline run name started: {pipeline_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4f89d9a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AEP : 5 year flows ie: rf_5_inundation kicked off. Can take 15 - 45 mins.\n",
      "Step Function Pipeline : hv-vpp-ti-viz-pipeline : Pipeline run name started: sagemaker_aep_5_20241116T1921\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#### 5 Year Flow\n",
    "pipeline_input = get_aep_pipeline_input(\"5\")\n",
    "\n",
    "# notice, slightly different object name\n",
    "pipeline_name = f\"hv_ti_data_loads_sage_aep_5_{datetime.now().strftime('%Y%m%dT%H%M')}\"\n",
    "\n",
    "STEPFUNCTION_CLIENT.start_execution(\n",
    "    stateMachineArn = PIPELINE_ARN,\n",
    "    name = pipeline_name,\n",
    "    input= json.dumps(pipeline_input)\n",
    ")\n",
    "\n",
    "print(f\"AEP : 5 year flows ie: rf_5_inundation kicked off. Can take 15 - 45 mins.\")\n",
    "print(f\"Step Function Pipeline : hv-vpp-ti-viz-pipeline : Pipeline run name started: {pipeline_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "791d1a8b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AEP : 10 year flows ie: rf_10_inundation kicked off. Can take 15 - 45 mins.\n",
      "Step Function Pipeline : hv-vpp-ti-viz-pipeline : Pipeline run name started: sagemaker_aep_10_20241116T1921\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#### 10 Year Flow\n",
    "pipeline_input = get_aep_pipeline_input(\"10\")\n",
    "\n",
    "# notice, slightly different object name\n",
    "pipeline_name = f\"hv_ti_data_loads_sage_aep_10_{datetime.now().strftime('%Y%m%dT%H%M')}\"\n",
    "\n",
    "STEPFUNCTION_CLIENT.start_execution(\n",
    "    stateMachineArn = PIPELINE_ARN,\n",
    "    name = pipeline_name,\n",
    "    input= json.dumps(pipeline_input)\n",
    ")\n",
    "\n",
    "print(f\"AEP : 10 year flows ie: rf_10_inundation kicked off. Can take 15 - 45 mins.\")\n",
    "print(f\"Step Function Pipeline : hv-vpp-ti-viz-pipeline : Pipeline run name started: {pipeline_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bb87128",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AEP : 25 year flows ie: rf_25_inundation kicked off. Can take 15 - 45 mins.\n",
      "Step Function Pipeline : hv-vpp-ti-viz-pipeline : Pipeline run name started: sagemaker_aep_25_20241116T1921\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#### 25 Year Flow\n",
    "pipeline_input = get_aep_pipeline_input(\"25\")\n",
    "\n",
    "# notice, slightly different object name\n",
    "pipeline_name = f\"hv_ti_data_loads_sage_aep_25_{datetime.now().strftime('%Y%m%dT%H%M')}\"\n",
    "\n",
    "STEPFUNCTION_CLIENT.start_execution(\n",
    "    stateMachineArn = PIPELINE_ARN,\n",
    "    name = pipeline_name,\n",
    "    input= json.dumps(pipeline_input)\n",
    ")\n",
    "\n",
    "print(f\"AEP : 25 year flows ie: rf_25_inundation kicked off. Can take 15 - 45 mins.\")\n",
    "print(f\"Step Function Pipeline : hv-vpp-ti-viz-pipeline : Pipeline run name started: {pipeline_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4832e4e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AEP : 50 year flows ie: rf_50_inundation kicked off. Can take 15 - 45 mins.\n",
      "Step Function Pipeline : hv-vpp-ti-viz-pipeline : Pipeline run name started: sagemaker_aep_50_20241116T1921\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#### 50 Year Flow\n",
    "pipeline_input = get_aep_pipeline_input(\"50\")\n",
    "\n",
    "# notice, slightly different object name\n",
    "pipeline_name = f\"hv_ti_data_loads_sage_aep_50_{datetime.now().strftime('%Y%m%dT%H%M')}\"\n",
    "\n",
    "STEPFUNCTION_CLIENT.start_execution(\n",
    "    stateMachineArn = PIPELINE_ARN,\n",
    "    name = pipeline_name,\n",
    "    input= json.dumps(pipeline_input)\n",
    ")\n",
    "\n",
    "print(f\"AEP : 50 year flows ie: rf_50_inundation kicked off. Can take 15 - 45 mins.\")\n",
    "print(f\"Step Function Pipeline : hv-vpp-ti-viz-pipeline : Pipeline run name started: {pipeline_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "187e83bc-ebbe-4615-a046-e0ef7b09ad3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AEP : High Water year flows ie: rf_hw_inundation kicked off. Can take 15 - 45 mins.\n",
      "Step Function Pipeline : hv-vpp-ti-viz-pipeline : Pipeline run name started: sagemaker_aep_hw_20241116T1921\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#### HW (High Water) Flow\n",
    "pipeline_input = get_aep_pipeline_input(\"high_water\")\n",
    "\n",
    "# notice, slightly different object name\n",
    "pipeline_name = f\"hv_ti_data_loads_sage_aep_hw_{datetime.now().strftime('%Y%m%dT%H%M')}\"\n",
    "\n",
    "STEPFUNCTION_CLIENT.start_execution(\n",
    "     stateMachineArn = PIPELINE_ARN,\n",
    "     name = pipeline_name,\n",
    "     input= json.dumps(pipeline_input)\n",
    ")\n",
    "\n",
    "print(f\"AEP : High Water year flows ie: rf_hw_inundation kicked off. Can take 15 - 45 mins.\")\n",
    "print(f\"Step Function Pipeline : hv-vpp-ti-viz-pipeline : Pipeline run name started: {pipeline_name}\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c810767-2f5d-46b2-860c-5d5c549f2e2a",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h3>IMPORTANT: Return hv-vpp-ti-viz-fim-data-prep Lambda memory to 2048mb</h3>\n",
    "\n",
    "https://us-east-1.console.aws.amazon.com/lambda/home?region=us-east-1#/functions/hv-vpp-ti-viz-fim-data-prep?tab=code\n",
    "\n",
    "Lambda name: hv-vpp-ti-viz-hand-fim-processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fa6c78-efc2-4dbe-ae9a-55b3c4842492",
   "metadata": {},
   "source": [
    "<h2>6 - RUN CATCHMENT WORKFLOWS 2 CONFIGS AT A TIME. CHECK FOR STEP FUNCTION FINISHING BEFORE STARTING NEW ONE</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4574e4b-1569-4362-8d3f-42f4319752a1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 6a - Branch 0 Catchments. Wait until it is done before kicking off the next GMS (Level Path) catchments load a bit lower. ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c77d6e5b-f5fb-422f-a6c3-0f53743cd631",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Catchment Truncation for Branch 0 Done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Add backups to these (4.4.0.0)  (already not available for 4.4.0.0)\n",
    "\n",
    "\n",
    "# TODO: We likely need to keep the schema, so trun is fine for now, but eventually, get a lsit of the indexes and re-build \n",
    "# indexes each time as/if needed. Granted these tables are loaded via Lambdas, so I am not sure how indexes will play into that\n",
    "\n",
    "sf.execute_sql('''\n",
    "TRUNCATE \n",
    "    fim_catchments.branch_0_catchments, \n",
    "    fim_catchments.branch_0_catchments_hi, \n",
    "    fim_catchments.branch_0_catchments_prvi;\n",
    "''', db_type=\"egis\")\n",
    "\n",
    "print(\"Catchment Truncation for Branch 0 Done\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7495759",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Catchments Branch 0 load kicked off. Last runtime: 23:38.019. Step Function Pipeline : hv-vpp-ti-viz-pipeline : Run Name - sagemaker_0_catchments_20241116T1945\n"
     ]
    }
   ],
   "source": [
    "pipeline_input = {\n",
    "  \"configuration\": \"reference\",\n",
    "  \"job_type\": \"auto\",\n",
    "  \"data_type\": \"channel\",\n",
    "  \"keep_raw\": False,\n",
    "  \"reference_time\": datetime.now().strftime('%Y-%m-%d 00:00:00'),\n",
    "  \"configuration_data_flow\": {\n",
    "    \"db_max_flows\": [],\n",
    "    \"db_ingest_groups\": [],\n",
    "    \"python_preprocessing\": []\n",
    "  },\n",
    "  \"pipeline_products\": [\n",
    "    {\n",
    "      \"product\": \"static_hand_catchments_0_branches\",\n",
    "      \"configuration\": \"reference\",\n",
    "      \"product_type\": \"fim\",\n",
    "      \"run\": True,\n",
    "      \"fim_configs\": [\n",
    "        {\n",
    "          \"name\": \"catchments_0_branches\",\n",
    "          \"target_table\": \"fim_catchments.branch_0_catchments\",\n",
    "          \"fim_type\": \"hand\",\n",
    "          \"sql_file\": \"catchments_0_branches\"\n",
    "        }\n",
    "      ],\n",
    "      \"services\": [\n",
    "        \"static_hand_catchments_0_branches_noaa\"\n",
    "      ],\n",
    "      \"raster_outputs\": {\n",
    "        \"output_bucket\": \"\",\n",
    "        \"output_raster_workspaces\": []\n",
    "      },\n",
    "      \"postprocess_sql\": [],\n",
    "      \"product_summaries\": [],\n",
    "      \"python_preprocesing_dependent\": False\n",
    "    },\n",
    "    {\n",
    "      \"product\": \"static_hand_catchments_0_branches_hi\",\n",
    "      \"configuration\": \"reference\",\n",
    "      \"product_type\": \"fim\",\n",
    "      \"run\": True,\n",
    "      \"fim_configs\": [\n",
    "        {\n",
    "          \"name\": \"catchments_0_branches_hi\",\n",
    "          \"target_table\": \"fim_catchments.branch_0_catchments_hi\",\n",
    "          \"fim_type\": \"hand\",\n",
    "          \"sql_file\": \"catchments_0_branches_hi\"\n",
    "        }\n",
    "      ],\n",
    "      \"services\": [\n",
    "        \"static_hand_catchments_0_branches_hi_noaa\"\n",
    "      ],\n",
    "      \"raster_outputs\": {\n",
    "        \"output_bucket\": \"\",\n",
    "        \"output_raster_workspaces\": []\n",
    "      },\n",
    "      \"postprocess_sql\": [],\n",
    "      \"product_summaries\": [],\n",
    "      \"python_preprocesing_dependent\": False\n",
    "    },\n",
    "    {\n",
    "      \"product\": \"static_hand_catchments_0_branches_prvi\",\n",
    "      \"configuration\": \"reference\",\n",
    "      \"product_type\": \"fim\",\n",
    "      \"run\": True,\n",
    "      \"fim_configs\": [\n",
    "        {\n",
    "          \"name\": \"catchments_0_branches_prvi\",\n",
    "          \"target_table\": \"fim_catchments.branch_0_catchments_prvi\",\n",
    "          \"fim_type\": \"hand\",\n",
    "          \"sql_file\": \"catchments_0_branches_prvi\"\n",
    "        }\n",
    "      ],\n",
    "      \"services\": [\n",
    "        \"static_hand_catchments_0_branches_prvi_noaa\"\n",
    "      ],\n",
    "      \"raster_outputs\": {\n",
    "        \"output_bucket\": \"\",\n",
    "        \"output_raster_workspaces\": []\n",
    "      },\n",
    "      \"postprocess_sql\": [],\n",
    "      \"product_summaries\": [],\n",
    "      \"python_preprocesing_dependent\": False\n",
    "    }\n",
    "  ],\n",
    "  \"sql_rename_dict\": {},\n",
    "  \"logging_info\": {\n",
    "      \"Timestamp\": int(datetime.now().timestamp())\n",
    "  }\n",
    "}\n",
    "\n",
    "pipeline_name = f\"hv_ti_data_loads_sage_0_catchments_{datetime.now().strftime('%Y%m%dT%H%M')}\"\n",
    "\n",
    "# TODO: For later... fix fim_version value and add model_version column. current fim_version vlaue is showing 4.5.2.11\n",
    "\n",
    "STEPFUNCTION_CLIENT.start_execution(\n",
    "    stateMachineArn = PIPELINE_ARN,\n",
    "    name = pipeline_name,\n",
    "    input= json.dumps(pipeline_input)\n",
    ")\n",
    "\n",
    "print(\"Catchments Branch 0 load kicked off. Last runtime: 23:38.019. \"\n",
    "      f\"Step Function Pipeline : hv-vpp-ti-viz-pipeline : Run Name - {pipeline_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c75c4b7-bf13-4e8c-a33e-261219ce4338",
   "metadata": {},
   "source": [
    "### 6b - GMS (Level Paths / non branch 0) catchments ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea2a5be-9936-4abe-832a-8582f3c3af64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Add backups to these\n",
    "\n",
    "sf.execute_sql('''\n",
    "TRUNCATE\n",
    "    fim_catchments.branch_gms_catchments,\n",
    "    fim_catchments.branch_gms_catchments_hi,\n",
    "    fim_catchments.branch_gms_catchments_prvi;\n",
    "''', db_type=\"egis\")\n",
    "\n",
    "print(\"Catchment Truncation for GMS (Level Path) Branchs Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae8917f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline_input = {\n",
    "  \"configuration\": \"reference\",\n",
    "  \"job_type\": \"auto\",\n",
    "  \"data_type\": \"channel\",\n",
    "  \"keep_raw\": False,\n",
    "  \"reference_time\": datetime.now().strftime('%Y-%m-%d 00:00:00'),\n",
    "  \"configuration_data_flow\": {\n",
    "    \"db_max_flows\": [],\n",
    "    \"db_ingest_groups\": [],\n",
    "    \"python_preprocessing\": []\n",
    "  },\n",
    "  \"pipeline_products\": [\n",
    "    {\n",
    "      \"product\": \"static_hand_catchments_gms_branches\",\n",
    "      \"configuration\": \"reference\",\n",
    "      \"product_type\": \"fim\",\n",
    "      \"run\": True,\n",
    "      \"fim_configs\": [\n",
    "        {\n",
    "          \"name\": \"catchments_gms_branches\",\n",
    "          \"target_table\": \"fim_catchments.branch_gms_catchments\",\n",
    "          \"fim_type\": \"hand\",\n",
    "          \"sql_file\": \"catchments_gms_branches\"\n",
    "        }\n",
    "      ],\n",
    "      \"services\": [\n",
    "        \"static_hand_catchments_gms_branches_noaa\"\n",
    "      ],\n",
    "      \"raster_outputs\": {\n",
    "        \"output_bucket\": \"\",\n",
    "        \"output_raster_workspaces\": []\n",
    "      },\n",
    "      \"postprocess_sql\": [],\n",
    "      \"product_summaries\": [],\n",
    "      \"python_preprocesing_dependent\": False\n",
    "    },\n",
    "    {\n",
    "      \"product\": \"static_hand_catchments_gms_branches_hi\",\n",
    "      \"configuration\": \"reference\",\n",
    "      \"product_type\": \"fim\",\n",
    "      \"run\": True,\n",
    "      \"fim_configs\": [\n",
    "        {\n",
    "          \"name\": \"catchments_gms_branches_hi\",\n",
    "          \"target_table\": \"fim_catchments.branch_gms_catchments_hi\",\n",
    "          \"fim_type\": \"hand\",\n",
    "          \"sql_file\": \"catchments_gms_branches_hi\"\n",
    "        }\n",
    "      ],\n",
    "      \"services\": [\n",
    "        \"static_hand_catchments_gms_branches_hi_noaa\"\n",
    "      ],\n",
    "      \"raster_outputs\": {\n",
    "        \"output_bucket\": \"\",\n",
    "        \"output_raster_workspaces\": []\n",
    "      },\n",
    "      \"postprocess_sql\": [],\n",
    "      \"product_summaries\": [],\n",
    "      \"python_preprocesing_dependent\": False\n",
    "    },\n",
    "    {\n",
    "      \"product\": \"static_hand_catchments_gms_branches_prvi\",\n",
    "      \"configuration\": \"reference\",\n",
    "      \"product_type\": \"fim\",\n",
    "      \"run\": True,\n",
    "      \"fim_configs\": [\n",
    "        {\n",
    "          \"name\": \"catchments_gms_branches_prvi\",\n",
    "          \"target_table\": \"fim_catchments.branch_gms_catchments_prvi\",\n",
    "          \"fim_type\": \"hand\",\n",
    "          \"sql_file\": \"catchments_gms_branches_prvi\"\n",
    "        }\n",
    "      ],\n",
    "      \"services\": [\n",
    "        \"static_hand_catchments_gms_branches_prvi_noaa\"\n",
    "      ],\n",
    "      \"raster_outputs\": {\n",
    "        \"output_bucket\": \"\",\n",
    "        \"output_raster_workspaces\": []\n",
    "      },\n",
    "      \"postprocess_sql\": [],\n",
    "      \"product_summaries\": [],\n",
    "      \"python_preprocesing_dependent\": False\n",
    "    }\n",
    "  ],\n",
    "  \"sql_rename_dict\": {},\n",
    "  \"logging_info\": {\n",
    "      \"Timestamp\": int(datetime.now().timestamp())\n",
    "  }\n",
    "}\n",
    "\n",
    "pipeline_name = f\"hv_ti_data_loads_sage_gms_catchments_{datetime.now().strftime('%Y%m%dT%H%M')}\"\n",
    "\n",
    "# TODO: For later... fix fim_version value and add model_version column. current fim_version vlaue is showing 4.5.2.11\n",
    "\n",
    "STEPFUNCTION_CLIENT.start_execution(\n",
    "    stateMachineArn = PIPELINE_ARN,\n",
    "    name = pipeline_name,\n",
    "    input= json.dumps(pipeline_input)\n",
    ")\n",
    "\n",
    "print(\"Catchments GMS Branches (Level Paths / non branch 0) load kicked off.\"\n",
    "      f\" Last runtime: 24:45.150. Pipeline : hv-vpp-ti-viz-pipeline  - {pipeline_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fb7850-5be1-4127-86ba-bbfde273424a",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2>7 - Recreate derived.usgs_elev_table</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd0c144-5a20-4899-b946-a6dd9ae49b53",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Has appx 2,150 HUCs to process, but this section goes quickly.\n",
    "\n",
    "sf.execute_sql('DROP TABLE IF EXISTS derived.usgs_elev_table;')\n",
    "\n",
    "uet_usecols = ['location_id', 'HydroID', 'dem_adj_elevation', 'nws_lid', 'levpa_id']\n",
    "\n",
    "paginator = S3_CLIENT.get_paginator('list_objects')\n",
    "operation_parameters = {'Bucket': FIM_BUCKET,\n",
    "                        'Prefix': f'{HAND_DATASETS_DPATH}/',\n",
    "                        'Delimiter': '/'}\n",
    "page_iterator = paginator.paginate(**operation_parameters)\n",
    "page_count = 0\n",
    "num_pages = len(page_iterator)\n",
    "print(f\"num of pages is {num_pages}\")\n",
    "# for page in page_iterator:\n",
    "    \n",
    "#     prefix_objects = page['CommonPrefixes']\n",
    "#     for i, prefix_obj in enumerate(prefix_objects):\n",
    "#         display_index = (i + 1) + (1000 * page_count)\n",
    "#         print(f\"Processing {i+1} of\"\n",
    "#               f\" {len(prefix_objects) * page_count} on page {page_count + 1} (1000 per page)\")\n",
    "#         huc_prefix = prefix_obj.get(\"Prefix\")\n",
    "#         usgs_elev_table_key = f'{huc_prefix}usgs_elev_table.csv'\n",
    "#         try:\n",
    "#             uet = S3_CLIENT.get_object(\n",
    "#                 Bucket=FIM_BUCKET, \n",
    "#                 Key=usgs_elev_table_key\n",
    "#             )['Body']\n",
    "#             uet_df = pd.read_csv(uet, header=0, usecols=uet_usecols)\n",
    "#             uet_df['fim_version'] = PUBLIC_FIM_VERSION\n",
    "#             uet_df[COLUMN_NAME_MODEL_VERSION] = FIM_MODEL_VERSION\n",
    "#             uet_df.to_sql(\n",
    "#                 con=VIZ_DB_ENGINE,\n",
    "#                 dtype={\n",
    "#                     \"location_id\": Text(),\n",
    "#                     \"nws_data_huc\": Text()\n",
    "#                 },\n",
    "#                 schema='derived',\n",
    "#                 name='usgs_elev_table',\n",
    "#                 index=False, \n",
    "#                 if_exists='append'\n",
    "#             )\n",
    "#         except Exception as e:\n",
    "#             if \"NoSuchKey\" in str(e):\n",
    "#                 pass\n",
    "#             else:\n",
    "#                 raise e\n",
    "\n",
    "#     page_count += 1\n",
    "                                     \n",
    "                \n",
    "print(\"usgs_elev_tables load completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9951d778-5d75-4550-a63d-1dee833417f9",
   "metadata": {},
   "source": [
    "<h2>8 - Recreate derived.hydrotable_staggered</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33026eb6-fbc7-47f7-a44b-f9cea8f0911b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Takes appx 5.75 to 6 hrs to run\n",
    "\n",
    "print(\"hydrotable reloaded - started\")\n",
    "start_dt = datetime.now()\n",
    "\n",
    "sf.execute_sql('DROP TABLE IF EXISTS derived.hydrotable;')\n",
    "sql = '''\n",
    "SELECT distinct LPAD(huc8::text, 8, '0') as huc8 FROM derived.featureid_huc_crosswalk WHERE huc8 is not null;\n",
    "'''\n",
    "df = sf.sql_to_dataframe(sql)\n",
    "ht_usecols = ['HydroID', 'feature_id', 'stage', 'discharge_cms']\n",
    "\n",
    "paginator = S3_CLIENT.get_paginator('list_objects')\n",
    "operation_parameters = {'Bucket': FIM_BUCKET,\n",
    "                        'Prefix': f'{HAND_DATASETS_DPATH}/',\n",
    "                        'Delimiter': '/'}\n",
    "page_iterator = paginator.paginate(**operation_parameters)\n",
    "page_count = 0\n",
    "for page in page_iterator:\n",
    "\n",
    "    prefix_objects = page['CommonPrefixes']\n",
    "    for i, prefix_obj in enumerate(prefix_objects):\n",
    "        \n",
    "        print(f\"Processing {(i+1) + (1000 * page_count)} of\"\n",
    "              f\"{len(prefix_objects) * page_count} on page {page_count + 1} (1000 per page)\")        \n",
    "        \n",
    "        print(f\"Processing {(i+1) + (1000 * page_count)} of\"\n",
    "              f\" {len(prefix_objects) * page_count} on page {page_count + 1} (1000 per page)\")\n",
    "        branch_prefix = f'{prefix_obj.get(\"Prefix\")}branches/0/'\n",
    "        branch_files_result = S3_CLIENT.list_objects(\n",
    "            Bucket=FIM_BUCKET, \n",
    "            Prefix=branch_prefix, \n",
    "            Delimiter='/'\n",
    "        )\n",
    "        hydro_table_key = None\n",
    "        for content_obj in branch_files_result.get('Contents'):\n",
    "            branch_file_prefix = content_obj['Key']\n",
    "            if 'hydroTable' in branch_file_prefix:\n",
    "                hydro_table_key = branch_file_prefix\n",
    "\n",
    "        if hydro_table_key:\n",
    "            # print(f\"Found usgs_elev_table and hydroTable in {branch_prefix}\")\n",
    "            try:\n",
    "                # print(\"...Fetching csvs...\")\n",
    "                ht = S3_CLIENT.get_object(\n",
    "                    Bucket=FIM_BUCKET,\n",
    "                    Key=hydro_table_key\n",
    "                )['Body']\n",
    "                # print(\"...Reading with pandas...\")\n",
    "                ht_df = pd.read_csv(ht, header=0, usecols=ht_usecols)\n",
    "                # print('...Writing to db...')\n",
    "                ht_df['fim_version'] = PUBLIC_FIM_VERSION\n",
    "                ht_df[COLUMN_NAME_MODEL_VERSION] = FIM_MODEL_VERSION\n",
    "                ht_df.to_sql(\n",
    "                    con=VIZ_DB_ENGINE, \n",
    "                    schema='derived',\n",
    "                    name='hydrotable',\n",
    "                    index=False,\n",
    "                    if_exists='append'\n",
    "                )\n",
    "            except Exception as e:\n",
    "                raise e\n",
    "                print(f'Fetch failed: {e}')\n",
    "                \n",
    "        page_count += 1\n",
    "                \n",
    "                \n",
    "end_dt = datetime.now()\n",
    "time_duration = end_dt - start_dt\n",
    "print(\"hydrotable reload done\")\n",
    "print(f\"... duration was  {str(time_duration).split('.')[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc764e4-7e67-4b51-a100-6021e31416cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"hydrotable_staggered started\")\n",
    "\n",
    "start_dt = datetime.now()\n",
    "\n",
    "sql = '''\n",
    "DROP TABLE IF EXISTS derived.hydrotable_staggered;\n",
    "SELECT\n",
    "    et.location_id,\n",
    "    ht.feature_id,\n",
    "    (stage + et.dem_adj_elevation) * 3.28084 as elevation_ft,\n",
    "    LEAD((stage + et.dem_adj_elevation) * 3.28084) OVER (PARTITION BY ht.feature_id ORDER BY ht.feature_id, stage) as next_elevation_ft,\n",
    "    discharge_cms * 35.3147 as discharge_cfs,\n",
    "    LEAD(discharge_cms * 35.3147) OVER (PARTITION BY ht.feature_id ORDER BY ht.feature_id, stage) as next_discharge_cfs\n",
    "INTO derived.hydrotable_staggered\n",
    "FROM derived.hydrotable AS ht\n",
    "JOIN derived.usgs_elev_table AS et ON ht.\"HydroID\" = et.\"HydroID\" AND et.location_id IS NOT NULL;\n",
    "'''\n",
    "sf.execute_sql(sql)\n",
    "\n",
    "print(\"hydrotable_staggered reload done\")\n",
    "end_dt = datetime.now()\n",
    "time_duration = end_dt - start_dt\n",
    "print(f\"... duration was  {str(time_duration).split('.')[0]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f21c03-ca36-402c-a69b-396162720f66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# we don't need the hydrotable anymore as it has been reloaded and adjusted above in hydrotable_staggered\n",
    "sf.execute_sql('DROP TABLE IF EXISTS derived.hydrotable;')\n",
    "print(\"Done dropping derived.hydrotable, post hydrotable_staggered load\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16ad30a-f0d6-4fba-8a9f-059d3d11ff6a",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2>9 - Recreate derived.usgs_rating_curves_staggered</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461f20b4-8442-4c0e-9114-ea62764cd789",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Aug 16, 2024 - done for 4.4.0.0 (4.5.2.11)\n",
    "\n",
    "# TODO: Aug 2024: Change this to a backup without indexes and not rename\n",
    "# Aug 27, 2024: This needs to be redone so we don't rename tables, it messes up indexes and index names when we use _to_sql commands later\n",
    "\n",
    "# sf.execute_sql(f'ALTER TABLE IF EXISTS derived.usgs_rating_curves RENAME TO usgs_rating_curves_{OLD_FIM_TAG};')\n",
    "# sf.execute_sql(f'ALTER TABLE IF EXISTS derived.usgs_rating_curves_staggered RENAME TO usgs_rating_curves_staggered_{OLD_FIM_TAG};')\n",
    "# print(\"usgs rating curve tables renamed and cleaned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b752fce5-c8d7-4064-92d7-842c22d1723e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "sql = '''\n",
    "    DROP TABLE IF EXISTS derived.usgs_rating_curves;\n",
    "    DROP TABLE IF EXISTS derived.usgs_rating_curves_staggered;\n",
    "'''\n",
    "sf.execute_sql(sql)\n",
    "\n",
    "print(\"Done dropping usgs_rating_curves and usgs_rating_curves_staggered\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a0cc18-de82-4bd8-bd4d-c3ad021a1dec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run the script to load the usgs_rating_curve.csv. Exact duration not yet known. Appx 30 min (??)\n",
    "\n",
    "start_dt = datetime.now()\n",
    "event = {\n",
    "    'target_table': 'derived.usgs_rating_curves',\n",
    "    'target_cols': ['location_id', 'flow', 'stage', 'navd88_datum', 'elevation_navd88'],\n",
    "    'file': f'{QA_DATASETS_DPATH}/usgs_rating_curves.csv',\n",
    "    'bucket': FIM_BUCKET,\n",
    "    'reference_time': '2023-08-23 00:00:00',\n",
    "    'keep_flows_at_or_above': 0,\n",
    "    'iteration_index': 0\n",
    "}\n",
    "\n",
    "sf.execute_db_ingest(event, None)\n",
    "\n",
    "print(\"done loading usgs_rating_curves\")\n",
    "end_dt = datetime.now()\n",
    "time_duration = end_dt - start_dt\n",
    "print(f\"... duration was  {str(time_duration).split('.')[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4548d5e8-303e-44e0-a8a5-7b1947214b4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Takes under a minute\n",
    "print(\"Starting usgs_rating_curves_staggered build based on usgs_rating_curve table\")\n",
    "\n",
    "sql = '''\n",
    "SELECT \n",
    "    location_id,\n",
    "    flow as discharge_cfs, \n",
    "    LEAD(flow) OVER (PARTITION BY location_id ORDER BY location_id, stage) as next_discharge_cfs,\n",
    "    stage,\n",
    "    navd88_datum,\n",
    "    elevation_navd88 as elevation_ft,\n",
    "    LEAD(elevation_navd88) OVER (PARTITION BY location_id ORDER BY location_id, stage) as next_elevation_ft\n",
    "INTO derived.usgs_rating_curves_staggered\n",
    "FROM derived.usgs_rating_curves;\n",
    "'''\n",
    "\n",
    "sf.execute_sql(sql)\n",
    "\n",
    "print(\"Done loading usgs_rating_curves_staggered\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a77875-f569-4410-888c-b132714a7c9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# usgs_rating_curves is a temp table and is loaded with some changes into the usgs_rating_curves_staggered\n",
    "sf.execute_sql('DROP TABLE IF EXISTS derived.usgs_rating_curves;')\n",
    "print(\"Done dropping derived.usgs_rating_curves, post loading usgs_rating_curves_staggered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e926c7a-9d1c-41ea-a9b3-5fc688994157",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2>10 - UPDATE SRC SKILL METRICS IN DB</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c472f8-0557-4864-8536-1814f3ac4286",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Already run for 4.4.0.0 (4.5.2.11)\n",
    "\n",
    "'''\n",
    "Be Very Careful to just rename tables. If they have indexes, the index will now point to the new\n",
    "table names but maintain the original index name. Those index names can really mess stuff up.\n",
    "Best to never rename unless you rename indexes as well. This particular on is ok. \n",
    "Note: When various '\"to_sql\" tools are run which have GIST indexes, this index column name issue\n",
    "will be the problem.\n",
    "\n",
    "Why Drop instead of Truncate? if the schema changes for the incoming, truncate will have column\n",
    "missmatches.\n",
    "\n",
    "We really should be backing up indexes and constraints as well.\n",
    "\n",
    "'''\n",
    "\n",
    "# TODO: Aug 2024: Change this away from \"rename\" to copy / drop. \n",
    "# sf.execute_sql(f'ALTER TABLE IF EXISTS derived.src_skill_temp RENAME TO src_skill_temp_{OLD_FIM_TAG};')\n",
    "# sf.execute_sql(f'ALTER TABLE IF EXISTS derived.src_skill RENAME TO src_skill_{OLD_FIM_TAG};')\n",
    "\n",
    "# print(\"src_skill and src_skill_temps db renamed\")\n",
    "\n",
    "\n",
    "# TODO: Rob Aug 2024: change this to backup of table and not rename as it messses with indexes\n",
    "# Don't need a copy of the reference src_skill table , so just drop it.\n",
    "new_table_name = f\"derived.src_skill_temp_{OLD_FIM_TAG}\"\n",
    "sql = f'''\n",
    "   CREATE TABLE IF NOT EXISTS {new_table_name} AS TABLE derived.src_skill_temp;\n",
    "'''\n",
    "\n",
    "\n",
    "#print(\"src_skill and src_skill_temps db renamed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a249854-b9cd-4ccb-afea-96c35708515d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prep the dbs for the new load\n",
    "#sf.execute_sql('DROP TABLE IF EXISTS derived.src_skill_temp;')\n",
    "#sf.execute_sql('DROP TABLE IF EXISTS reference.src_skill;', db_type='egis')\n",
    "#print(\"Done dropping src_skill and src_skill_temp tables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24f509a-90df-492a-a0d4-aa9eab8e7041",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the src_skill_temp table\n",
    "start_dt = datetime.now()\n",
    "\n",
    "event = {\n",
    "    'target_table': 'derived.src_skill_temp',\n",
    "    'target_cols': None,  # This means \"all\"\n",
    "    'file': f'{QA_DATASETS_DPATH}/agg_nwm_recurr_flow_elev_stats_location_id.csv',\n",
    "    'bucket': FIM_BUCKET,\n",
    "    'reference_time': '2023-08-23 00:00:00',\n",
    "    'keep_flows_at_or_above': 0,\n",
    "    'iteration_index': 0,\n",
    "    'db_type': 'viz'\n",
    "}\n",
    "\n",
    "execute_db_ingest(event, None)\n",
    "print(\"Done loading derived.src_skill_temp table\")\n",
    "end_dt = datetime.now()\n",
    "time_duration = end_dt - start_dt\n",
    "print(f\"... duration was  {str(time_duration).split('.')[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5639447-9a5a-4add-9c36-ebd945a5a8d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load into src_skill table adding geometry to it from external.usgs_gage. Yes.. more/less straight from WRDS tables\n",
    "# Some recs appear to be in error in the csv. location id = 394220106431500 (those are dropped below)\n",
    "\n",
    "start_dt = datetime.now()\n",
    "\n",
    "sf.execute_sql('DROP TABLE IF EXISTS derived.src_skill;')\n",
    "\n",
    "sql = f\"\"\"\n",
    "SELECT\n",
    "\t(row_number() OVER ())::int as oid,\n",
    "\tgage.name,\n",
    "\tLPAD(skill.location_id::text, 8, '0') as location_id,\n",
    "\tskill.nrmse,\n",
    "\tskill.mean_abs_y_diff_ft,\n",
    "\tskill.mean_y_diff_ft,\n",
    "\tskill.percent_bias,\n",
    "    '{PUBLIC_FIM_VERSION}' as {COLUMN_NAME_FIM_VERSION},\n",
    "    '{FIM_MODEL_VERSION}' as {COLUMN_NAME_MODEL_VERSION},\n",
    "\tgage.geo_point as geom\n",
    "INTO derived.src_skill\n",
    "FROM derived.src_skill_temp skill\n",
    "JOIN external.usgs_gage AS gage ON LPAD(gage.usgs_gage_id::text, 8, '0') = LPAD(skill.location_id::text, 8, '0')\n",
    "\"\"\"\n",
    "\n",
    "sf.execute_sql(sql)\n",
    "\n",
    "print(\"Done loading derived.src_skill table\")\n",
    "end_dt = datetime.now()\n",
    "time_duration = end_dt - start_dt\n",
    "print(f\"... duration was  {str(time_duration).split('.')[0]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa32cd0-6edc-4441-9be3-f5bcba499ad0",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h4>Then export the derived.src_skill table and import it into the EGIS reference.src_skill table</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df23345-47a4-490a-b10f-2020e3a2a68e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "sf.move_data_from_viz_to_egis(\"derived.src_skill\", \"reference.src_skill\")\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118c126e-00b8-4c7f-8a8d-0bb427f413f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2>11 - UPDATE FIM PERFORMANCE METRICS IN DB</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b291a461-e36f-4049-84b6-dd59cd1c4ad2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Make copies of current dbs for 4.4.0.0 (4.5.2.11)\n",
    "# DONE: for 4.4.0.0 (4.5.2.11)\n",
    "\n",
    "# NOTE: Aug 2024: The problem with not droppign them and rebuilding them with indexes, is that if the table schema\n",
    "# changes it is not reflected\n",
    "\n",
    "\n",
    "# Points\n",
    "new_table_name = f\"reference.fim_performance_points_{OLD_FIM_TAG}\"\n",
    "sql = f'''\n",
    "    CREATE TABLE IF NOT EXISTS {new_table_name} AS TABLE reference.fim_performance_points;\n",
    "'''\n",
    "sf.execute_sql(sql, db_type='egis')\n",
    "print(f\"fim_performance_points copied to {new_table_name} if it does not already exists\")\n",
    "\n",
    "\n",
    "# Catchments\n",
    "new_table_name = f\"reference.fim_performance_catchments_{OLD_FIM_TAG}\"\n",
    "sql = f'''\n",
    "   CREATE TABLE IF NOT EXISTS {new_table_name} AS TABLE reference.fim_performance_catchments;\n",
    "'''\n",
    "sf.execute_sql(sql, db_type='egis')\n",
    "print(f\"fim_performance_catchments copied to {new_table_name} if it does not already exists\")\n",
    "\n",
    "\n",
    "# Polys\n",
    "new_table_name = f\"reference.fim_performance_polys_{OLD_FIM_TAG}\"\n",
    "sql = f'''\n",
    "   CREATE TABLE IF NOT EXISTS {new_table_name} AS TABLE reference.fim_performance_polys;\n",
    "'''\n",
    "sf.execute_sql(sql, db_type='egis')\n",
    "print(f\"fim_performance_polys copied to {new_table_name} if it does not already exists\")\n",
    "\n",
    "print(\"Done making backups of the FIM performance tables\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fcdfab-3936-4a0a-8ab9-605e9a64f9bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# clean up tables for new load\n",
    "\n",
    "# TODO: Aug 2024: Add postgresql if / else. Truncate \"if exists\" doesn't exist. :)\n",
    "\n",
    "table_names = [\n",
    "    \"reference.fim_performance_points\",\n",
    "    \"reference.fim_performance_polys\",\n",
    "    \"reference.fim_performance_catchments\"\n",
    "]\n",
    "\n",
    "for tb_name in table_names:\n",
    "    sql = f\"TRUNCATE TABLE {tb_name}\"\n",
    "#    print(sql)\n",
    "    sf.execute_sql(sql,db_type='egis')\n",
    "\n",
    "\n",
    "print(f\"All fim_performance tables trunated if they exist\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ca5581",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the new fim performance tables\n",
    "\n",
    "start_dt = datetime.now()\n",
    "\n",
    "# os.environ['EGIS_DB_HOST'] =''  #TI DB\n",
    "\n",
    "db_type = \"egis\"\n",
    "db_engine = sf.get_db_engine(db_type)\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Define bucket and parent directories.\n",
    "bucket = \"hydrovis-ti-deployment-us-east-1\"\n",
    "\n",
    "# file_handles = ['fim_performance_points.csv']\n",
    "# file_handles = ['fim_performance_points.csv', 'fim_performance_polys.csv', 'fim_performance_catchments_dissolved.csv']\n",
    "# file_handles = ['fim_performance_points.csv', 'fim_performance_polys.csv']\n",
    "file_handles = ['fim_performance_catchments.csv']\n",
    "\n",
    "for file_handle in file_handles:\n",
    "\n",
    "    print(\"Reading file...\")\n",
    "    # df = pd.read_csv(local_download_path)\n",
    "    file_to_download = f\"{QA_DATASETS_DPATH}/{file_handle}\"\n",
    "    df = s3_sf.download_S3_csv_files_to_df_from_list(FIM_BUCKET, [file_to_download], True)\n",
    "    print(\"File read.\")\n",
    "\n",
    "    # Rename headers.\n",
    "\n",
    "    if file_handle == 'fim_performance_points.csv':\n",
    "        df = df.rename(columns={'Unnamed: 0': 'oid', 'geometry': 'geom'})\n",
    "    else:\n",
    "        df = df.rename(columns={'Unnamed: 0': 'oid', 'geometry': 'geom', 'huc':'huc8'})\n",
    "\n",
    "    print(df.dtypes)\n",
    "    # Convert all field names to lowercase (needed for ArcGIS Pro).\n",
    "    df.columns = df.columns.str.lower()\n",
    "\n",
    "    # Enforce data types on df before loading in DB (TODO: need to create special cases for each layer).\n",
    "    if file_handle == 'fim_performance_points.csv':\n",
    "        df = df.astype({'huc': 'str'})\n",
    "    else:\n",
    "        df = df.astype({'huc8': 'str'})\n",
    "    df = df.fillna(0)\n",
    "    try:\n",
    "        df = df.astype({'feature_id': 'int'})\n",
    "        df = df.astype({'feature_id': 'str'})\n",
    "        df = df.astype({'oid': 'int'})\n",
    "    except KeyError:  # If there is no feature_id field\n",
    "        pass\n",
    "    try:\n",
    "        df = df.astype({'nwm_seg': 'int'})\n",
    "        df = df.astype({'nwm_seg': 'str'})\n",
    "    except KeyError:  # If there is no nwm_seg field\n",
    "        pass\n",
    "    try:\n",
    "        df = df.astype({'usgs_gage': 'int'})\n",
    "        df = df.astype({'usgs_gage': 'str'})\n",
    "    except KeyError:  # If there is no usgs_gage field\n",
    "        pass\n",
    "\n",
    "    # zfill HUC8 field.\n",
    "    if file_handle == 'fim_performance_points.csv':\n",
    "        df['huc'] = df['huc'].apply(lambda x: x.zfill(8))\n",
    "    else:\n",
    "        df['huc8'] = df['huc8'].apply(lambda x: x.zfill(8))\n",
    "\n",
    "    df['version'] = PUBLIC_FIM_VERSION\n",
    "    df[COLUMN_NAME_MODEL_VERSION] = FIM_MODEL_VERSION\n",
    "\n",
    "    # Upload df to database.\n",
    "    stripped_layer_name = file_handle.replace(\".csv\", \"\")\n",
    "    table_name = \"reference.\" + stripped_layer_name\n",
    "    print(\"Loading data into DB...\")\n",
    "\n",
    "    # Chunk load data into DB\n",
    "\n",
    "    if file_handle in ['fim_performance_catchments.csv']:\n",
    "\n",
    "        print(\"Chunk loading...\")\n",
    "        # Create list of df chunks\n",
    "        n = 10000  # chunk row size\n",
    "        list_df = [df[i:i+n] for i in range(0,df.shape[0],n)]\n",
    "        # geometry = 'MULTIPOLYGON'\n",
    "        # Load the first chunk into the DB as a new table\n",
    "        first_chunk_df = list_df[0]\n",
    "        print(first_chunk_df.shape[0])\n",
    "\n",
    "        first_chunk_df.to_sql(\n",
    "            name=stripped_layer_name, \n",
    "            con=db_engine, \n",
    "            schema='reference',\n",
    "            if_exists='replace', \n",
    "            index=False,\n",
    "            dtype={'oid': sqlalchemy.types.Integer(),\n",
    "                   'version': sqlalchemy.types.String(),\n",
    "                   'geom': Geometry('MULTIPOLYGON', srid=3857)\n",
    "                  }\n",
    "        )\n",
    "        # Load remaining chunks into newly created table\n",
    "\n",
    "        for remaining_chunk_df in list_df[1:]:\n",
    "            print(remaining_chunk_df.shape[0])\n",
    "            remaining_chunk_df.to_sql(\n",
    "                name=stripped_layer_name,\n",
    "                con=db_engine,\n",
    "                schema='reference',\n",
    "                if_exists='append',\n",
    "                index=False,\n",
    "                dtype={'oid': sqlalchemy.types.Integer(),\n",
    "                       'version': sqlalchemy.types.String(),\n",
    "                       'geom': Geometry('MULTIPOLYGON', srid=3857)\n",
    "                      }\n",
    "            )\n",
    "    else:\n",
    "        if 'points' in stripped_layer_name: geometry = 'POINT'\n",
    "        if 'polys' in stripped_layer_name: geometry = 'POLYGON'\n",
    "        # print(\"GEOMETRY\")\n",
    "        # print(geometry)\n",
    "        df.to_sql(\n",
    "            name=stripped_layer_name,\n",
    "            con=db_engine,\n",
    "            schema='reference',\n",
    "            if_exists='replace',\n",
    "            index=False,\n",
    "            dtype={'oid': sqlalchemy.types.Integer(),\n",
    "                   'version': sqlalchemy.types.String(),\n",
    "                   'geom': Geometry(geometry, srid=3857)\n",
    "                  }\n",
    "        )\n",
    "\n",
    "    print(f\">>> {file_handle} downloaded and loaded\")\n",
    "\n",
    "    # deleted the downloaded file that was just processed.\n",
    "    # if os.path.exists(local_download_path):\n",
    "\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "end_dt = datetime.now()\n",
    "time_duration = end_dt - start_dt\n",
    "# print(\"All FIM Performance files loaded\")\n",
    "print(f\"... duration was  {str(time_duration).split('.')[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9309a4b-1d94-4af2-90dc-57a08c4add68",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2>12 - CatFIM (Stage-Based and Flow-Based)</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7938fcb2-313c-4cf8-8274-d34d80bdf7bf",
   "metadata": {},
   "source": [
    "<h4>Function to load CatFIM Data (Non Public)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f37c81-d105-4c38-aa49-b4aef40a7543",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "''' Function to load CatFIM data (for any flow / stage / library / sites but non public)'''\n",
    "\n",
    "\n",
    "def load_catfim_table(catfim_type):\n",
    "\n",
    "    '''\n",
    "    Inputs:\n",
    "        - catfim_type: name identififer for the set, such as \"flow_based_catfim\" or \"flow_based_catfim_sites\", etc\n",
    "              Sometimes the file_handle name can be the name of the s3 file (without extension) and/or the table\n",
    "              name.\n",
    "              Options: flow_based_catfim, flow_based_catfim_sites, stage_based_catfim, stage_based_catfim_sites\n",
    "    '''\n",
    "\n",
    "    db_type = \"egis\"\n",
    "    db_engine = sf.get_db_engine(db_type)\n",
    "    src_crs = \"3857\"\n",
    "\n",
    "    # --------------------------------------\n",
    "    # Drop the original Db if already in place\n",
    "    table_name = catfim_type  # yes, dup variable for now\n",
    "\n",
    "    sf.execute_sql(f\"DROP TABLE IF EXISTS reference.{table_name};\", db_type=db_type)\n",
    "    print(f\"Dropping reference.{table_name} table if it existed\")\n",
    "    print(\"\")\n",
    "\n",
    "    # --------------------------------------\n",
    "    # Get the data from S3 and load it into a df\n",
    "    if catfim_type in ['flow_based_catfim', 'stage_based_catfim']:\n",
    "        file_to_download = f\"{QA_DATASETS_DPATH}/{catfim_type}_library.csv\"\n",
    "    else:\n",
    "        file_to_download = f\"{QA_DATASETS_DPATH}/{catfim_type}.csv\"\n",
    "\n",
    "    # print(f\"Downloading {file_to_download} ... \")\n",
    "\n",
    "    df = s3_sf.download_S3_csv_files_to_df_from_list(FIM_BUCKET, [file_to_download], True)\n",
    "    num_recs = len(df)\n",
    "    print(f\"File read. {num_recs} records to load\")\n",
    "\n",
    "    # --------------------------------------\n",
    "    # Adjusting Columns and data\n",
    "    # Rename headers. All files this name\n",
    "    df = df.rename(columns={'Unnamed: 0': 'oid',\n",
    "                            'geometry': 'geom',\n",
    "                            'huc': 'huc8'})\n",
    "\n",
    "    # Convert all field names to lowercase (needed for ArcGIS Pro).\n",
    "    df.columns = df.columns.str.lower()\n",
    "\n",
    "    # Enforce data types on df before loading in DB (TODO: need to create special cases for each layer).\n",
    "    df = df.astype({'huc8': 'str'})\n",
    "    df = df.fillna(0)\n",
    "    try:\n",
    "        df = df.astype({'feature_id': 'int'})\n",
    "        df = df.astype({'feature_id': 'str'})\n",
    "    except KeyError:  # If there is no feature_id field\n",
    "        pass\n",
    "    try:\n",
    "        df = df.astype({'nwm_seg': 'int'})\n",
    "        df = df.astype({'nwm_seg': 'str'})\n",
    "    except KeyError:  # If there is no nwm_seg field\n",
    "        pass\n",
    "    try:\n",
    "        df = df.astype({'usgs_gage': 'int'})\n",
    "        df = df.astype({'usgs_gage': 'str'})\n",
    "    except KeyError:  # If there is no usgs_gage field\n",
    "        pass\n",
    "\n",
    "    # zfill HUC8 field.\n",
    "    df['huc8'] = df['huc8'].apply(lambda x: x.zfill(8))\n",
    "\n",
    "    if '_sites' in catfim_type:\n",
    "        df = df.astype({'nws_data_rfc_forecast_point': 'str'})\n",
    "        df = df.astype({'nws_data_rfc_defined_fcst_point': 'str'})\n",
    "        df = df.astype({'nws_data_riverpoint': 'str'})\n",
    "\n",
    "\n",
    "    # As of Nov 1, 2024: Ignore the incoming \"version\" from dataset\n",
    "    # df['version'] = PUBLIC_FIM_VERSION\n",
    "    df[COLUMN_NAME_FIM_VERSION] = PUBLIC_FIM_VERSION\n",
    "    df[COLUMN_NAME_MODEL_VERSION] = FIM_MODEL_VERSION\n",
    "\n",
    "    # --------------------------------------\n",
    "    # Load to DB\n",
    "    # Chunk load data into DB\n",
    "    if catfim_type in ['flow_based_catfim', 'stage_based_catfim']:\n",
    "\n",
    "        # Create list of df chunks\n",
    "        n = 1000  # chunk row size\n",
    "        print(f\"Chunk loading... into {table_name} -- {n} records at a time\")\n",
    "        print(\"\")\n",
    "        chunk_df = [df[i:i+n] for i in range(0, df.shape[0], n)]\n",
    "\n",
    "        # Load the first chunk into the DB as a new table\n",
    "        first_chunk_df = chunk_df[0]\n",
    "        num_chunks = len(chunk_df)\n",
    "\n",
    "        print(f\" ... loading chunk 1 of {num_chunks}\")\n",
    "\n",
    "        first_chunk_df.to_sql(\n",
    "            name=table_name,\n",
    "            con=db_engine,\n",
    "            schema='reference',\n",
    "            if_exists='replace',\n",
    "            index=False,\n",
    "            dtype={'oid': sqlalchemy.types.Integer(),\n",
    "                   'geom': Geometry('MULTIPOLYGON', srid=src_crs)}\n",
    "        )\n",
    "\n",
    "        # Load remaining chunks into newly created table\n",
    "        ctr = 1  # Already loaded one\n",
    "        for remaining_chunk in chunk_df[1:]:\n",
    "            # print(remaining_chunk.shape[0])\n",
    "            ctr += 1\n",
    "            print(f\" ... loading chunk {ctr} of {num_chunks}\")\n",
    "            remaining_chunk.to_sql(\n",
    "                        name=table_name,\n",
    "                        con=db_engine,\n",
    "                        schema='reference',\n",
    "                        if_exists='append',\n",
    "                        index=False,\n",
    "                        dtype={'oid': sqlalchemy.types.Integer(),\n",
    "                               'geom': Geometry('MULTIPOLYGON', srid=src_crs)\n",
    "                              }\n",
    "                    )\n",
    "        # end for\n",
    "    else:  # sites tables\n",
    "        print(f\"Loading data into {table_name} ...\")\n",
    "\n",
    "        df.to_sql(\n",
    "            name=table_name,\n",
    "            con=db_engine,\n",
    "            schema='reference',\n",
    "            if_exists='replace',\n",
    "            index=False,\n",
    "            dtype={'oid': sqlalchemy.types.Integer(),\n",
    "                   'geom': Geometry('POINT', srid=src_crs)}\n",
    "        )\n",
    "\n",
    "    # This should auto create a gist index against the geometry column\n",
    "    # if that index name already exists, the upload will fail, the index can not pre-exist\n",
    "    # Best to drop the table before loading.\n",
    "\n",
    "    # return\n",
    "\n",
    "print(\"load_catfim_table function loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca12f850-8826-4177-884a-04f461496d7b",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h3>12.a - Backup old DBs and prepare new databases (but not the \"public\" FIM 10/30 db's)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9577edb-4aa6-423b-819e-df8c922c7ec2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This covers both Stage Based and Flow Based (but not the \"public\" catfim db's)\n",
    "\n",
    "# The \"Public\" db backups ana loads are in cells lower (12.d and higher)\n",
    "\n",
    "# DONE for 4.4.0.0.  (4.5.2.11)\n",
    "\n",
    "# # print(\"Starting Data Backups and table drops for stage and flow based catfim\")\n",
    "# db_names = [\"stage_based_catfim\", \"stage_based_catfim_sites\",\n",
    "#             \"flow_based_catfim\", \"flow_based_catfim_sites\"]\n",
    "\n",
    "# for db_name in db_names:\n",
    "#     new_table_name = f\"reference.{db_name}_{OLD_FIM_TAG}\"\n",
    "#     sql = f'''\n",
    "#         CREATE TABLE IF NOT EXISTS {new_table_name} AS TABLE reference.{db_name};\n",
    "#     '''\n",
    "#     sf.execute_sql(sql, db_type='egis')\n",
    "#     print(f\"{db_name} copied to {new_table_name} if it does not already exist\")\n",
    "\n",
    "\n",
    "# Aug 2024: Now we can drop the tables as we don't have any indexes on them at this time other than the gist geom index.\n",
    "# By dropping them, we can auto adjust the tables schema. (don't truncate)\n",
    "\n",
    "# for db_name in db_names:\n",
    "#     sf.execute_sql(f\"DROP TABLE IF EXISTS reference.{db_name};\", db_type='egis')\n",
    "#     print(f\"reference.{db_name} table dropped if it existed\")\n",
    "\n",
    "\n",
    "# print(\"Data Backups of flow based catfim are complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0af91cf-5a87-4c0b-a7ae-54d6809dd152",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h3>12.b - Updated Flow and Stage Based CatFIM Data (Non Public)</h3>\n",
    "\n",
    "<h3>AUG 2024: IMPORTANT NOTE:</h3>\n",
    "The stage based catfim (library) csv has grown to appx 10 GiB. Our current notebook, hv-vpp-ti-viz-notebook only has 15 GiB memory.\n",
    "Running tool can easily overwhelm the notebook server and freeze it up forcing a reboot.\n",
    "Sometimes when the notebook instance comes back up, it no longer has ths swap system in place. You will need most of the memory\n",
    "and some swap to load it.  Keep an eye a \"terminal\" windows and keep entering `free -h` to keep an eye on it's usage.\n",
    "</br>\n",
    "We will need to review to see if we want to:\n",
    "\n",
    "1. Upgrade this notebook server with more memory (and harddrive space would be good)\n",
    "\n",
    "2. Change the load of the catfim library (non sites) data to another system. Maybe we can load it via a lambda to an EC2 or something?\n",
    "\n",
    "3. Get the FIM Team to break it to smaller pieces, but watch carefully for the OID system (unique id for all records)\n",
    "\n",
    "**When you are done running this script, Please restart this kernal as it does not appear to be releasing all memory. (memory leak?)**\n",
    "\n",
    "\n",
    "Also looks like Tyler has some notebooks where he was moving this into a lambda load? We need to look into that\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c55cd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"Starting of CatFIM data\")\n",
    "\n",
    "# catfim_types = ['flow_based_catfim', 'flow_based_catfim_sites']\n",
    "# catfim_types =  ['stage_based_catfim', 'stage_based_catfim_sites']\n",
    "catfim_types = ['stage_based_catfim_sites']\n",
    "# catfim_types = ['stage_based_catfim']\n",
    "\n",
    "start_dt = datetime.now()\n",
    "\n",
    "for catfim_type in catfim_types:\n",
    "    print(f\"Loading {catfim_type} data\")\n",
    "    load_catfim_table(catfim_type)\n",
    "\n",
    "print(\"\")\n",
    "end_dt = datetime.now()\n",
    "time_duration = end_dt - start_dt\n",
    "print(f\"... duration was  {str(time_duration).split('.')[0]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d1a57d-089c-4042-98d8-6893d7a45acf",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h3>12.c - CatFIM Backup old \"public\" FIM 10 / 30 DBs and prepare new databases</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cf108d-7360-48a9-a4a2-81b23b9e51b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This covers ONLY Catfim public FIM 10/30 for both flow based and stage based\n",
    "'''\n",
    "\n",
    "''' DONE for 4.4.0.0.  (4.5.2.11)'''\n",
    "\n",
    "# db_name_appendix = f\"{OLD_FIM_TAG}_fim_10\"\n",
    "\n",
    "# print(\"Starting Data Backups and table drops for stage and flow based PUBLIC catfim\")\n",
    "# # db_names = [\"stage_based_catfim_public\", \"stage_based_catfim_sites_public\",\n",
    "# #              \"flow_based_catfim_public\", \"flow_based_catfim_sites_public\"]\n",
    "\n",
    "# # stage_based_catfim_sites_public didn't exist for fim 10 but should have in TI (does in other enviros likely)\n",
    "# db_names = [\"stage_based_catfim_public\", \n",
    "#              \"flow_based_catfim_public\", \"flow_based_catfim_sites_public\"]\n",
    "\n",
    "# for db_name in db_names:\n",
    "#     new_table_name = f\"reference.{db_name}_{db_name_appendix}\"\n",
    "#     sql = f\"CREATE TABLE IF NOT EXISTS {new_table_name} AS TABLE reference.{db_name}\"\n",
    "#     sf.execute_sql(sql, db_type='egis')\n",
    "#     print(f\"{db_name} copied to {new_table_name} if it does not already exist\")\n",
    "\n",
    "    \n",
    "# # Aug 2024: Now we can drop the tables as we don't have any indexes on them at this time other than the gist geom index.\n",
    "# # By dropping them, we can auto adjust the tables schema. (don't truncate)\n",
    "\n",
    "# for db_name in db_names:\n",
    "#     sf.execute_sql(f\"DROP TABLE IF EXISTS reference.{db_name};\", db_type='egis')\n",
    "#     print(f\"reference.{db_name} table dropped if it existed\")\n",
    "\n",
    "# print(\"Data Backups of flow based catfim are complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb73b36-056f-47b2-90cd-38b26171723a",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h3>12.d - Load CatFIM \"public\" FIM 30 DBs</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd38c00d-22ad-476e-ab05-cf293e5bbc15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"Loading CatFIM Public datasets (FIM 30)\")\n",
    "\n",
    "catfim_types = [\"stage_based_catfim\", \"stage_based_catfim_sites\",\n",
    "                \"flow_based_catfim\", \"flow_based_catfim_sites\"]\n",
    "\n",
    "__public_fim_release = \"fim_30\"  # The new fim public release being loaded (ie. fim_10, fim_30, fim_60..)\n",
    "\n",
    "start_dt = datetime.now()\n",
    "\n",
    "for catfim_type in catfim_types:\n",
    "    print(\"\")\n",
    "    sql = f'''\n",
    "    DROP TABLE IF EXISTS reference.{catfim_type}_public;\n",
    "\n",
    "    SELECT\n",
    "        catfim.*,\n",
    "        '{__public_fim_release}' as public_fim_release\n",
    "    INTO reference.{catfim_type}_public\n",
    "    FROM reference.{catfim_type} as catfim\n",
    "    JOIN reference.public_fim_domain as fim_domain ON ST_Intersects(catfim.geom, fim_domain.geom)\n",
    "    '''\n",
    "    print(sf.execute_sql(sql, db_type='egis'))\n",
    "    print(f\"public {__public_fim_release} data load for {catfim_type} is complete\")\n",
    "\n",
    "# what about indexes again?\n",
    "\n",
    "# for db_name in db_names:\n",
    "#     new_table_name = f\"reference.{db_name}_{db_name_appendix}\"\n",
    "#     sql = f\"CREATE TABLE IF NOT EXISTS {new_table_name} AS TABLE reference.{db_name}\"\n",
    "#     sf.execute_sql(sql, db_type='egis')\n",
    "#     print(f\"{db_name} copied to {new_table_name} if it does not already exist\")\n",
    "\n",
    "print(\"\")\n",
    "end_dt = datetime.now()\n",
    "time_duration = end_dt - start_dt\n",
    "print(f\"... duration was  {str(time_duration).split('.')[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb36b537-f957-457a-9bfa-64d944f85599",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2>13 - Clear the HAND Cache</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d1ab20-4148-4d1d-bbb6-a2c3983ba145",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "TRUNCATE TABLE fim_cache.hand_hydrotable_cached;\n",
    "TRUNCATE TABLE fim_cache.hand_hydrotable_cached_max;\n",
    "TRUNCATE TABLE fim_cache.hand_hydrotable_cached_geo;\n",
    "TRUNCATE TABLE fim_cache.hand_hydrotable_cached_zero_stage;\n",
    "\"\"\"\n",
    "sf.execute_sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166702e8-60d2-485e-a5bc-23b475841e5a",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2>14 - SAVE TO REPO (AND REDEPLOY TO TI WITH NEW VERSION VARIABLE IN TERRAFORM ??)</h2>\n",
    "\n",
    "Oct 21, 2024: We don''t have a system per-say to update for Terraform, but we now have github hooks\n",
    "built right into JupyterHub. We need to figure out how to work with multiple branches and \"getting latest\"\n",
    "but this gives us source control management now.\n",
    "\n",
    "\n",
    "Note from Rob: While, un-elegant, there so much quick evolution here that I recommend we even keep seperate named load scripts in GIT\n",
    "ie) one for FIM Version 4.4.0.0 and one for 4.5.2.11, etc. So many changes for each edition and very fast script changes WIP may \n",
    "make it smarter to keep each script seperately (ie. 4.4.0.0, 4.5.2.11, etc)\n",
    "\n",
    "<h4>Make sure to Publish the changes to git and add a PR</h4>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
