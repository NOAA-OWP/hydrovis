{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68fce701-8ba2-400d-b662-dca6439ef9b0",
   "metadata": {},
   "source": [
    "### Notes - Oct 21, 2024 ###\n",
    "This is a copy from 10.FIM Version 4.5.2.11 which included hand data loads plus ras2fim data. \n",
    "We will remove all ras2fim stuff here knowing that sometimes ras2fim will be uploaded on its own.\n",
    "However.. when ras2fim is loaded, some steps here will need to be re-run. Those steps will be\n",
    "duplicated when we build our next ras2fim load. This hand release does not have a ras2fim update so\n",
    "we will keep the one in place.\n",
    "</br></br>\n",
    "All code in here will be reviewed and adjusted as the loads progress. Consider each step to be\n",
    "WIP until you see a load date below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4c0734-01e8-4f37-a718-402d9be7a076",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load Status for hand 4.5.11.1 - Started Oct 31, 2024 (well... restarted from the 21st)\n",
    "\n",
    "#### Add dates to each line as they have been loaded\n",
    "\n",
    "1. `Crosswalk` :  ---  Done: Nov 5\n",
    "2. `Lambda FIM_PREFIX` :   ---  Done: Nov 5\n",
    "3. `Lambda FIM_VERSION and Memory` :   ---  Done: Nov 5\n",
    "4. `ras2fim` :  No update in this release. But a few adjustments for fim_version and model_version here :  -- Done: Nov 5\n",
    "5. `AEP`   --- (HOLD) - needs lambda and image updates (via tf)\n",
    "    - `2 year` :  -- \n",
    "    - `5 year` :  -- \n",
    "    - `10 year` :  -- \n",
    "    - `25 year` :  -- \n",
    "    - `50 year` :  -- \n",
    "    - `HW / High Water` :  -- \n",
    "    - `Change the hv-vpp-ti-viz-fim-data-prep Lambda memory back to 2048mb` :  -- \n",
    "6. `Catchments`  --- (HOLD) - needs lambda and image updates (via tf)\n",
    "    - `Branch 0` :  -- \n",
    "    - `GMS` :  -- \n",
    "7. `usgs_elev_table` :  --  Done Nov 7\n",
    "8. `hydrotable / hydrotable_staggered` : -- \n",
    "9. `usgs_rating_curve / usgs_rating_curves staggered` : -- \n",
    "10. `Skills Metrics` :  -- \n",
    "11. `FIM Performance` :  -- \n",
    "12. `CatFIM`\n",
    "    - `Stage Based CatFIM` :  -- \n",
    "    - `Flow Based CatFIM` :   -- \n",
    "    - `CatFIM FIM 30` : Stage based only? flow not needed but confirm this.\n",
    "13. `Clear HAND cache` :\n",
    "14. `GIT` and `Terraform ??` : We can now do GitHub check in from here. Watch for branches.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e1ee2b5-b109-49e8-8255-f06449b44ee6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.0.1\n",
      "All loaded\n"
     ]
    }
   ],
   "source": [
    "# Cell to manually pip reload a packages that the Jupyter engine not retained\n",
    "# !pip install numpy\n",
    "# !pip install geopandas\n",
    "# !pip install pyarrow\n",
    "# !pip install xarray\n",
    "# !pip install geoalchemy2\n",
    "# !pip install contextily\n",
    "# !pip install rioxarray\n",
    "\n",
    "!pip install python-dotenv\n",
    "print(\"All loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103a0d12-b2a0-4586-ba0f-0ed6b2c4eb89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pd.set_option(\"max_info_rows\", 100000) # override  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b656259",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imports loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import codecs\n",
    "import csv\n",
    "\n",
    "import sys\n",
    "\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import boto3\n",
    "import geopandas as gpd\n",
    "import json\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "import sqlalchemy\n",
    "import xarray as xr\n",
    "\n",
    "from geopandas import GeoDataFrame\n",
    "from io import StringIO\n",
    "from geoalchemy2 import Geometry\n",
    "from shapely import wkt\n",
    "from shapely.geometry import Polygon\n",
    "from sqlalchemy.exc import DataError   # yes, reduntant, fix it later\n",
    "from sqlalchemy.types import Text    # yes, reduntant, fix it later\n",
    "\n",
    "\n",
    "sys.path.append(os.path.join(os.path.abspath(''), '..'))\n",
    "\n",
    "import helper_functions.shared_functions as sf\n",
    "import helper_functions.s3_shared_functions as s3_sf\n",
    "\n",
    "from helper_functions.viz_classes import database\n",
    "\n",
    "print(\"imports loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae5ca0f-e582-4ff2-89b6-087285433a97",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load AWS Keys\n",
    "from dotenv import load_dotenv\n",
    "aws_keys_path = os.path.join(Path.home(),\"SageMaker\", \"AWS_keys.env\")\n",
    "print(f\"aws_keys are at {aws_keys_path}\")\n",
    "load_dotenv(aws_keys_path)\n",
    "\n",
    "TI_ACCESS_KEY = os.environ['WF_TI_ACCESS_KEY']\n",
    "TI_SECRET_KEY = os.environ['WF_TI_SECRET_KEY']\n",
    "TI_TOKEN = os.environ['WF_TI_TOKEN']\n",
    "\n",
    "# I updated the file but it is not being honored in the enviro values\n",
    "\n",
    "# print(TI_ACCESS_KEY)\n",
    "# print(TI_SECRET_KEY)\n",
    "# print(TI_TOKEN)\n",
    "\n",
    "print(\"aws_keys loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e944eff3-6023-48f4-9f57-27304a240447",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Variables loaded\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# we won't load this into any tables at this time\n",
    "# The phrase of FIM 5.1.0 will be embedded in config files\n",
    "#PU0LIC_FIM_VERSION = \"FIM 5.1.0\"\n",
    "HAND_MODEL_VERSION = \"4.5.11.1\"\n",
    "RAS2FIM_MODEL_VERSION = \"2.0\"\n",
    "\n",
    "HAND_ROOT_DPATH = \"fim/hand_4_5_11_1\"\n",
    "HAND_DATASETS_DPATH = f\"{FIM_ROOT_DPATH}/hand_datasets\"\n",
    "QA_DATASETS_DPATH = f\"{FIM_ROOT_DPATH}/qa_datasets\"\n",
    "\n",
    "FIM_BUCKET = \"hydrovis-ti-deployment-us-east-1\"\n",
    "FIM_CROSSWALK_FPATH = os.path.join(HAND_DATASETS_DPATH, \"crosswalk_table.csv\")\n",
    "PIPELINE_ARN = 'arn:aws:states:us-east-1:526904826677:stateMachine:hv-vpp-ti-viz-pipeline'\n",
    "\n",
    "COLUMN_NAME_MODEL_VERSION = \"model_version\"\n",
    "\n",
    "# Sometimes these credential values get updated. To find the latest correct values, go to your AWS Console log page and click on the \"Access Key\"\n",
    "# link to get the latest valid set. Using the \"AWS environment variables\" values.\n",
    "# If this is not set correctly, you will get an HTTP error 400 when you call S3 lower.\n",
    "# You might also see an error of 'An error occurred (NoSuchKey) when calling the GetObject operation:\n",
    "# The specified key does not exist.\" the creds are not correct\"\n",
    "\n",
    "S3_CLIENT = boto3.client(\"s3\")\n",
    "STEPFUNCTION_CLIENT = boto3.client('stepfunctions')\n",
    "VIZ_DB_ENGINE = sf.get_db_engine('viz')\n",
    "\n",
    "print(\"Global Variables loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5827eb-3ca3-4b29-8ee7-f7c4f3c2c736",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2>1 - UPLOAD FIM4 HYDRO ID/FEATURE ID CROSSWALK</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96a49f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "print(f\"Getting column name from {FIM_CROSSWALK_FPATH}\")\n",
    "\n",
    "data = S3_CLIENT.get_object(Bucket=FIM_BUCKET, Key=FIM_CROSSWALK_FPATH)\n",
    "d_reader = csv.DictReader(codecs.getreader(\"utf-8\")(data[\"Body\"]))\n",
    "headers = d_reader.fieldnames\n",
    "\n",
    "\n",
    "header_str = \"(\"\n",
    "for header in headers:\n",
    "    header_str += header\n",
    "    if header in ['hand_id', 'hydro_id', 'lake_id']:\n",
    "        header_str += ' integer,'\n",
    "    elif header in ['branch_id', 'feature_id']:\n",
    "        header_str += ' bigint,'\n",
    "    else:\n",
    "        header_str += ' TEXT,'\n",
    "header_str = header_str[:-1] + \")\"\n",
    "print(header_str)\n",
    "\n",
    "db = database(db_type=\"viz\")\n",
    "with db.get_db_connection() as conn, conn.cursor() as cur:\n",
    "    \n",
    "    print(f\"Deleting/Creating derived.fim4_featureid_crosswalk using columns {header_str}\")\n",
    "    sql = f\"DROP TABLE IF EXISTS derived.fim4_featureid_crosswalk; CREATE TABLE derived.fim4_featureid_crosswalk {header_str};\"\n",
    "    cur.execute(sql)\n",
    "    conn.commit()\n",
    "    \n",
    "    # TODO: Nov: Drop the other 2 tables? No. ignore featureid_huc_crosswalk and featureid_huc_crosswalk_ak (not ours)\n",
    "    \n",
    "\n",
    "    print(f\"Importing {FIM_CROSSWALK_FPATH} to derived.fim4_featureid_crosswalk\")\n",
    "    sql = f\"\"\"\n",
    "        SELECT aws_s3.table_import_from_s3(\n",
    "           'derived.fim4_featureid_crosswalk',\n",
    "           '', \n",
    "           '(format csv, HEADER true)',\n",
    "           (SELECT aws_commons.create_s3_uri(\n",
    "               '{FIM_BUCKET}',\n",
    "               '{FIM_CROSSWALK_FPATH}',\n",
    "               'us-east-1'\n",
    "                ) AS s3_uri\n",
    "            ),\n",
    "            aws_commons.create_aws_credentials('{TI_ACCESS_KEY}', '{TI_SECRET_KEY}', '{TI_TOKEN}')\n",
    "           );\n",
    "        \"\"\"\n",
    "    cur.execute(sql)\n",
    "    conn.commit()\n",
    "   \n",
    "    \n",
    "    print(f\"Adding {COLUMN_NAME_FIM_VERSION} column to derived.fim4_featureid_crosswalk\")\n",
    "    sql = f\"ALTER TABLE derived.fim4_featureid_crosswalk ADD COLUMN IF NOT EXISTS {COLUMN_NAME_FIM_VERSION} text DEFAULT '{PUBLIC_FIM_VERSION}';\"\n",
    "    cur.execute(sql)\n",
    "    conn.commit()\n",
    "    \n",
    "    print(f\"Adding {COLUMN_NAME_MODEL_VERSION} column to derived.fim4_featureid_crosswalk\")\n",
    "    sql = f\"ALTER TABLE derived.fim4_featureid_crosswalk ADD COLUMN IF NOT EXISTS {COLUMN_NAME_MODEL_VERSION} text DEFAULT '{FIM_MODEL_VERSION}';\"\n",
    "    cur.execute(sql)\n",
    "    conn.commit()\n",
    "    \n",
    "    print(\"Adding feature id index to derived.fim4_featureid_crosswalk\")\n",
    "    # Drop it already exists\n",
    "    sql = \"DROP INDEX IF EXISTS derived.fim4_crosswalk_feature_id\"\n",
    "    cur.execute(sql)\n",
    "    conn.commit()    \n",
    "    sql = \"CREATE INDEX fim4_crosswalk_feature_id ON derived.fim4_featureid_crosswalk USING btree (feature_id)\"\n",
    "    cur.execute(sql)\n",
    "    conn.commit()\n",
    "\n",
    "    print(\"Adding hydro id index to derived.fim4_featureid_crosswalk\")\n",
    "    # Drop it already exists\n",
    "    sql = \"DROP INDEX IF EXISTS derived.fim4_crosswalk_hydro_id\"\n",
    "    cur.execute(sql)\n",
    "    conn.commit()    \n",
    "    sql = \"CREATE INDEX fim4_crosswalk_hydro_id ON derived.fim4_featureid_crosswalk USING btree (hydro_id)\"\n",
    "    cur.execute(sql)\n",
    "    conn.commit()\n",
    "\n",
    "print(\"\")\n",
    "print(\"Successully loaded derived.fim4_featureid_crosswalk and updated it\")\n",
    "print(\"... Estimated time to completion is just a few mins\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17fdbfc-6f88-483d-a76d-29ebd525c5e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2>2 - UPDATE FIM HAND PROCESSING LAMBDA ENV VARIABLE WITH NEW FIM PREFIX</h2>\n",
    "\n",
    "https://us-east-1.console.aws.amazon.com/lambda/home?region=us-east-1#/functions/hv-vpp-ti-viz-hand-fim-processing?tab=configure\n",
    "\n",
    "Lambda name: hv-vpp-ti-viz-hand-fim-processing\n",
    "\n",
    "In the Configuration Tab, click on the `Environment variables` (left menu), then change the `FIX_PREFIX` to location of the latest hand_dataset you are working on. Referencial to S3 Bucket name.\n",
    "<br>\n",
    "ie) fim/fim_4_5_11_1/hand_datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13507d85-ff04-42f3-bf72-b896d63d4679",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2>3 - UPDATE FIM DATA PREP LAMBDA ENV VARIABLE WITH NEW FIM VERSION AND MEMORY</h2>\n",
    "\n",
    "https://us-east-1.console.aws.amazon.com/lambda/home?region=us-east-1#/functions/hv-vpp-ti-viz-fim-data-prep?tab=code\n",
    "\n",
    "Lambda name: hv-vpp-ti-viz-hand-fim-processing\n",
    "\n",
    "In the `Configuration` Tab, click on the `Environment variables` (left menu), then change the `FIM_VERSION` to the latest fim model version. \n",
    "<br>\n",
    "ie) 4.5.11.1\n",
    "<br><br>\n",
    "<b>Then:</b> Still in the Configuration Tab, now click on the `General Configuration` (left menu), followed \n",
    "by the `edit` button on the far right side, to get into the `General Configuration` page details.\n",
    "<br>Change (if they are not already there)\n",
    "<br>Memory (text field) to 4096 (MB)  and\n",
    "<br>Emphermeral Storage tp 1024 (MB)\n",
    "<br>\n",
    "\n",
    "#### Note: Later in these steps we will change the Memory and Emphermal Storage back to default values, see below ####\n",
    "Nov 5, 2024: Added new variable called \"MODEL_VERSION\" : \"HAND 4.5.11.1\".  FIM_VERSION IS NOW: 5.1.0\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f19f7ac-61d1-4323-8138-6823da658aa3",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2>4 - UPDATE RAS2FIM DATA (inc ras2fim boundaries) IN DB</h2>\n",
    "\n",
    "As of Oct 2024, we have a new fim (hand) release covered in this file, but ras2fim does not have a new\n",
    "release. ras2fim will likely be loaded as new datasets become available. \n",
    "\n",
    "***The code for ras2fim is removed here from the 4.5.2.11 set and will rebuilt as it''s own new separate load script when that happens.***\n",
    "\n",
    "However, we will have a few modifications for ras2fim data (not a reload) to help bring in the new\n",
    "fim_version and model_version columns. Those changes are included here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfc0a44c-8b46-4db5-a9a0-c65e0bf1e20b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated geocurves table to new FIM version value\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Update \"geocurves\" to update the \"fim_version\" field to \"FIM 5.1.0:\n",
    "\n",
    "print(f\"Updating geocurves table to fim_version of {PUBLIC_FIM_VERSION}\"\n",
    "\n",
    "sf.execute_sql(f'''\n",
    "UPDATE\n",
    "    ras2fim.geocurves\n",
    "SET\n",
    "    fim_version = '{PUBLIC_FIM_VERSION}';\n",
    "''', db_type=\"viz\")\n",
    "\n",
    "print(\"Updating done for geocurves\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5994fa-8fa5-471d-86e9-745d67e79ecf",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2>5 - Run AEP FIM Pipelines.</h2>\n",
    "Updated Documentation from Tyler Early 2024: This can be done in a couple of diferent ways.\n",
    "\n",
    "1) One option is to use the pipeline_input code created below by Corey to start the AEP pipelines directly from this notebook.<br>\n",
    "   However, those pipeline_input dictionaries may very well be be out of date, pending more recent updates to the pipelines.<br?\n",
    "\n",
    "\n",
    "2) The other option, which I prefer, is to setup a manual test event in the initialize_pipeline lambda function to trigger an AEP pipeline like this:</b>\n",
    "{\n",
    "  \"configuration\": \"reference\",\n",
    "  \"products_to_run\": \"static_nwm_aep_inundation_extent_library\",\n",
    "  \"invoke_step_function\": false\n",
    "}\n",
    "\n",
    "Using this test event will produce the pipeline instructions, printing any errors that come up, and you can simply change the invoke_step_function flag to True when you're ready to actually invoke a pipeline run (which you can monitor/manage in the step function gui). You will need to manually update the static_nwm_aep_inundation_extent_library.yml product config file to only run 1 aep configuration at a time, and work through the configs as the pipelines finish (takes about an hour each). I've also found that the fim_data_prep lambda function needs to be temporarilly increased to ~4,500mb of memory to run these pipelines. It's also worth noting that these are very resource intesive pipelines, as FIM is calculated for every reach in the nation. AWS costs can amount to hundreds or even thousands of dollars by running these pipelines, so use responsibly.\n",
    "\n",
    "A couple other important notes:\n",
    "- These AEP configurations write data directly to the aep_fim schema in the egis RDS database, instead of the viz database.\n",
    "- <b>You'll need to dump the aep_fim schema after that is complete for backup / deployment into other environments.</b>\n",
    "- This process has not been tested with new NWM 3.0 Recurrence Flows, and a good thorough audit / QC check of output data is warranted, given those changes and the recent updates to the pipelines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a698067",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Aug 6, 2024: Note: This was created after all intervals were created, so only HW was tested against\n",
    "\n",
    "def get_aep_pipeline_input(stage_interval):\n",
    "    pipeline_input = {\n",
    "      \"configuration\": \"reference\",\n",
    "      \"job_type\": \"auto\",\n",
    "      \"data_type\": \"channel\",\n",
    "      \"keep_raw\": False,\n",
    "      \"reference_time\": datetime.now().strftime('%Y-%m-%d 00:00:00'),\n",
    "      \"configuration_data_flow\": {\n",
    "        \"db_max_flows\": [],\n",
    "        \"db_ingest_groups\": [],\n",
    "        \"python_preprocessing\": []\n",
    "      },\n",
    "      \"pipeline_products\": [\n",
    "        {\n",
    "          \"product\": \"static_nwm_aep_inundation_extent_library\",\n",
    "          \"configuration\": \"reference\",\n",
    "          \"product_type\": \"fim\",\n",
    "          \"run\": True,\n",
    "          \"fim_configs\": [\n",
    "            {\n",
    "              \"name\": f\"rf_{stage_interval}_inundation\",\n",
    "              \"target_table\": f\"aep_fim.rf_{stage_interval}_inundation\",\n",
    "              \"fim_type\": \"hand\",\n",
    "              \"sql_file\": f\"rf_{stage_interval}_inundation\"\n",
    "            }\n",
    "          ],\n",
    "          \"services\": [\n",
    "            \"static_nwm_aep_inundation_extent_library_noaa\"\n",
    "          ],\n",
    "          \"raster_outputs\": {\n",
    "            \"output_bucket\": \"\",\n",
    "            \"output_raster_workspaces\": []\n",
    "          },\n",
    "          \"postprocess_sql\": [],\n",
    "          \"product_summaries\": [],\n",
    "          \"python_preprocesing_dependent\": False\n",
    "        }\n",
    "      ],\n",
    "      \"sql_rename_dict\": {},\n",
    "      \"logging_info\": {\n",
    "          \"Timestamp\": int(datetime.now().timestamp())\n",
    "      }\n",
    "    }\n",
    "\n",
    "    return pipeline_input\n",
    "\n",
    "print(\"function: get_aep_pipeline_input loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d6ee69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#### 2 Year Flow\n",
    "pipeline_input = get_aep_pipeline_input(\"2\")\n",
    "\n",
    "# notice, slightly different object name\n",
    "pipeline_name = f\"sagemaker_aep_2_{datetime.now().strftime('%Y%m%dT%H%M')}\"\n",
    "\n",
    "STEPFUNCTION_CLIENT.start_execution(\n",
    "    stateMachineArn = PIPELINE_ARN,\n",
    "    name = pipeline_name,\n",
    "    input= json.dumps(pipeline_input)\n",
    ")\n",
    "\n",
    "print(f\"AEP : 2 year flows ie: rf_2_inundation kicked off. Can take 45 mins. Pipeline : hv-vpp-ti-viz-pipeline - {pipeline_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f89d9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#### 5 Year Flow\n",
    "pipeline_input = get_aep_pipeline_input(\"5\")\n",
    "\n",
    "# notice, slightly different object name\n",
    "pipeline_name = f\"sagemaker_aep_5_{datetime.now().strftime('%Y%m%dT%H%M')}\"\n",
    "\n",
    "STEPFUNCTION_CLIENT.start_execution(\n",
    "    stateMachineArn = PIPELINE_ARN,\n",
    "    name = pipeline_name,\n",
    "    input= json.dumps(pipeline_input)\n",
    ")\n",
    "\n",
    "print(f\"AEP : 5 year flows ie: rf_5_inundation kicked off. Can take 45 mins. Pipeline : hv-vpp-ti-viz-pipeline  - {pipeline_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791d1a8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#### 10 Year Flow\n",
    "pipeline_input = get_aep_pipeline_input(\"10\")\n",
    "\n",
    "# notice, slightly different object name\n",
    "pipeline_name = f\"sagemaker_aep_10_{datetime.now().strftime('%Y%m%dT%H%M')}\"\n",
    "\n",
    "STEPFUNCTION_CLIENT.start_execution(\n",
    "    stateMachineArn = PIPELINE_ARN,\n",
    "    name = pipeline_name,\n",
    "    input= json.dumps(pipeline_input)\n",
    ")\n",
    "\n",
    "print(f\"AEP : 10 year flows ie: rf_10_inundation kicked off. Can take 45 mins. Pipeline : hv-vpp-ti-viz-pipeline  - {pipeline_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb87128",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#### 25 Year Flow\n",
    "pipeline_input = get_aep_pipeline_input(\"25\")\n",
    "\n",
    "# notice, slightly different object name\n",
    "pipeline_name = f\"sagemaker_aep_25_{datetime.now().strftime('%Y%m%dT%H%M')}\"\n",
    "\n",
    "STEPFUNCTION_CLIENT.start_execution(\n",
    "    stateMachineArn = PIPELINE_ARN,\n",
    "    name = pipeline_name,\n",
    "    input= json.dumps(pipeline_input)\n",
    ")\n",
    "\n",
    "print(f\"AEP : 25 year flows ie: rf_25_inundation kicked off. Can take 45 mins. Pipeline : hv-vpp-ti-viz-pipeline  - {pipeline_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4832e4e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#### 50 Year Flow\n",
    "pipeline_input = get_aep_pipeline_input(\"50\")\n",
    "\n",
    "# notice, slightly different object name\n",
    "pipeline_name = f\"sagemaker_aep_50_{datetime.now().strftime('%Y%m%dT%H%M')}\"\n",
    "\n",
    "STEPFUNCTION_CLIENT.start_execution(\n",
    "    stateMachineArn = PIPELINE_ARN,\n",
    "    name = pipeline_name,\n",
    "    input= json.dumps(pipeline_input)\n",
    ")\n",
    "\n",
    "print(f\"AEP : 50 year flows ie: rf_50_inundation kicked off. Can take 45 mins. Pipeline : hv-vpp-ti-viz-pipeline  - {pipeline_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187e83bc-ebbe-4615-a046-e0ef7b09ad3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#### HW (High Water) Flow\n",
    "pipeline_input = get_aep_pipeline_input(\"high_water\")\n",
    "\n",
    "# notice, slightly different object name\n",
    "pipeline_name = f\"sagemaker_aep_hw_{datetime.now().strftime('%Y%m%dT%H%M')}\"\n",
    "\n",
    "STEPFUNCTION_CLIENT.start_execution(\n",
    "     stateMachineArn = PIPELINE_ARN,\n",
    "     name = pipeline_name,\n",
    "     input= json.dumps(pipeline_input)\n",
    ")\n",
    "\n",
    "print(f\"AEP : High Water year flows ie: rf_hw_inundation kicked off. Can take 45 mins. Pipeline : hv-vpp-ti-viz-pipeline  - {pipeline_name}\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c810767-2f5d-46b2-860c-5d5c549f2e2a",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h3>IMPORTANT: Return hv-vpp-ti-viz-fim-data-prep Lambda memory to 2048mb</h3>\n",
    "\n",
    "https://us-east-1.console.aws.amazon.com/lambda/home?region=us-east-1#/functions/hv-vpp-ti-viz-fim-data-prep?tab=code\n",
    "\n",
    "Lambda name: hv-vpp-ti-viz-hand-fim-processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fa6c78-efc2-4dbe-ae9a-55b3c4842492",
   "metadata": {},
   "source": [
    "<h2>6 - RUN CATCHMENT WORKFLOWS 2 CONFIGS AT A TIME. CHECK FOR STEP FUNCTION FINISHING BEFORE STARTING NEW ONE</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4574e4b-1569-4362-8d3f-42f4319752a1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 6a - Branch 0 Catchments. Wait until it is done before kicking off the next GMS (Level Path) catchments load a bit lower. ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77d6e5b-f5fb-422f-a6c3-0f53743cd631",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Add backups to these (4.4.0.0)  (already not available for 4.4.0.0)\n",
    "\n",
    "\n",
    "# TODO: We likely need to keep the schema, so trun is fine for now, but eventually, get a lsit of the indexes and re-build \n",
    "# indexes each time as/if needed. Granted these tables are loaded via Lambdas, so I am not sure how indexes will play into that\n",
    "\n",
    "sf.execute_sql('''\n",
    "TRUNCATE \n",
    "    fim_catchments.branch_0_catchments, \n",
    "    fim_catchments.branch_0_catchments_hi, \n",
    "    fim_catchments.branch_0_catchments_prvi;\n",
    "''', db_type=\"egis\")\n",
    "\n",
    "print(\"Catchment Truncation for Branch 0 Done\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7495759",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline_input = {\n",
    "  \"configuration\": \"reference\",\n",
    "  \"job_type\": \"auto\",\n",
    "  \"data_type\": \"channel\",\n",
    "  \"keep_raw\": False,\n",
    "  \"reference_time\": datetime.now().strftime('%Y-%m-%d 00:00:00'),\n",
    "  \"configuration_data_flow\": {\n",
    "    \"db_max_flows\": [],\n",
    "    \"db_ingest_groups\": [],\n",
    "    \"python_preprocessing\": []\n",
    "  },\n",
    "  \"pipeline_products\": [\n",
    "    {\n",
    "      \"product\": \"static_hand_catchments_0_branches\",\n",
    "      \"configuration\": \"reference\",\n",
    "      \"product_type\": \"fim\",\n",
    "      \"run\": True,\n",
    "      \"fim_configs\": [\n",
    "        {\n",
    "          \"name\": \"catchments_0_branches\",\n",
    "          \"target_table\": \"fim_catchments.branch_0_catchments\",\n",
    "          \"fim_type\": \"hand\",\n",
    "          \"sql_file\": \"catchments_0_branches\"\n",
    "        }\n",
    "      ],\n",
    "      \"services\": [\n",
    "        \"static_hand_catchments_0_branches_noaa\"\n",
    "      ],\n",
    "      \"raster_outputs\": {\n",
    "        \"output_bucket\": \"\",\n",
    "        \"output_raster_workspaces\": []\n",
    "      },\n",
    "      \"postprocess_sql\": [],\n",
    "      \"product_summaries\": [],\n",
    "      \"python_preprocesing_dependent\": False\n",
    "    },\n",
    "    {\n",
    "      \"product\": \"static_hand_catchments_0_branches_hi\",\n",
    "      \"configuration\": \"reference\",\n",
    "      \"product_type\": \"fim\",\n",
    "      \"run\": True,\n",
    "      \"fim_configs\": [\n",
    "        {\n",
    "          \"name\": \"catchments_0_branches_hi\",\n",
    "          \"target_table\": \"fim_catchments.branch_0_catchments_hi\",\n",
    "          \"fim_type\": \"hand\",\n",
    "          \"sql_file\": \"catchments_0_branches_hi\"\n",
    "        }\n",
    "      ],\n",
    "      \"services\": [\n",
    "        \"static_hand_catchments_0_branches_hi_noaa\"\n",
    "      ],\n",
    "      \"raster_outputs\": {\n",
    "        \"output_bucket\": \"\",\n",
    "        \"output_raster_workspaces\": []\n",
    "      },\n",
    "      \"postprocess_sql\": [],\n",
    "      \"product_summaries\": [],\n",
    "      \"python_preprocesing_dependent\": False\n",
    "    },\n",
    "    {\n",
    "      \"product\": \"static_hand_catchments_0_branches_prvi\",\n",
    "      \"configuration\": \"reference\",\n",
    "      \"product_type\": \"fim\",\n",
    "      \"run\": True,\n",
    "      \"fim_configs\": [\n",
    "        {\n",
    "          \"name\": \"catchments_0_branches_prvi\",\n",
    "          \"target_table\": \"fim_catchments.branch_0_catchments_prvi\",\n",
    "          \"fim_type\": \"hand\",\n",
    "          \"sql_file\": \"catchments_0_branches_prvi\"\n",
    "        }\n",
    "      ],\n",
    "      \"services\": [\n",
    "        \"static_hand_catchments_0_branches_prvi_noaa\"\n",
    "      ],\n",
    "      \"raster_outputs\": {\n",
    "        \"output_bucket\": \"\",\n",
    "        \"output_raster_workspaces\": []\n",
    "      },\n",
    "      \"postprocess_sql\": [],\n",
    "      \"product_summaries\": [],\n",
    "      \"python_preprocesing_dependent\": False\n",
    "    }\n",
    "  ],\n",
    "  \"sql_rename_dict\": {},\n",
    "  \"logging_info\": {\n",
    "      \"Timestamp\": int(datetime.now().timestamp())\n",
    "  }\n",
    "}\n",
    "\n",
    "pipeline_name = f\"sagemaker_0_catchments_{datetime.now().strftime('%Y%m%dT%H%M')}\"\n",
    "\n",
    "# TODO: For later... fix fim_version value and add model_version column. current fim_version vlaue is showing 4.5.2.11\n",
    "\n",
    "STEPFUNCTION_CLIENT.start_execution(\n",
    "    stateMachineArn = PIPELINE_ARN,\n",
    "    name = pipeline_name,\n",
    "    input= json.dumps(pipeline_input)\n",
    ")\n",
    "\n",
    "print(\"Catchments Branch 0 load kicked off. Last runtime: 23:38.019. \"\n",
    "      f\"Pipeline : hv-vpp-ti-viz-pipeline  - {pipeline_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c75c4b7-bf13-4e8c-a33e-261219ce4338",
   "metadata": {},
   "source": [
    "### 6b - GMS (Level Paths / non branch 0) catchments ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea2a5be-9936-4abe-832a-8582f3c3af64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Add backups to these\n",
    "\n",
    "sf.execute_sql('''\n",
    "TRUNCATE\n",
    "    fim_catchments.branch_gms_catchments,\n",
    "    fim_catchments.branch_gms_catchments_hi,\n",
    "    fim_catchments.branch_gms_catchments_prvi;\n",
    "''', db_type=\"egis\")\n",
    "\n",
    "print(\"Catchment Truncation for GMS (Level Path) Branchs Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae8917f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline_input = {\n",
    "  \"configuration\": \"reference\",\n",
    "  \"job_type\": \"auto\",\n",
    "  \"data_type\": \"channel\",\n",
    "  \"keep_raw\": False,\n",
    "  \"reference_time\": datetime.now().strftime('%Y-%m-%d 00:00:00'),\n",
    "  \"configuration_data_flow\": {\n",
    "    \"db_max_flows\": [],\n",
    "    \"db_ingest_groups\": [],\n",
    "    \"python_preprocessing\": []\n",
    "  },\n",
    "  \"pipeline_products\": [\n",
    "    {\n",
    "      \"product\": \"static_hand_catchments_gms_branches\",\n",
    "      \"configuration\": \"reference\",\n",
    "      \"product_type\": \"fim\",\n",
    "      \"run\": True,\n",
    "      \"fim_configs\": [\n",
    "        {\n",
    "          \"name\": \"catchments_gms_branches\",\n",
    "          \"target_table\": \"fim_catchments.branch_gms_catchments\",\n",
    "          \"fim_type\": \"hand\",\n",
    "          \"sql_file\": \"catchments_gms_branches\"\n",
    "        }\n",
    "      ],\n",
    "      \"services\": [\n",
    "        \"static_hand_catchments_gms_branches_noaa\"\n",
    "      ],\n",
    "      \"raster_outputs\": {\n",
    "        \"output_bucket\": \"\",\n",
    "        \"output_raster_workspaces\": []\n",
    "      },\n",
    "      \"postprocess_sql\": [],\n",
    "      \"product_summaries\": [],\n",
    "      \"python_preprocesing_dependent\": False\n",
    "    },\n",
    "    {\n",
    "      \"product\": \"static_hand_catchments_gms_branches_hi\",\n",
    "      \"configuration\": \"reference\",\n",
    "      \"product_type\": \"fim\",\n",
    "      \"run\": True,\n",
    "      \"fim_configs\": [\n",
    "        {\n",
    "          \"name\": \"catchments_gms_branches_hi\",\n",
    "          \"target_table\": \"fim_catchments.branch_gms_catchments_hi\",\n",
    "          \"fim_type\": \"hand\",\n",
    "          \"sql_file\": \"catchments_gms_branches_hi\"\n",
    "        }\n",
    "      ],\n",
    "      \"services\": [\n",
    "        \"static_hand_catchments_gms_branches_hi_noaa\"\n",
    "      ],\n",
    "      \"raster_outputs\": {\n",
    "        \"output_bucket\": \"\",\n",
    "        \"output_raster_workspaces\": []\n",
    "      },\n",
    "      \"postprocess_sql\": [],\n",
    "      \"product_summaries\": [],\n",
    "      \"python_preprocesing_dependent\": False\n",
    "    },\n",
    "    {\n",
    "      \"product\": \"static_hand_catchments_gms_branches_prvi\",\n",
    "      \"configuration\": \"reference\",\n",
    "      \"product_type\": \"fim\",\n",
    "      \"run\": True,\n",
    "      \"fim_configs\": [\n",
    "        {\n",
    "          \"name\": \"catchments_gms_branches_prvi\",\n",
    "          \"target_table\": \"fim_catchments.branch_gms_catchments_prvi\",\n",
    "          \"fim_type\": \"hand\",\n",
    "          \"sql_file\": \"catchments_gms_branches_prvi\"\n",
    "        }\n",
    "      ],\n",
    "      \"services\": [\n",
    "        \"static_hand_catchments_gms_branches_prvi_noaa\"\n",
    "      ],\n",
    "      \"raster_outputs\": {\n",
    "        \"output_bucket\": \"\",\n",
    "        \"output_raster_workspaces\": []\n",
    "      },\n",
    "      \"postprocess_sql\": [],\n",
    "      \"product_summaries\": [],\n",
    "      \"python_preprocesing_dependent\": False\n",
    "    }\n",
    "  ],\n",
    "  \"sql_rename_dict\": {},\n",
    "  \"logging_info\": {\n",
    "      \"Timestamp\": int(datetime.now().timestamp())\n",
    "  }\n",
    "}\n",
    "\n",
    "pipeline_name = f\"sagemaker_gms_catchments_{datetime.now().strftime('%Y%m%dT%H%M')}\"\n",
    "\n",
    "# TODO: For later... fix fim_version value and add model_version column. current fim_version vlaue is showing 4.5.2.11\n",
    "\n",
    "STEPFUNCTION_CLIENT.start_execution(\n",
    "    stateMachineArn = PIPELINE_ARN,\n",
    "    name = pipeline_name,\n",
    "    input= json.dumps(pipeline_input)\n",
    ")\n",
    "\n",
    "print(\"Catchments GMS Branches (Level Paths / non branch 0) load kicked off.\"\n",
    "      f\" Last runtime: 24:45.150. Pipeline : hv-vpp-ti-viz-pipeline  - {pipeline_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fb7850-5be1-4127-86ba-bbfde273424a",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2>7 - Recreate derived.usgs_elev_table</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2dd0c144-5a20-4899-b946-a6dd9ae49b53",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1 of 1000 on page 1\n",
      "Processing 2 of 1000 on page 1\n",
      "Processing 3 of 1000 on page 1\n",
      "Processing 4 of 1000 on page 1\n",
      "Processing 5 of 1000 on page 1\n",
      "Processing 6 of 1000 on page 1\n",
      "Processing 7 of 1000 on page 1\n",
      "Processing 8 of 1000 on page 1\n",
      "Processing 9 of 1000 on page 1\n",
      "Processing 10 of 1000 on page 1\n",
      "Processing 11 of 1000 on page 1\n",
      "Processing 12 of 1000 on page 1\n",
      "Processing 13 of 1000 on page 1\n",
      "Processing 14 of 1000 on page 1\n",
      "Processing 15 of 1000 on page 1\n",
      "Processing 16 of 1000 on page 1\n",
      "Processing 17 of 1000 on page 1\n",
      "Processing 18 of 1000 on page 1\n",
      "Processing 19 of 1000 on page 1\n",
      "Processing 20 of 1000 on page 1\n",
      "Processing 21 of 1000 on page 1\n",
      "Processing 22 of 1000 on page 1\n",
      "Processing 23 of 1000 on page 1\n",
      "Processing 24 of 1000 on page 1\n",
      "Processing 25 of 1000 on page 1\n",
      "Processing 26 of 1000 on page 1\n",
      "Processing 27 of 1000 on page 1\n",
      "Processing 28 of 1000 on page 1\n",
      "Processing 29 of 1000 on page 1\n",
      "Processing 30 of 1000 on page 1\n",
      "Processing 31 of 1000 on page 1\n",
      "Processing 32 of 1000 on page 1\n",
      "Processing 33 of 1000 on page 1\n",
      "Processing 34 of 1000 on page 1\n",
      "Processing 35 of 1000 on page 1\n",
      "Processing 36 of 1000 on page 1\n",
      "Processing 37 of 1000 on page 1\n",
      "Processing 38 of 1000 on page 1\n",
      "Processing 39 of 1000 on page 1\n",
      "Processing 40 of 1000 on page 1\n",
      "Processing 41 of 1000 on page 1\n",
      "Processing 42 of 1000 on page 1\n",
      "Processing 43 of 1000 on page 1\n",
      "Processing 44 of 1000 on page 1\n",
      "Processing 45 of 1000 on page 1\n",
      "Processing 46 of 1000 on page 1\n",
      "Processing 47 of 1000 on page 1\n",
      "Processing 48 of 1000 on page 1\n",
      "Processing 49 of 1000 on page 1\n",
      "Processing 50 of 1000 on page 1\n",
      "Processing 51 of 1000 on page 1\n",
      "Processing 52 of 1000 on page 1\n",
      "Processing 53 of 1000 on page 1\n",
      "Processing 54 of 1000 on page 1\n",
      "Processing 55 of 1000 on page 1\n",
      "Processing 56 of 1000 on page 1\n",
      "Processing 57 of 1000 on page 1\n",
      "Processing 58 of 1000 on page 1\n",
      "Processing 59 of 1000 on page 1\n",
      "Processing 60 of 1000 on page 1\n",
      "Processing 61 of 1000 on page 1\n",
      "Processing 62 of 1000 on page 1\n",
      "Processing 63 of 1000 on page 1\n",
      "Processing 64 of 1000 on page 1\n",
      "Processing 65 of 1000 on page 1\n",
      "Processing 66 of 1000 on page 1\n",
      "Processing 67 of 1000 on page 1\n",
      "Processing 68 of 1000 on page 1\n",
      "Processing 69 of 1000 on page 1\n",
      "Processing 70 of 1000 on page 1\n",
      "Processing 71 of 1000 on page 1\n",
      "Processing 72 of 1000 on page 1\n",
      "Processing 73 of 1000 on page 1\n",
      "Processing 74 of 1000 on page 1\n",
      "Processing 75 of 1000 on page 1\n",
      "Processing 76 of 1000 on page 1\n",
      "Processing 77 of 1000 on page 1\n",
      "Processing 78 of 1000 on page 1\n",
      "Processing 79 of 1000 on page 1\n",
      "Processing 80 of 1000 on page 1\n",
      "Processing 81 of 1000 on page 1\n",
      "Processing 82 of 1000 on page 1\n",
      "Processing 83 of 1000 on page 1\n",
      "Processing 84 of 1000 on page 1\n",
      "Processing 85 of 1000 on page 1\n",
      "Processing 86 of 1000 on page 1\n",
      "Processing 87 of 1000 on page 1\n",
      "Processing 88 of 1000 on page 1\n",
      "Processing 89 of 1000 on page 1\n",
      "Processing 90 of 1000 on page 1\n",
      "Processing 91 of 1000 on page 1\n",
      "Processing 92 of 1000 on page 1\n",
      "Processing 93 of 1000 on page 1\n",
      "Processing 94 of 1000 on page 1\n",
      "Processing 95 of 1000 on page 1\n",
      "Processing 96 of 1000 on page 1\n",
      "Processing 97 of 1000 on page 1\n",
      "Processing 98 of 1000 on page 1\n",
      "Processing 99 of 1000 on page 1\n",
      "Processing 100 of 1000 on page 1\n",
      "Processing 101 of 1000 on page 1\n",
      "Processing 102 of 1000 on page 1\n",
      "Processing 103 of 1000 on page 1\n",
      "Processing 104 of 1000 on page 1\n",
      "Processing 105 of 1000 on page 1\n",
      "Processing 106 of 1000 on page 1\n",
      "Processing 107 of 1000 on page 1\n",
      "Processing 108 of 1000 on page 1\n",
      "Processing 109 of 1000 on page 1\n",
      "Processing 110 of 1000 on page 1\n",
      "Processing 111 of 1000 on page 1\n",
      "Processing 112 of 1000 on page 1\n",
      "Processing 113 of 1000 on page 1\n",
      "Processing 114 of 1000 on page 1\n",
      "Processing 115 of 1000 on page 1\n",
      "Processing 116 of 1000 on page 1\n",
      "Processing 117 of 1000 on page 1\n",
      "Processing 118 of 1000 on page 1\n",
      "Processing 119 of 1000 on page 1\n",
      "Processing 120 of 1000 on page 1\n",
      "Processing 121 of 1000 on page 1\n",
      "Processing 122 of 1000 on page 1\n",
      "Processing 123 of 1000 on page 1\n",
      "Processing 124 of 1000 on page 1\n",
      "Processing 125 of 1000 on page 1\n",
      "Processing 126 of 1000 on page 1\n",
      "Processing 127 of 1000 on page 1\n",
      "Processing 128 of 1000 on page 1\n",
      "Processing 129 of 1000 on page 1\n",
      "Processing 130 of 1000 on page 1\n",
      "Processing 131 of 1000 on page 1\n",
      "Processing 132 of 1000 on page 1\n",
      "Processing 133 of 1000 on page 1\n",
      "Processing 134 of 1000 on page 1\n",
      "Processing 135 of 1000 on page 1\n",
      "Processing 136 of 1000 on page 1\n",
      "Processing 137 of 1000 on page 1\n",
      "Processing 138 of 1000 on page 1\n",
      "Processing 139 of 1000 on page 1\n",
      "Processing 140 of 1000 on page 1\n",
      "Processing 141 of 1000 on page 1\n",
      "Processing 142 of 1000 on page 1\n",
      "Processing 143 of 1000 on page 1\n",
      "Processing 144 of 1000 on page 1\n",
      "Processing 145 of 1000 on page 1\n",
      "Processing 146 of 1000 on page 1\n",
      "Processing 147 of 1000 on page 1\n",
      "Processing 148 of 1000 on page 1\n",
      "Processing 149 of 1000 on page 1\n",
      "Processing 150 of 1000 on page 1\n",
      "Processing 151 of 1000 on page 1\n",
      "Processing 152 of 1000 on page 1\n",
      "Processing 153 of 1000 on page 1\n",
      "Processing 154 of 1000 on page 1\n",
      "Processing 155 of 1000 on page 1\n",
      "Processing 156 of 1000 on page 1\n",
      "Processing 157 of 1000 on page 1\n",
      "Processing 158 of 1000 on page 1\n",
      "Processing 159 of 1000 on page 1\n",
      "Processing 160 of 1000 on page 1\n",
      "Processing 161 of 1000 on page 1\n",
      "Processing 162 of 1000 on page 1\n",
      "Processing 163 of 1000 on page 1\n",
      "Processing 164 of 1000 on page 1\n",
      "Processing 165 of 1000 on page 1\n",
      "Processing 166 of 1000 on page 1\n",
      "Processing 167 of 1000 on page 1\n",
      "Processing 168 of 1000 on page 1\n",
      "Processing 169 of 1000 on page 1\n",
      "Processing 170 of 1000 on page 1\n",
      "Processing 171 of 1000 on page 1\n",
      "Processing 172 of 1000 on page 1\n",
      "Processing 173 of 1000 on page 1\n",
      "Processing 174 of 1000 on page 1\n",
      "Processing 175 of 1000 on page 1\n",
      "Processing 176 of 1000 on page 1\n",
      "Processing 177 of 1000 on page 1\n",
      "Processing 178 of 1000 on page 1\n",
      "Processing 179 of 1000 on page 1\n",
      "Processing 180 of 1000 on page 1\n",
      "Processing 181 of 1000 on page 1\n",
      "Processing 182 of 1000 on page 1\n",
      "Processing 183 of 1000 on page 1\n",
      "Processing 184 of 1000 on page 1\n",
      "Processing 185 of 1000 on page 1\n",
      "Processing 186 of 1000 on page 1\n",
      "Processing 187 of 1000 on page 1\n",
      "Processing 188 of 1000 on page 1\n",
      "Processing 189 of 1000 on page 1\n",
      "Processing 190 of 1000 on page 1\n",
      "Processing 191 of 1000 on page 1\n",
      "Processing 192 of 1000 on page 1\n",
      "Processing 193 of 1000 on page 1\n",
      "Processing 194 of 1000 on page 1\n",
      "Processing 195 of 1000 on page 1\n",
      "Processing 196 of 1000 on page 1\n",
      "Processing 197 of 1000 on page 1\n",
      "Processing 198 of 1000 on page 1\n",
      "Processing 199 of 1000 on page 1\n",
      "Processing 200 of 1000 on page 1\n",
      "Processing 201 of 1000 on page 1\n",
      "Processing 202 of 1000 on page 1\n",
      "Processing 203 of 1000 on page 1\n",
      "Processing 204 of 1000 on page 1\n",
      "Processing 205 of 1000 on page 1\n",
      "Processing 206 of 1000 on page 1\n",
      "Processing 207 of 1000 on page 1\n",
      "Processing 208 of 1000 on page 1\n",
      "Processing 209 of 1000 on page 1\n",
      "Processing 210 of 1000 on page 1\n",
      "Processing 211 of 1000 on page 1\n",
      "Processing 212 of 1000 on page 1\n",
      "Processing 213 of 1000 on page 1\n",
      "Processing 214 of 1000 on page 1\n",
      "Processing 215 of 1000 on page 1\n",
      "Processing 216 of 1000 on page 1\n",
      "Processing 217 of 1000 on page 1\n",
      "Processing 218 of 1000 on page 1\n",
      "Processing 219 of 1000 on page 1\n",
      "Processing 220 of 1000 on page 1\n",
      "Processing 221 of 1000 on page 1\n",
      "Processing 222 of 1000 on page 1\n",
      "Processing 223 of 1000 on page 1\n",
      "Processing 224 of 1000 on page 1\n",
      "Processing 225 of 1000 on page 1\n",
      "Processing 226 of 1000 on page 1\n",
      "Processing 227 of 1000 on page 1\n",
      "Processing 228 of 1000 on page 1\n",
      "Processing 229 of 1000 on page 1\n",
      "Processing 230 of 1000 on page 1\n",
      "Processing 231 of 1000 on page 1\n",
      "Processing 232 of 1000 on page 1\n",
      "Processing 233 of 1000 on page 1\n",
      "Processing 234 of 1000 on page 1\n",
      "Processing 235 of 1000 on page 1\n",
      "Processing 236 of 1000 on page 1\n",
      "Processing 237 of 1000 on page 1\n",
      "Processing 238 of 1000 on page 1\n",
      "Processing 239 of 1000 on page 1\n",
      "Processing 240 of 1000 on page 1\n",
      "Processing 241 of 1000 on page 1\n",
      "Processing 242 of 1000 on page 1\n",
      "Processing 243 of 1000 on page 1\n",
      "Processing 244 of 1000 on page 1\n",
      "Processing 245 of 1000 on page 1\n",
      "Processing 246 of 1000 on page 1\n",
      "Processing 247 of 1000 on page 1\n",
      "Processing 248 of 1000 on page 1\n",
      "Processing 249 of 1000 on page 1\n",
      "Processing 250 of 1000 on page 1\n",
      "Processing 251 of 1000 on page 1\n",
      "Processing 252 of 1000 on page 1\n",
      "Processing 253 of 1000 on page 1\n",
      "Processing 254 of 1000 on page 1\n",
      "Processing 255 of 1000 on page 1\n",
      "Processing 256 of 1000 on page 1\n",
      "Processing 257 of 1000 on page 1\n",
      "Processing 258 of 1000 on page 1\n",
      "Processing 259 of 1000 on page 1\n",
      "Processing 260 of 1000 on page 1\n",
      "Processing 261 of 1000 on page 1\n",
      "Processing 262 of 1000 on page 1\n",
      "Processing 263 of 1000 on page 1\n",
      "Processing 264 of 1000 on page 1\n",
      "Processing 265 of 1000 on page 1\n",
      "Processing 266 of 1000 on page 1\n",
      "Processing 267 of 1000 on page 1\n",
      "Processing 268 of 1000 on page 1\n",
      "Processing 269 of 1000 on page 1\n",
      "Processing 270 of 1000 on page 1\n",
      "Processing 271 of 1000 on page 1\n",
      "Processing 272 of 1000 on page 1\n",
      "Processing 273 of 1000 on page 1\n",
      "Processing 274 of 1000 on page 1\n",
      "Processing 275 of 1000 on page 1\n",
      "Processing 276 of 1000 on page 1\n",
      "Processing 277 of 1000 on page 1\n",
      "Processing 278 of 1000 on page 1\n",
      "Processing 279 of 1000 on page 1\n",
      "Processing 280 of 1000 on page 1\n",
      "Processing 281 of 1000 on page 1\n",
      "Processing 282 of 1000 on page 1\n",
      "Processing 283 of 1000 on page 1\n",
      "Processing 284 of 1000 on page 1\n",
      "Processing 285 of 1000 on page 1\n",
      "Processing 286 of 1000 on page 1\n",
      "Processing 287 of 1000 on page 1\n",
      "Processing 288 of 1000 on page 1\n",
      "Processing 289 of 1000 on page 1\n",
      "Processing 290 of 1000 on page 1\n",
      "Processing 291 of 1000 on page 1\n",
      "Processing 292 of 1000 on page 1\n",
      "Processing 293 of 1000 on page 1\n",
      "Processing 294 of 1000 on page 1\n",
      "Processing 295 of 1000 on page 1\n",
      "Processing 296 of 1000 on page 1\n",
      "Processing 297 of 1000 on page 1\n",
      "Processing 298 of 1000 on page 1\n",
      "Processing 299 of 1000 on page 1\n",
      "Processing 300 of 1000 on page 1\n",
      "Processing 301 of 1000 on page 1\n",
      "Processing 302 of 1000 on page 1\n",
      "Processing 303 of 1000 on page 1\n",
      "Processing 304 of 1000 on page 1\n",
      "Processing 305 of 1000 on page 1\n",
      "Processing 306 of 1000 on page 1\n",
      "Processing 307 of 1000 on page 1\n",
      "Processing 308 of 1000 on page 1\n",
      "Processing 309 of 1000 on page 1\n",
      "Processing 310 of 1000 on page 1\n",
      "Processing 311 of 1000 on page 1\n",
      "Processing 312 of 1000 on page 1\n",
      "Processing 313 of 1000 on page 1\n",
      "Processing 314 of 1000 on page 1\n",
      "Processing 315 of 1000 on page 1\n",
      "Processing 316 of 1000 on page 1\n",
      "Processing 317 of 1000 on page 1\n",
      "Processing 318 of 1000 on page 1\n",
      "Processing 319 of 1000 on page 1\n",
      "Processing 320 of 1000 on page 1\n",
      "Processing 321 of 1000 on page 1\n",
      "Processing 322 of 1000 on page 1\n",
      "Processing 323 of 1000 on page 1\n",
      "Processing 324 of 1000 on page 1\n",
      "Processing 325 of 1000 on page 1\n",
      "Processing 326 of 1000 on page 1\n",
      "Processing 327 of 1000 on page 1\n",
      "Processing 328 of 1000 on page 1\n",
      "Processing 329 of 1000 on page 1\n",
      "Processing 330 of 1000 on page 1\n",
      "Processing 331 of 1000 on page 1\n",
      "Processing 332 of 1000 on page 1\n",
      "Processing 333 of 1000 on page 1\n",
      "Processing 334 of 1000 on page 1\n",
      "Processing 335 of 1000 on page 1\n",
      "Processing 336 of 1000 on page 1\n",
      "Processing 337 of 1000 on page 1\n",
      "Processing 338 of 1000 on page 1\n",
      "Processing 339 of 1000 on page 1\n",
      "Processing 340 of 1000 on page 1\n",
      "Processing 341 of 1000 on page 1\n",
      "Processing 342 of 1000 on page 1\n",
      "Processing 343 of 1000 on page 1\n",
      "Processing 344 of 1000 on page 1\n",
      "Processing 345 of 1000 on page 1\n",
      "Processing 346 of 1000 on page 1\n",
      "Processing 347 of 1000 on page 1\n",
      "Processing 348 of 1000 on page 1\n",
      "Processing 349 of 1000 on page 1\n",
      "Processing 350 of 1000 on page 1\n",
      "Processing 351 of 1000 on page 1\n",
      "Processing 352 of 1000 on page 1\n",
      "Processing 353 of 1000 on page 1\n",
      "Processing 354 of 1000 on page 1\n",
      "Processing 355 of 1000 on page 1\n",
      "Processing 356 of 1000 on page 1\n",
      "Processing 357 of 1000 on page 1\n",
      "Processing 358 of 1000 on page 1\n",
      "Processing 359 of 1000 on page 1\n",
      "Processing 360 of 1000 on page 1\n",
      "Processing 361 of 1000 on page 1\n",
      "Processing 362 of 1000 on page 1\n",
      "Processing 363 of 1000 on page 1\n",
      "Processing 364 of 1000 on page 1\n",
      "Processing 365 of 1000 on page 1\n",
      "Processing 366 of 1000 on page 1\n",
      "Processing 367 of 1000 on page 1\n",
      "Processing 368 of 1000 on page 1\n",
      "Processing 369 of 1000 on page 1\n",
      "Processing 370 of 1000 on page 1\n",
      "Processing 371 of 1000 on page 1\n",
      "Processing 372 of 1000 on page 1\n",
      "Processing 373 of 1000 on page 1\n",
      "Processing 374 of 1000 on page 1\n",
      "Processing 375 of 1000 on page 1\n",
      "Processing 376 of 1000 on page 1\n",
      "Processing 377 of 1000 on page 1\n",
      "Processing 378 of 1000 on page 1\n",
      "Processing 379 of 1000 on page 1\n",
      "Processing 380 of 1000 on page 1\n",
      "Processing 381 of 1000 on page 1\n",
      "Processing 382 of 1000 on page 1\n",
      "Processing 383 of 1000 on page 1\n",
      "Processing 384 of 1000 on page 1\n",
      "Processing 385 of 1000 on page 1\n",
      "Processing 386 of 1000 on page 1\n",
      "Processing 387 of 1000 on page 1\n",
      "Processing 388 of 1000 on page 1\n",
      "Processing 389 of 1000 on page 1\n",
      "Processing 390 of 1000 on page 1\n",
      "Processing 391 of 1000 on page 1\n",
      "Processing 392 of 1000 on page 1\n",
      "Processing 393 of 1000 on page 1\n",
      "Processing 394 of 1000 on page 1\n",
      "Processing 395 of 1000 on page 1\n",
      "Processing 396 of 1000 on page 1\n",
      "Processing 397 of 1000 on page 1\n",
      "Processing 398 of 1000 on page 1\n",
      "Processing 399 of 1000 on page 1\n",
      "Processing 400 of 1000 on page 1\n",
      "Processing 401 of 1000 on page 1\n",
      "Processing 402 of 1000 on page 1\n",
      "Processing 403 of 1000 on page 1\n",
      "Processing 404 of 1000 on page 1\n",
      "Processing 405 of 1000 on page 1\n",
      "Processing 406 of 1000 on page 1\n",
      "Processing 407 of 1000 on page 1\n",
      "Processing 408 of 1000 on page 1\n",
      "Processing 409 of 1000 on page 1\n",
      "Processing 410 of 1000 on page 1\n",
      "Processing 411 of 1000 on page 1\n",
      "Processing 412 of 1000 on page 1\n",
      "Processing 413 of 1000 on page 1\n",
      "Processing 414 of 1000 on page 1\n",
      "Processing 415 of 1000 on page 1\n",
      "Processing 416 of 1000 on page 1\n",
      "Processing 417 of 1000 on page 1\n",
      "Processing 418 of 1000 on page 1\n",
      "Processing 419 of 1000 on page 1\n",
      "Processing 420 of 1000 on page 1\n",
      "Processing 421 of 1000 on page 1\n",
      "Processing 422 of 1000 on page 1\n",
      "Processing 423 of 1000 on page 1\n",
      "Processing 424 of 1000 on page 1\n",
      "Processing 425 of 1000 on page 1\n",
      "Processing 426 of 1000 on page 1\n",
      "Processing 427 of 1000 on page 1\n",
      "Processing 428 of 1000 on page 1\n",
      "Processing 429 of 1000 on page 1\n",
      "Processing 430 of 1000 on page 1\n",
      "Processing 431 of 1000 on page 1\n",
      "Processing 432 of 1000 on page 1\n",
      "Processing 433 of 1000 on page 1\n",
      "Processing 434 of 1000 on page 1\n",
      "Processing 435 of 1000 on page 1\n",
      "Processing 436 of 1000 on page 1\n",
      "Processing 437 of 1000 on page 1\n",
      "Processing 438 of 1000 on page 1\n",
      "Processing 439 of 1000 on page 1\n",
      "Processing 440 of 1000 on page 1\n",
      "Processing 441 of 1000 on page 1\n",
      "Processing 442 of 1000 on page 1\n",
      "Processing 443 of 1000 on page 1\n",
      "Processing 444 of 1000 on page 1\n",
      "Processing 445 of 1000 on page 1\n",
      "Processing 446 of 1000 on page 1\n",
      "Processing 447 of 1000 on page 1\n",
      "Processing 448 of 1000 on page 1\n",
      "Processing 449 of 1000 on page 1\n",
      "Processing 450 of 1000 on page 1\n",
      "Processing 451 of 1000 on page 1\n",
      "Processing 452 of 1000 on page 1\n",
      "Processing 453 of 1000 on page 1\n",
      "Processing 454 of 1000 on page 1\n",
      "Processing 455 of 1000 on page 1\n",
      "Processing 456 of 1000 on page 1\n",
      "Processing 457 of 1000 on page 1\n",
      "Processing 458 of 1000 on page 1\n",
      "Processing 459 of 1000 on page 1\n",
      "Processing 460 of 1000 on page 1\n",
      "Processing 461 of 1000 on page 1\n",
      "Processing 462 of 1000 on page 1\n",
      "Processing 463 of 1000 on page 1\n",
      "Processing 464 of 1000 on page 1\n",
      "Processing 465 of 1000 on page 1\n",
      "Processing 466 of 1000 on page 1\n",
      "Processing 467 of 1000 on page 1\n",
      "Processing 468 of 1000 on page 1\n",
      "Processing 469 of 1000 on page 1\n",
      "Processing 470 of 1000 on page 1\n",
      "Processing 471 of 1000 on page 1\n",
      "Processing 472 of 1000 on page 1\n",
      "Processing 473 of 1000 on page 1\n",
      "Processing 474 of 1000 on page 1\n",
      "Processing 475 of 1000 on page 1\n",
      "Processing 476 of 1000 on page 1\n",
      "Processing 477 of 1000 on page 1\n",
      "Processing 478 of 1000 on page 1\n",
      "Processing 479 of 1000 on page 1\n",
      "Processing 480 of 1000 on page 1\n",
      "Processing 481 of 1000 on page 1\n",
      "Processing 482 of 1000 on page 1\n",
      "Processing 483 of 1000 on page 1\n",
      "Processing 484 of 1000 on page 1\n",
      "Processing 485 of 1000 on page 1\n",
      "Processing 486 of 1000 on page 1\n",
      "Processing 487 of 1000 on page 1\n",
      "Processing 488 of 1000 on page 1\n",
      "Processing 489 of 1000 on page 1\n",
      "Processing 490 of 1000 on page 1\n",
      "Processing 491 of 1000 on page 1\n",
      "Processing 492 of 1000 on page 1\n",
      "Processing 493 of 1000 on page 1\n",
      "Processing 494 of 1000 on page 1\n",
      "Processing 495 of 1000 on page 1\n",
      "Processing 496 of 1000 on page 1\n",
      "Processing 497 of 1000 on page 1\n",
      "Processing 498 of 1000 on page 1\n",
      "Processing 499 of 1000 on page 1\n",
      "Processing 500 of 1000 on page 1\n",
      "Processing 501 of 1000 on page 1\n",
      "Processing 502 of 1000 on page 1\n",
      "Processing 503 of 1000 on page 1\n",
      "Processing 504 of 1000 on page 1\n",
      "Processing 505 of 1000 on page 1\n",
      "Processing 506 of 1000 on page 1\n",
      "Processing 507 of 1000 on page 1\n",
      "Processing 508 of 1000 on page 1\n",
      "Processing 509 of 1000 on page 1\n",
      "Processing 510 of 1000 on page 1\n",
      "Processing 511 of 1000 on page 1\n",
      "Processing 512 of 1000 on page 1\n",
      "Processing 513 of 1000 on page 1\n",
      "Processing 514 of 1000 on page 1\n",
      "Processing 515 of 1000 on page 1\n",
      "Processing 516 of 1000 on page 1\n",
      "Processing 517 of 1000 on page 1\n",
      "Processing 518 of 1000 on page 1\n",
      "Processing 519 of 1000 on page 1\n",
      "Processing 520 of 1000 on page 1\n",
      "Processing 521 of 1000 on page 1\n",
      "Processing 522 of 1000 on page 1\n",
      "Processing 523 of 1000 on page 1\n",
      "Processing 524 of 1000 on page 1\n",
      "Processing 525 of 1000 on page 1\n",
      "Processing 526 of 1000 on page 1\n",
      "Processing 527 of 1000 on page 1\n",
      "Processing 528 of 1000 on page 1\n",
      "Processing 529 of 1000 on page 1\n",
      "Processing 530 of 1000 on page 1\n",
      "Processing 531 of 1000 on page 1\n",
      "Processing 532 of 1000 on page 1\n",
      "Processing 533 of 1000 on page 1\n",
      "Processing 534 of 1000 on page 1\n",
      "Processing 535 of 1000 on page 1\n",
      "Processing 536 of 1000 on page 1\n",
      "Processing 537 of 1000 on page 1\n",
      "Processing 538 of 1000 on page 1\n",
      "Processing 539 of 1000 on page 1\n",
      "Processing 540 of 1000 on page 1\n",
      "Processing 541 of 1000 on page 1\n",
      "Processing 542 of 1000 on page 1\n",
      "Processing 543 of 1000 on page 1\n",
      "Processing 544 of 1000 on page 1\n",
      "Processing 545 of 1000 on page 1\n",
      "Processing 546 of 1000 on page 1\n",
      "Processing 547 of 1000 on page 1\n",
      "Processing 548 of 1000 on page 1\n",
      "Processing 549 of 1000 on page 1\n",
      "Processing 550 of 1000 on page 1\n",
      "Processing 551 of 1000 on page 1\n",
      "Processing 552 of 1000 on page 1\n",
      "Processing 553 of 1000 on page 1\n",
      "Processing 554 of 1000 on page 1\n",
      "Processing 555 of 1000 on page 1\n",
      "Processing 556 of 1000 on page 1\n",
      "Processing 557 of 1000 on page 1\n",
      "Processing 558 of 1000 on page 1\n",
      "Processing 559 of 1000 on page 1\n",
      "Processing 560 of 1000 on page 1\n",
      "Processing 561 of 1000 on page 1\n",
      "Processing 562 of 1000 on page 1\n",
      "Processing 563 of 1000 on page 1\n",
      "Processing 564 of 1000 on page 1\n",
      "Processing 565 of 1000 on page 1\n",
      "Processing 566 of 1000 on page 1\n",
      "Processing 567 of 1000 on page 1\n",
      "Processing 568 of 1000 on page 1\n",
      "Processing 569 of 1000 on page 1\n",
      "Processing 570 of 1000 on page 1\n",
      "Processing 571 of 1000 on page 1\n",
      "Processing 572 of 1000 on page 1\n",
      "Processing 573 of 1000 on page 1\n",
      "Processing 574 of 1000 on page 1\n",
      "Processing 575 of 1000 on page 1\n",
      "Processing 576 of 1000 on page 1\n",
      "Processing 577 of 1000 on page 1\n",
      "Processing 578 of 1000 on page 1\n",
      "Processing 579 of 1000 on page 1\n",
      "Processing 580 of 1000 on page 1\n",
      "Processing 581 of 1000 on page 1\n",
      "Processing 582 of 1000 on page 1\n",
      "Processing 583 of 1000 on page 1\n",
      "Processing 584 of 1000 on page 1\n",
      "Processing 585 of 1000 on page 1\n",
      "Processing 586 of 1000 on page 1\n",
      "Processing 587 of 1000 on page 1\n",
      "Processing 588 of 1000 on page 1\n",
      "Processing 589 of 1000 on page 1\n",
      "Processing 590 of 1000 on page 1\n",
      "Processing 591 of 1000 on page 1\n",
      "Processing 592 of 1000 on page 1\n",
      "Processing 593 of 1000 on page 1\n",
      "Processing 594 of 1000 on page 1\n",
      "Processing 595 of 1000 on page 1\n",
      "Processing 596 of 1000 on page 1\n",
      "Processing 597 of 1000 on page 1\n",
      "Processing 598 of 1000 on page 1\n",
      "Processing 599 of 1000 on page 1\n",
      "Processing 600 of 1000 on page 1\n",
      "Processing 601 of 1000 on page 1\n",
      "Processing 602 of 1000 on page 1\n",
      "Processing 603 of 1000 on page 1\n",
      "Processing 604 of 1000 on page 1\n",
      "Processing 605 of 1000 on page 1\n",
      "Processing 606 of 1000 on page 1\n",
      "Processing 607 of 1000 on page 1\n",
      "Processing 608 of 1000 on page 1\n",
      "Processing 609 of 1000 on page 1\n",
      "Processing 610 of 1000 on page 1\n",
      "Processing 611 of 1000 on page 1\n",
      "Processing 612 of 1000 on page 1\n",
      "Processing 613 of 1000 on page 1\n",
      "Processing 614 of 1000 on page 1\n",
      "Processing 615 of 1000 on page 1\n",
      "Processing 616 of 1000 on page 1\n",
      "Processing 617 of 1000 on page 1\n",
      "Processing 618 of 1000 on page 1\n",
      "Processing 619 of 1000 on page 1\n",
      "Processing 620 of 1000 on page 1\n",
      "Processing 621 of 1000 on page 1\n",
      "Processing 622 of 1000 on page 1\n",
      "Processing 623 of 1000 on page 1\n",
      "Processing 624 of 1000 on page 1\n",
      "Processing 625 of 1000 on page 1\n",
      "Processing 626 of 1000 on page 1\n",
      "Processing 627 of 1000 on page 1\n",
      "Processing 628 of 1000 on page 1\n",
      "Processing 629 of 1000 on page 1\n",
      "Processing 630 of 1000 on page 1\n",
      "Processing 631 of 1000 on page 1\n",
      "Processing 632 of 1000 on page 1\n",
      "Processing 633 of 1000 on page 1\n",
      "Processing 634 of 1000 on page 1\n",
      "Processing 635 of 1000 on page 1\n",
      "Processing 636 of 1000 on page 1\n",
      "Processing 637 of 1000 on page 1\n",
      "Processing 638 of 1000 on page 1\n",
      "Processing 639 of 1000 on page 1\n",
      "Processing 640 of 1000 on page 1\n",
      "Processing 641 of 1000 on page 1\n",
      "Processing 642 of 1000 on page 1\n",
      "Processing 643 of 1000 on page 1\n",
      "Processing 644 of 1000 on page 1\n",
      "Processing 645 of 1000 on page 1\n",
      "Processing 646 of 1000 on page 1\n",
      "Processing 647 of 1000 on page 1\n",
      "Processing 648 of 1000 on page 1\n",
      "Processing 649 of 1000 on page 1\n",
      "Processing 650 of 1000 on page 1\n",
      "Processing 651 of 1000 on page 1\n",
      "Processing 652 of 1000 on page 1\n",
      "Processing 653 of 1000 on page 1\n",
      "Processing 654 of 1000 on page 1\n",
      "Processing 655 of 1000 on page 1\n",
      "Processing 656 of 1000 on page 1\n",
      "Processing 657 of 1000 on page 1\n",
      "Processing 658 of 1000 on page 1\n",
      "Processing 659 of 1000 on page 1\n",
      "Processing 660 of 1000 on page 1\n",
      "Processing 661 of 1000 on page 1\n",
      "Processing 662 of 1000 on page 1\n",
      "Processing 663 of 1000 on page 1\n",
      "Processing 664 of 1000 on page 1\n",
      "Processing 665 of 1000 on page 1\n",
      "Processing 666 of 1000 on page 1\n",
      "Processing 667 of 1000 on page 1\n",
      "Processing 668 of 1000 on page 1\n",
      "Processing 669 of 1000 on page 1\n",
      "Processing 670 of 1000 on page 1\n",
      "Processing 671 of 1000 on page 1\n",
      "Processing 672 of 1000 on page 1\n",
      "Processing 673 of 1000 on page 1\n",
      "Processing 674 of 1000 on page 1\n",
      "Processing 675 of 1000 on page 1\n",
      "Processing 676 of 1000 on page 1\n",
      "Processing 677 of 1000 on page 1\n",
      "Processing 678 of 1000 on page 1\n",
      "Processing 679 of 1000 on page 1\n",
      "Processing 680 of 1000 on page 1\n",
      "Processing 681 of 1000 on page 1\n",
      "Processing 682 of 1000 on page 1\n",
      "Processing 683 of 1000 on page 1\n",
      "Processing 684 of 1000 on page 1\n",
      "Processing 685 of 1000 on page 1\n",
      "Processing 686 of 1000 on page 1\n",
      "Processing 687 of 1000 on page 1\n",
      "Processing 688 of 1000 on page 1\n",
      "Processing 689 of 1000 on page 1\n",
      "Processing 690 of 1000 on page 1\n",
      "Processing 691 of 1000 on page 1\n",
      "Processing 692 of 1000 on page 1\n",
      "Processing 693 of 1000 on page 1\n",
      "Processing 694 of 1000 on page 1\n",
      "Processing 695 of 1000 on page 1\n",
      "Processing 696 of 1000 on page 1\n",
      "Processing 697 of 1000 on page 1\n",
      "Processing 698 of 1000 on page 1\n",
      "Processing 699 of 1000 on page 1\n",
      "Processing 700 of 1000 on page 1\n",
      "Processing 701 of 1000 on page 1\n",
      "Processing 702 of 1000 on page 1\n",
      "Processing 703 of 1000 on page 1\n",
      "Processing 704 of 1000 on page 1\n",
      "Processing 705 of 1000 on page 1\n",
      "Processing 706 of 1000 on page 1\n",
      "Processing 707 of 1000 on page 1\n",
      "Processing 708 of 1000 on page 1\n",
      "Processing 709 of 1000 on page 1\n",
      "Processing 710 of 1000 on page 1\n",
      "Processing 711 of 1000 on page 1\n",
      "Processing 712 of 1000 on page 1\n",
      "Processing 713 of 1000 on page 1\n",
      "Processing 714 of 1000 on page 1\n",
      "Processing 715 of 1000 on page 1\n",
      "Processing 716 of 1000 on page 1\n",
      "Processing 717 of 1000 on page 1\n",
      "Processing 718 of 1000 on page 1\n",
      "Processing 719 of 1000 on page 1\n",
      "Processing 720 of 1000 on page 1\n",
      "Processing 721 of 1000 on page 1\n",
      "Processing 722 of 1000 on page 1\n",
      "Processing 723 of 1000 on page 1\n",
      "Processing 724 of 1000 on page 1\n",
      "Processing 725 of 1000 on page 1\n",
      "Processing 726 of 1000 on page 1\n",
      "Processing 727 of 1000 on page 1\n",
      "Processing 728 of 1000 on page 1\n",
      "Processing 729 of 1000 on page 1\n",
      "Processing 730 of 1000 on page 1\n",
      "Processing 731 of 1000 on page 1\n",
      "Processing 732 of 1000 on page 1\n",
      "Processing 733 of 1000 on page 1\n",
      "Processing 734 of 1000 on page 1\n",
      "Processing 735 of 1000 on page 1\n",
      "Processing 736 of 1000 on page 1\n",
      "Processing 737 of 1000 on page 1\n",
      "Processing 738 of 1000 on page 1\n",
      "Processing 739 of 1000 on page 1\n",
      "Processing 740 of 1000 on page 1\n",
      "Processing 741 of 1000 on page 1\n",
      "Processing 742 of 1000 on page 1\n",
      "Processing 743 of 1000 on page 1\n",
      "Processing 744 of 1000 on page 1\n",
      "Processing 745 of 1000 on page 1\n",
      "Processing 746 of 1000 on page 1\n",
      "Processing 747 of 1000 on page 1\n",
      "Processing 748 of 1000 on page 1\n",
      "Processing 749 of 1000 on page 1\n",
      "Processing 750 of 1000 on page 1\n",
      "Processing 751 of 1000 on page 1\n",
      "Processing 752 of 1000 on page 1\n",
      "Processing 753 of 1000 on page 1\n",
      "Processing 754 of 1000 on page 1\n",
      "Processing 755 of 1000 on page 1\n",
      "Processing 756 of 1000 on page 1\n",
      "Processing 757 of 1000 on page 1\n",
      "Processing 758 of 1000 on page 1\n",
      "Processing 759 of 1000 on page 1\n",
      "Processing 760 of 1000 on page 1\n",
      "Processing 761 of 1000 on page 1\n",
      "Processing 762 of 1000 on page 1\n",
      "Processing 763 of 1000 on page 1\n",
      "Processing 764 of 1000 on page 1\n",
      "Processing 765 of 1000 on page 1\n",
      "Processing 766 of 1000 on page 1\n",
      "Processing 767 of 1000 on page 1\n",
      "Processing 768 of 1000 on page 1\n",
      "Processing 769 of 1000 on page 1\n",
      "Processing 770 of 1000 on page 1\n",
      "Processing 771 of 1000 on page 1\n",
      "Processing 772 of 1000 on page 1\n",
      "Processing 773 of 1000 on page 1\n",
      "Processing 774 of 1000 on page 1\n",
      "Processing 775 of 1000 on page 1\n",
      "Processing 776 of 1000 on page 1\n",
      "Processing 777 of 1000 on page 1\n",
      "Processing 778 of 1000 on page 1\n",
      "Processing 779 of 1000 on page 1\n",
      "Processing 780 of 1000 on page 1\n",
      "Processing 781 of 1000 on page 1\n",
      "Processing 782 of 1000 on page 1\n",
      "Processing 783 of 1000 on page 1\n",
      "Processing 784 of 1000 on page 1\n",
      "Processing 785 of 1000 on page 1\n",
      "Processing 786 of 1000 on page 1\n",
      "Processing 787 of 1000 on page 1\n",
      "Processing 788 of 1000 on page 1\n",
      "Processing 789 of 1000 on page 1\n",
      "Processing 790 of 1000 on page 1\n",
      "Processing 791 of 1000 on page 1\n",
      "Processing 792 of 1000 on page 1\n",
      "Processing 793 of 1000 on page 1\n",
      "Processing 794 of 1000 on page 1\n",
      "Processing 795 of 1000 on page 1\n",
      "Processing 796 of 1000 on page 1\n",
      "Processing 797 of 1000 on page 1\n",
      "Processing 798 of 1000 on page 1\n",
      "Processing 799 of 1000 on page 1\n",
      "Processing 800 of 1000 on page 1\n",
      "Processing 801 of 1000 on page 1\n",
      "Processing 802 of 1000 on page 1\n",
      "Processing 803 of 1000 on page 1\n",
      "Processing 804 of 1000 on page 1\n",
      "Processing 805 of 1000 on page 1\n",
      "Processing 806 of 1000 on page 1\n",
      "Processing 807 of 1000 on page 1\n",
      "Processing 808 of 1000 on page 1\n",
      "Processing 809 of 1000 on page 1\n",
      "Processing 810 of 1000 on page 1\n",
      "Processing 811 of 1000 on page 1\n",
      "Processing 812 of 1000 on page 1\n",
      "Processing 813 of 1000 on page 1\n",
      "Processing 814 of 1000 on page 1\n",
      "Processing 815 of 1000 on page 1\n",
      "Processing 816 of 1000 on page 1\n",
      "Processing 817 of 1000 on page 1\n",
      "Processing 818 of 1000 on page 1\n",
      "Processing 819 of 1000 on page 1\n",
      "Processing 820 of 1000 on page 1\n",
      "Processing 821 of 1000 on page 1\n",
      "Processing 822 of 1000 on page 1\n",
      "Processing 823 of 1000 on page 1\n",
      "Processing 824 of 1000 on page 1\n",
      "Processing 825 of 1000 on page 1\n",
      "Processing 826 of 1000 on page 1\n",
      "Processing 827 of 1000 on page 1\n",
      "Processing 828 of 1000 on page 1\n",
      "Processing 829 of 1000 on page 1\n",
      "Processing 830 of 1000 on page 1\n",
      "Processing 831 of 1000 on page 1\n",
      "Processing 832 of 1000 on page 1\n",
      "Processing 833 of 1000 on page 1\n",
      "Processing 834 of 1000 on page 1\n",
      "Processing 835 of 1000 on page 1\n",
      "Processing 836 of 1000 on page 1\n",
      "Processing 837 of 1000 on page 1\n",
      "Processing 838 of 1000 on page 1\n",
      "Processing 839 of 1000 on page 1\n",
      "Processing 840 of 1000 on page 1\n",
      "Processing 841 of 1000 on page 1\n",
      "Processing 842 of 1000 on page 1\n",
      "Processing 843 of 1000 on page 1\n",
      "Processing 844 of 1000 on page 1\n",
      "Processing 845 of 1000 on page 1\n",
      "Processing 846 of 1000 on page 1\n",
      "Processing 847 of 1000 on page 1\n",
      "Processing 848 of 1000 on page 1\n",
      "Processing 849 of 1000 on page 1\n",
      "Processing 850 of 1000 on page 1\n",
      "Processing 851 of 1000 on page 1\n",
      "Processing 852 of 1000 on page 1\n",
      "Processing 853 of 1000 on page 1\n",
      "Processing 854 of 1000 on page 1\n",
      "Processing 855 of 1000 on page 1\n",
      "Processing 856 of 1000 on page 1\n",
      "Processing 857 of 1000 on page 1\n",
      "Processing 858 of 1000 on page 1\n",
      "Processing 859 of 1000 on page 1\n",
      "Processing 860 of 1000 on page 1\n",
      "Processing 861 of 1000 on page 1\n",
      "Processing 862 of 1000 on page 1\n",
      "Processing 863 of 1000 on page 1\n",
      "Processing 864 of 1000 on page 1\n",
      "Processing 865 of 1000 on page 1\n",
      "Processing 866 of 1000 on page 1\n",
      "Processing 867 of 1000 on page 1\n",
      "Processing 868 of 1000 on page 1\n",
      "Processing 869 of 1000 on page 1\n",
      "Processing 870 of 1000 on page 1\n",
      "Processing 871 of 1000 on page 1\n",
      "Processing 872 of 1000 on page 1\n",
      "Processing 873 of 1000 on page 1\n",
      "Processing 874 of 1000 on page 1\n",
      "Processing 875 of 1000 on page 1\n",
      "Processing 876 of 1000 on page 1\n",
      "Processing 877 of 1000 on page 1\n",
      "Processing 878 of 1000 on page 1\n",
      "Processing 879 of 1000 on page 1\n",
      "Processing 880 of 1000 on page 1\n",
      "Processing 881 of 1000 on page 1\n",
      "Processing 882 of 1000 on page 1\n",
      "Processing 883 of 1000 on page 1\n",
      "Processing 884 of 1000 on page 1\n",
      "Processing 885 of 1000 on page 1\n",
      "Processing 886 of 1000 on page 1\n",
      "Processing 887 of 1000 on page 1\n",
      "Processing 888 of 1000 on page 1\n",
      "Processing 889 of 1000 on page 1\n",
      "Processing 890 of 1000 on page 1\n",
      "Processing 891 of 1000 on page 1\n",
      "Processing 892 of 1000 on page 1\n",
      "Processing 893 of 1000 on page 1\n",
      "Processing 894 of 1000 on page 1\n",
      "Processing 895 of 1000 on page 1\n",
      "Processing 896 of 1000 on page 1\n",
      "Processing 897 of 1000 on page 1\n",
      "Processing 898 of 1000 on page 1\n",
      "Processing 899 of 1000 on page 1\n",
      "Processing 900 of 1000 on page 1\n",
      "Processing 901 of 1000 on page 1\n",
      "Processing 902 of 1000 on page 1\n",
      "Processing 903 of 1000 on page 1\n",
      "Processing 904 of 1000 on page 1\n",
      "Processing 905 of 1000 on page 1\n",
      "Processing 906 of 1000 on page 1\n",
      "Processing 907 of 1000 on page 1\n",
      "Processing 908 of 1000 on page 1\n",
      "Processing 909 of 1000 on page 1\n",
      "Processing 910 of 1000 on page 1\n",
      "Processing 911 of 1000 on page 1\n",
      "Processing 912 of 1000 on page 1\n",
      "Processing 913 of 1000 on page 1\n",
      "Processing 914 of 1000 on page 1\n",
      "Processing 915 of 1000 on page 1\n",
      "Processing 916 of 1000 on page 1\n",
      "Processing 917 of 1000 on page 1\n",
      "Processing 918 of 1000 on page 1\n",
      "Processing 919 of 1000 on page 1\n",
      "Processing 920 of 1000 on page 1\n",
      "Processing 921 of 1000 on page 1\n",
      "Processing 922 of 1000 on page 1\n",
      "Processing 923 of 1000 on page 1\n",
      "Processing 924 of 1000 on page 1\n",
      "Processing 925 of 1000 on page 1\n",
      "Processing 926 of 1000 on page 1\n",
      "Processing 927 of 1000 on page 1\n",
      "Processing 928 of 1000 on page 1\n",
      "Processing 929 of 1000 on page 1\n",
      "Processing 930 of 1000 on page 1\n",
      "Processing 931 of 1000 on page 1\n",
      "Processing 932 of 1000 on page 1\n",
      "Processing 933 of 1000 on page 1\n",
      "Processing 934 of 1000 on page 1\n",
      "Processing 935 of 1000 on page 1\n",
      "Processing 936 of 1000 on page 1\n",
      "Processing 937 of 1000 on page 1\n",
      "Processing 938 of 1000 on page 1\n",
      "Processing 939 of 1000 on page 1\n",
      "Processing 940 of 1000 on page 1\n",
      "Processing 941 of 1000 on page 1\n",
      "Processing 942 of 1000 on page 1\n",
      "Processing 943 of 1000 on page 1\n",
      "Processing 944 of 1000 on page 1\n",
      "Processing 945 of 1000 on page 1\n",
      "Processing 946 of 1000 on page 1\n",
      "Processing 947 of 1000 on page 1\n",
      "Processing 948 of 1000 on page 1\n",
      "Processing 949 of 1000 on page 1\n",
      "Processing 950 of 1000 on page 1\n",
      "Processing 951 of 1000 on page 1\n",
      "Processing 952 of 1000 on page 1\n",
      "Processing 953 of 1000 on page 1\n",
      "Processing 954 of 1000 on page 1\n",
      "Processing 955 of 1000 on page 1\n",
      "Processing 956 of 1000 on page 1\n",
      "Processing 957 of 1000 on page 1\n",
      "Processing 958 of 1000 on page 1\n",
      "Processing 959 of 1000 on page 1\n",
      "Processing 960 of 1000 on page 1\n",
      "Processing 961 of 1000 on page 1\n",
      "Processing 962 of 1000 on page 1\n",
      "Processing 963 of 1000 on page 1\n",
      "Processing 964 of 1000 on page 1\n",
      "Processing 965 of 1000 on page 1\n",
      "Processing 966 of 1000 on page 1\n",
      "Processing 967 of 1000 on page 1\n",
      "Processing 968 of 1000 on page 1\n",
      "Processing 969 of 1000 on page 1\n",
      "Processing 970 of 1000 on page 1\n",
      "Processing 971 of 1000 on page 1\n",
      "Processing 972 of 1000 on page 1\n",
      "Processing 973 of 1000 on page 1\n",
      "Processing 974 of 1000 on page 1\n",
      "Processing 975 of 1000 on page 1\n",
      "Processing 976 of 1000 on page 1\n",
      "Processing 977 of 1000 on page 1\n",
      "Processing 978 of 1000 on page 1\n",
      "Processing 979 of 1000 on page 1\n",
      "Processing 980 of 1000 on page 1\n",
      "Processing 981 of 1000 on page 1\n",
      "Processing 982 of 1000 on page 1\n",
      "Processing 983 of 1000 on page 1\n",
      "Processing 984 of 1000 on page 1\n",
      "Processing 985 of 1000 on page 1\n",
      "Processing 986 of 1000 on page 1\n",
      "Processing 987 of 1000 on page 1\n",
      "Processing 988 of 1000 on page 1\n",
      "Processing 989 of 1000 on page 1\n",
      "Processing 990 of 1000 on page 1\n",
      "Processing 991 of 1000 on page 1\n",
      "Processing 992 of 1000 on page 1\n",
      "Processing 993 of 1000 on page 1\n",
      "Processing 994 of 1000 on page 1\n",
      "Processing 995 of 1000 on page 1\n",
      "Processing 996 of 1000 on page 1\n",
      "Processing 997 of 1000 on page 1\n",
      "Processing 998 of 1000 on page 1\n",
      "Processing 999 of 1000 on page 1\n",
      "Processing 1000 of 1000 on page 1\n",
      "Processing 1001 of 1000 on page 2\n",
      "Processing 1002 of 1000 on page 2\n",
      "Processing 1003 of 1000 on page 2\n",
      "Processing 1004 of 1000 on page 2\n",
      "Processing 1005 of 1000 on page 2\n",
      "Processing 1006 of 1000 on page 2\n",
      "Processing 1007 of 1000 on page 2\n",
      "Processing 1008 of 1000 on page 2\n",
      "Processing 1009 of 1000 on page 2\n",
      "Processing 1010 of 1000 on page 2\n",
      "Processing 1011 of 1000 on page 2\n",
      "Processing 1012 of 1000 on page 2\n",
      "Processing 1013 of 1000 on page 2\n",
      "Processing 1014 of 1000 on page 2\n",
      "Processing 1015 of 1000 on page 2\n",
      "Processing 1016 of 1000 on page 2\n",
      "Processing 1017 of 1000 on page 2\n",
      "Processing 1018 of 1000 on page 2\n",
      "Processing 1019 of 1000 on page 2\n",
      "Processing 1020 of 1000 on page 2\n",
      "Processing 1021 of 1000 on page 2\n",
      "Processing 1022 of 1000 on page 2\n",
      "Processing 1023 of 1000 on page 2\n",
      "Processing 1024 of 1000 on page 2\n",
      "Processing 1025 of 1000 on page 2\n",
      "Processing 1026 of 1000 on page 2\n",
      "Processing 1027 of 1000 on page 2\n",
      "Processing 1028 of 1000 on page 2\n",
      "Processing 1029 of 1000 on page 2\n",
      "Processing 1030 of 1000 on page 2\n",
      "Processing 1031 of 1000 on page 2\n",
      "Processing 1032 of 1000 on page 2\n",
      "Processing 1033 of 1000 on page 2\n",
      "Processing 1034 of 1000 on page 2\n",
      "Processing 1035 of 1000 on page 2\n",
      "Processing 1036 of 1000 on page 2\n",
      "Processing 1037 of 1000 on page 2\n",
      "Processing 1038 of 1000 on page 2\n",
      "Processing 1039 of 1000 on page 2\n",
      "Processing 1040 of 1000 on page 2\n",
      "Processing 1041 of 1000 on page 2\n",
      "Processing 1042 of 1000 on page 2\n",
      "Processing 1043 of 1000 on page 2\n",
      "Processing 1044 of 1000 on page 2\n",
      "Processing 1045 of 1000 on page 2\n",
      "Processing 1046 of 1000 on page 2\n",
      "Processing 1047 of 1000 on page 2\n",
      "Processing 1048 of 1000 on page 2\n",
      "Processing 1049 of 1000 on page 2\n",
      "Processing 1050 of 1000 on page 2\n",
      "Processing 1051 of 1000 on page 2\n",
      "Processing 1052 of 1000 on page 2\n",
      "Processing 1053 of 1000 on page 2\n",
      "Processing 1054 of 1000 on page 2\n",
      "Processing 1055 of 1000 on page 2\n",
      "Processing 1056 of 1000 on page 2\n",
      "Processing 1057 of 1000 on page 2\n",
      "Processing 1058 of 1000 on page 2\n",
      "Processing 1059 of 1000 on page 2\n",
      "Processing 1060 of 1000 on page 2\n",
      "Processing 1061 of 1000 on page 2\n",
      "Processing 1062 of 1000 on page 2\n",
      "Processing 1063 of 1000 on page 2\n",
      "Processing 1064 of 1000 on page 2\n",
      "Processing 1065 of 1000 on page 2\n",
      "Processing 1066 of 1000 on page 2\n",
      "Processing 1067 of 1000 on page 2\n",
      "Processing 1068 of 1000 on page 2\n",
      "Processing 1069 of 1000 on page 2\n",
      "Processing 1070 of 1000 on page 2\n",
      "Processing 1071 of 1000 on page 2\n",
      "Processing 1072 of 1000 on page 2\n",
      "Processing 1073 of 1000 on page 2\n",
      "Processing 1074 of 1000 on page 2\n",
      "Processing 1075 of 1000 on page 2\n",
      "Processing 1076 of 1000 on page 2\n",
      "Processing 1077 of 1000 on page 2\n",
      "Processing 1078 of 1000 on page 2\n",
      "Processing 1079 of 1000 on page 2\n",
      "Processing 1080 of 1000 on page 2\n",
      "Processing 1081 of 1000 on page 2\n",
      "Processing 1082 of 1000 on page 2\n",
      "Processing 1083 of 1000 on page 2\n",
      "Processing 1084 of 1000 on page 2\n",
      "Processing 1085 of 1000 on page 2\n",
      "Processing 1086 of 1000 on page 2\n",
      "Processing 1087 of 1000 on page 2\n",
      "Processing 1088 of 1000 on page 2\n",
      "Processing 1089 of 1000 on page 2\n",
      "Processing 1090 of 1000 on page 2\n",
      "Processing 1091 of 1000 on page 2\n",
      "Processing 1092 of 1000 on page 2\n",
      "Processing 1093 of 1000 on page 2\n",
      "Processing 1094 of 1000 on page 2\n",
      "Processing 1095 of 1000 on page 2\n",
      "Processing 1096 of 1000 on page 2\n",
      "Processing 1097 of 1000 on page 2\n",
      "Processing 1098 of 1000 on page 2\n",
      "Processing 1099 of 1000 on page 2\n",
      "Processing 1100 of 1000 on page 2\n",
      "Processing 1101 of 1000 on page 2\n",
      "Processing 1102 of 1000 on page 2\n",
      "Processing 1103 of 1000 on page 2\n",
      "Processing 1104 of 1000 on page 2\n",
      "Processing 1105 of 1000 on page 2\n",
      "Processing 1106 of 1000 on page 2\n",
      "Processing 1107 of 1000 on page 2\n",
      "Processing 1108 of 1000 on page 2\n",
      "Processing 1109 of 1000 on page 2\n",
      "Processing 1110 of 1000 on page 2\n",
      "Processing 1111 of 1000 on page 2\n",
      "Processing 1112 of 1000 on page 2\n",
      "Processing 1113 of 1000 on page 2\n",
      "Processing 1114 of 1000 on page 2\n",
      "Processing 1115 of 1000 on page 2\n",
      "Processing 1116 of 1000 on page 2\n",
      "Processing 1117 of 1000 on page 2\n",
      "Processing 1118 of 1000 on page 2\n",
      "Processing 1119 of 1000 on page 2\n",
      "Processing 1120 of 1000 on page 2\n",
      "Processing 1121 of 1000 on page 2\n",
      "Processing 1122 of 1000 on page 2\n",
      "Processing 1123 of 1000 on page 2\n",
      "Processing 1124 of 1000 on page 2\n",
      "Processing 1125 of 1000 on page 2\n",
      "Processing 1126 of 1000 on page 2\n",
      "Processing 1127 of 1000 on page 2\n",
      "Processing 1128 of 1000 on page 2\n",
      "Processing 1129 of 1000 on page 2\n",
      "Processing 1130 of 1000 on page 2\n",
      "Processing 1131 of 1000 on page 2\n",
      "Processing 1132 of 1000 on page 2\n",
      "Processing 1133 of 1000 on page 2\n",
      "Processing 1134 of 1000 on page 2\n",
      "Processing 1135 of 1000 on page 2\n",
      "Processing 1136 of 1000 on page 2\n",
      "Processing 1137 of 1000 on page 2\n",
      "Processing 1138 of 1000 on page 2\n",
      "Processing 1139 of 1000 on page 2\n",
      "Processing 1140 of 1000 on page 2\n",
      "Processing 1141 of 1000 on page 2\n",
      "Processing 1142 of 1000 on page 2\n",
      "Processing 1143 of 1000 on page 2\n",
      "Processing 1144 of 1000 on page 2\n",
      "Processing 1145 of 1000 on page 2\n",
      "Processing 1146 of 1000 on page 2\n",
      "Processing 1147 of 1000 on page 2\n",
      "Processing 1148 of 1000 on page 2\n",
      "Processing 1149 of 1000 on page 2\n",
      "Processing 1150 of 1000 on page 2\n",
      "Processing 1151 of 1000 on page 2\n",
      "Processing 1152 of 1000 on page 2\n",
      "Processing 1153 of 1000 on page 2\n",
      "Processing 1154 of 1000 on page 2\n",
      "Processing 1155 of 1000 on page 2\n",
      "Processing 1156 of 1000 on page 2\n",
      "Processing 1157 of 1000 on page 2\n",
      "Processing 1158 of 1000 on page 2\n",
      "Processing 1159 of 1000 on page 2\n",
      "Processing 1160 of 1000 on page 2\n",
      "Processing 1161 of 1000 on page 2\n",
      "Processing 1162 of 1000 on page 2\n",
      "Processing 1163 of 1000 on page 2\n",
      "Processing 1164 of 1000 on page 2\n",
      "Processing 1165 of 1000 on page 2\n",
      "Processing 1166 of 1000 on page 2\n",
      "Processing 1167 of 1000 on page 2\n",
      "Processing 1168 of 1000 on page 2\n",
      "Processing 1169 of 1000 on page 2\n",
      "Processing 1170 of 1000 on page 2\n",
      "Processing 1171 of 1000 on page 2\n",
      "Processing 1172 of 1000 on page 2\n",
      "Processing 1173 of 1000 on page 2\n",
      "Processing 1174 of 1000 on page 2\n",
      "Processing 1175 of 1000 on page 2\n",
      "Processing 1176 of 1000 on page 2\n",
      "Processing 1177 of 1000 on page 2\n",
      "Processing 1178 of 1000 on page 2\n",
      "Processing 1179 of 1000 on page 2\n",
      "Processing 1180 of 1000 on page 2\n",
      "Processing 1181 of 1000 on page 2\n",
      "Processing 1182 of 1000 on page 2\n",
      "Processing 1183 of 1000 on page 2\n",
      "Processing 1184 of 1000 on page 2\n",
      "Processing 1185 of 1000 on page 2\n",
      "Processing 1186 of 1000 on page 2\n",
      "Processing 1187 of 1000 on page 2\n",
      "Processing 1188 of 1000 on page 2\n",
      "Processing 1189 of 1000 on page 2\n",
      "Processing 1190 of 1000 on page 2\n",
      "Processing 1191 of 1000 on page 2\n",
      "Processing 1192 of 1000 on page 2\n",
      "Processing 1193 of 1000 on page 2\n",
      "Processing 1194 of 1000 on page 2\n",
      "Processing 1195 of 1000 on page 2\n",
      "Processing 1196 of 1000 on page 2\n",
      "Processing 1197 of 1000 on page 2\n",
      "Processing 1198 of 1000 on page 2\n",
      "Processing 1199 of 1000 on page 2\n",
      "Processing 1200 of 1000 on page 2\n",
      "Processing 1201 of 1000 on page 2\n",
      "Processing 1202 of 1000 on page 2\n",
      "Processing 1203 of 1000 on page 2\n",
      "Processing 1204 of 1000 on page 2\n",
      "Processing 1205 of 1000 on page 2\n",
      "Processing 1206 of 1000 on page 2\n",
      "Processing 1207 of 1000 on page 2\n",
      "Processing 1208 of 1000 on page 2\n",
      "Processing 1209 of 1000 on page 2\n",
      "Processing 1210 of 1000 on page 2\n",
      "Processing 1211 of 1000 on page 2\n",
      "Processing 1212 of 1000 on page 2\n",
      "Processing 1213 of 1000 on page 2\n",
      "Processing 1214 of 1000 on page 2\n",
      "Processing 1215 of 1000 on page 2\n",
      "Processing 1216 of 1000 on page 2\n",
      "Processing 1217 of 1000 on page 2\n",
      "Processing 1218 of 1000 on page 2\n",
      "Processing 1219 of 1000 on page 2\n",
      "Processing 1220 of 1000 on page 2\n",
      "Processing 1221 of 1000 on page 2\n",
      "Processing 1222 of 1000 on page 2\n",
      "Processing 1223 of 1000 on page 2\n",
      "Processing 1224 of 1000 on page 2\n",
      "Processing 1225 of 1000 on page 2\n",
      "Processing 1226 of 1000 on page 2\n",
      "Processing 1227 of 1000 on page 2\n",
      "Processing 1228 of 1000 on page 2\n",
      "Processing 1229 of 1000 on page 2\n",
      "Processing 1230 of 1000 on page 2\n",
      "Processing 1231 of 1000 on page 2\n",
      "Processing 1232 of 1000 on page 2\n",
      "Processing 1233 of 1000 on page 2\n",
      "Processing 1234 of 1000 on page 2\n",
      "Processing 1235 of 1000 on page 2\n",
      "Processing 1236 of 1000 on page 2\n",
      "Processing 1237 of 1000 on page 2\n",
      "Processing 1238 of 1000 on page 2\n",
      "Processing 1239 of 1000 on page 2\n",
      "Processing 1240 of 1000 on page 2\n",
      "Processing 1241 of 1000 on page 2\n",
      "Processing 1242 of 1000 on page 2\n",
      "Processing 1243 of 1000 on page 2\n",
      "Processing 1244 of 1000 on page 2\n",
      "Processing 1245 of 1000 on page 2\n",
      "Processing 1246 of 1000 on page 2\n",
      "Processing 1247 of 1000 on page 2\n",
      "Processing 1248 of 1000 on page 2\n",
      "Processing 1249 of 1000 on page 2\n",
      "Processing 1250 of 1000 on page 2\n",
      "Processing 1251 of 1000 on page 2\n",
      "Processing 1252 of 1000 on page 2\n",
      "Processing 1253 of 1000 on page 2\n",
      "Processing 1254 of 1000 on page 2\n",
      "Processing 1255 of 1000 on page 2\n",
      "Processing 1256 of 1000 on page 2\n",
      "Processing 1257 of 1000 on page 2\n",
      "Processing 1258 of 1000 on page 2\n",
      "Processing 1259 of 1000 on page 2\n",
      "Processing 1260 of 1000 on page 2\n",
      "Processing 1261 of 1000 on page 2\n",
      "Processing 1262 of 1000 on page 2\n",
      "Processing 1263 of 1000 on page 2\n",
      "Processing 1264 of 1000 on page 2\n",
      "Processing 1265 of 1000 on page 2\n",
      "Processing 1266 of 1000 on page 2\n",
      "Processing 1267 of 1000 on page 2\n",
      "Processing 1268 of 1000 on page 2\n",
      "Processing 1269 of 1000 on page 2\n",
      "Processing 1270 of 1000 on page 2\n",
      "Processing 1271 of 1000 on page 2\n",
      "Processing 1272 of 1000 on page 2\n",
      "Processing 1273 of 1000 on page 2\n",
      "Processing 1274 of 1000 on page 2\n",
      "Processing 1275 of 1000 on page 2\n",
      "Processing 1276 of 1000 on page 2\n",
      "Processing 1277 of 1000 on page 2\n",
      "Processing 1278 of 1000 on page 2\n",
      "Processing 1279 of 1000 on page 2\n",
      "Processing 1280 of 1000 on page 2\n",
      "Processing 1281 of 1000 on page 2\n",
      "Processing 1282 of 1000 on page 2\n",
      "Processing 1283 of 1000 on page 2\n",
      "Processing 1284 of 1000 on page 2\n",
      "Processing 1285 of 1000 on page 2\n",
      "Processing 1286 of 1000 on page 2\n",
      "Processing 1287 of 1000 on page 2\n",
      "Processing 1288 of 1000 on page 2\n",
      "Processing 1289 of 1000 on page 2\n",
      "Processing 1290 of 1000 on page 2\n",
      "Processing 1291 of 1000 on page 2\n",
      "Processing 1292 of 1000 on page 2\n",
      "Processing 1293 of 1000 on page 2\n",
      "Processing 1294 of 1000 on page 2\n",
      "Processing 1295 of 1000 on page 2\n",
      "Processing 1296 of 1000 on page 2\n",
      "Processing 1297 of 1000 on page 2\n",
      "Processing 1298 of 1000 on page 2\n",
      "Processing 1299 of 1000 on page 2\n",
      "Processing 1300 of 1000 on page 2\n",
      "Processing 1301 of 1000 on page 2\n",
      "Processing 1302 of 1000 on page 2\n",
      "Processing 1303 of 1000 on page 2\n",
      "Processing 1304 of 1000 on page 2\n",
      "Processing 1305 of 1000 on page 2\n",
      "Processing 1306 of 1000 on page 2\n",
      "Processing 1307 of 1000 on page 2\n",
      "Processing 1308 of 1000 on page 2\n",
      "Processing 1309 of 1000 on page 2\n",
      "Processing 1310 of 1000 on page 2\n",
      "Processing 1311 of 1000 on page 2\n",
      "Processing 1312 of 1000 on page 2\n",
      "Processing 1313 of 1000 on page 2\n",
      "Processing 1314 of 1000 on page 2\n",
      "Processing 1315 of 1000 on page 2\n",
      "Processing 1316 of 1000 on page 2\n",
      "Processing 1317 of 1000 on page 2\n",
      "Processing 1318 of 1000 on page 2\n",
      "Processing 1319 of 1000 on page 2\n",
      "Processing 1320 of 1000 on page 2\n",
      "Processing 1321 of 1000 on page 2\n",
      "Processing 1322 of 1000 on page 2\n",
      "Processing 1323 of 1000 on page 2\n",
      "Processing 1324 of 1000 on page 2\n",
      "Processing 1325 of 1000 on page 2\n",
      "Processing 1326 of 1000 on page 2\n",
      "Processing 1327 of 1000 on page 2\n",
      "Processing 1328 of 1000 on page 2\n",
      "Processing 1329 of 1000 on page 2\n",
      "Processing 1330 of 1000 on page 2\n",
      "Processing 1331 of 1000 on page 2\n",
      "Processing 1332 of 1000 on page 2\n",
      "Processing 1333 of 1000 on page 2\n",
      "Processing 1334 of 1000 on page 2\n",
      "Processing 1335 of 1000 on page 2\n",
      "Processing 1336 of 1000 on page 2\n",
      "Processing 1337 of 1000 on page 2\n",
      "Processing 1338 of 1000 on page 2\n",
      "Processing 1339 of 1000 on page 2\n",
      "Processing 1340 of 1000 on page 2\n",
      "Processing 1341 of 1000 on page 2\n",
      "Processing 1342 of 1000 on page 2\n",
      "Processing 1343 of 1000 on page 2\n",
      "Processing 1344 of 1000 on page 2\n",
      "Processing 1345 of 1000 on page 2\n",
      "Processing 1346 of 1000 on page 2\n",
      "Processing 1347 of 1000 on page 2\n",
      "Processing 1348 of 1000 on page 2\n",
      "Processing 1349 of 1000 on page 2\n",
      "Processing 1350 of 1000 on page 2\n",
      "Processing 1351 of 1000 on page 2\n",
      "Processing 1352 of 1000 on page 2\n",
      "Processing 1353 of 1000 on page 2\n",
      "Processing 1354 of 1000 on page 2\n",
      "Processing 1355 of 1000 on page 2\n",
      "Processing 1356 of 1000 on page 2\n",
      "Processing 1357 of 1000 on page 2\n",
      "Processing 1358 of 1000 on page 2\n",
      "Processing 1359 of 1000 on page 2\n",
      "Processing 1360 of 1000 on page 2\n",
      "Processing 1361 of 1000 on page 2\n",
      "Processing 1362 of 1000 on page 2\n",
      "Processing 1363 of 1000 on page 2\n",
      "Processing 1364 of 1000 on page 2\n",
      "Processing 1365 of 1000 on page 2\n",
      "Processing 1366 of 1000 on page 2\n",
      "Processing 1367 of 1000 on page 2\n",
      "Processing 1368 of 1000 on page 2\n",
      "Processing 1369 of 1000 on page 2\n",
      "Processing 1370 of 1000 on page 2\n",
      "Processing 1371 of 1000 on page 2\n",
      "Processing 1372 of 1000 on page 2\n",
      "Processing 1373 of 1000 on page 2\n",
      "Processing 1374 of 1000 on page 2\n",
      "Processing 1375 of 1000 on page 2\n",
      "Processing 1376 of 1000 on page 2\n",
      "Processing 1377 of 1000 on page 2\n",
      "Processing 1378 of 1000 on page 2\n",
      "Processing 1379 of 1000 on page 2\n",
      "Processing 1380 of 1000 on page 2\n",
      "Processing 1381 of 1000 on page 2\n",
      "Processing 1382 of 1000 on page 2\n",
      "Processing 1383 of 1000 on page 2\n",
      "Processing 1384 of 1000 on page 2\n",
      "Processing 1385 of 1000 on page 2\n",
      "Processing 1386 of 1000 on page 2\n",
      "Processing 1387 of 1000 on page 2\n",
      "Processing 1388 of 1000 on page 2\n",
      "Processing 1389 of 1000 on page 2\n",
      "Processing 1390 of 1000 on page 2\n",
      "Processing 1391 of 1000 on page 2\n",
      "Processing 1392 of 1000 on page 2\n",
      "Processing 1393 of 1000 on page 2\n",
      "Processing 1394 of 1000 on page 2\n",
      "Processing 1395 of 1000 on page 2\n",
      "Processing 1396 of 1000 on page 2\n",
      "Processing 1397 of 1000 on page 2\n",
      "Processing 1398 of 1000 on page 2\n",
      "Processing 1399 of 1000 on page 2\n",
      "Processing 1400 of 1000 on page 2\n",
      "Processing 1401 of 1000 on page 2\n",
      "Processing 1402 of 1000 on page 2\n",
      "Processing 1403 of 1000 on page 2\n",
      "Processing 1404 of 1000 on page 2\n",
      "Processing 1405 of 1000 on page 2\n",
      "Processing 1406 of 1000 on page 2\n",
      "Processing 1407 of 1000 on page 2\n",
      "Processing 1408 of 1000 on page 2\n",
      "Processing 1409 of 1000 on page 2\n",
      "Processing 1410 of 1000 on page 2\n",
      "Processing 1411 of 1000 on page 2\n",
      "Processing 1412 of 1000 on page 2\n",
      "Processing 1413 of 1000 on page 2\n",
      "Processing 1414 of 1000 on page 2\n",
      "Processing 1415 of 1000 on page 2\n",
      "Processing 1416 of 1000 on page 2\n",
      "Processing 1417 of 1000 on page 2\n",
      "Processing 1418 of 1000 on page 2\n",
      "Processing 1419 of 1000 on page 2\n",
      "Processing 1420 of 1000 on page 2\n",
      "Processing 1421 of 1000 on page 2\n",
      "Processing 1422 of 1000 on page 2\n",
      "Processing 1423 of 1000 on page 2\n",
      "Processing 1424 of 1000 on page 2\n",
      "Processing 1425 of 1000 on page 2\n",
      "Processing 1426 of 1000 on page 2\n",
      "Processing 1427 of 1000 on page 2\n",
      "Processing 1428 of 1000 on page 2\n",
      "Processing 1429 of 1000 on page 2\n",
      "Processing 1430 of 1000 on page 2\n",
      "Processing 1431 of 1000 on page 2\n",
      "Processing 1432 of 1000 on page 2\n",
      "Processing 1433 of 1000 on page 2\n",
      "Processing 1434 of 1000 on page 2\n",
      "Processing 1435 of 1000 on page 2\n",
      "Processing 1436 of 1000 on page 2\n",
      "Processing 1437 of 1000 on page 2\n",
      "Processing 1438 of 1000 on page 2\n",
      "Processing 1439 of 1000 on page 2\n",
      "Processing 1440 of 1000 on page 2\n",
      "Processing 1441 of 1000 on page 2\n",
      "Processing 1442 of 1000 on page 2\n",
      "Processing 1443 of 1000 on page 2\n",
      "Processing 1444 of 1000 on page 2\n",
      "Processing 1445 of 1000 on page 2\n",
      "Processing 1446 of 1000 on page 2\n",
      "Processing 1447 of 1000 on page 2\n",
      "Processing 1448 of 1000 on page 2\n",
      "Processing 1449 of 1000 on page 2\n",
      "Processing 1450 of 1000 on page 2\n",
      "Processing 1451 of 1000 on page 2\n",
      "Processing 1452 of 1000 on page 2\n",
      "Processing 1453 of 1000 on page 2\n",
      "Processing 1454 of 1000 on page 2\n",
      "Processing 1455 of 1000 on page 2\n",
      "Processing 1456 of 1000 on page 2\n",
      "Processing 1457 of 1000 on page 2\n",
      "Processing 1458 of 1000 on page 2\n",
      "Processing 1459 of 1000 on page 2\n",
      "Processing 1460 of 1000 on page 2\n",
      "Processing 1461 of 1000 on page 2\n",
      "Processing 1462 of 1000 on page 2\n",
      "Processing 1463 of 1000 on page 2\n",
      "Processing 1464 of 1000 on page 2\n",
      "Processing 1465 of 1000 on page 2\n",
      "Processing 1466 of 1000 on page 2\n",
      "Processing 1467 of 1000 on page 2\n",
      "Processing 1468 of 1000 on page 2\n",
      "Processing 1469 of 1000 on page 2\n",
      "Processing 1470 of 1000 on page 2\n",
      "Processing 1471 of 1000 on page 2\n",
      "Processing 1472 of 1000 on page 2\n",
      "Processing 1473 of 1000 on page 2\n",
      "Processing 1474 of 1000 on page 2\n",
      "Processing 1475 of 1000 on page 2\n",
      "Processing 1476 of 1000 on page 2\n",
      "Processing 1477 of 1000 on page 2\n",
      "Processing 1478 of 1000 on page 2\n",
      "Processing 1479 of 1000 on page 2\n",
      "Processing 1480 of 1000 on page 2\n",
      "Processing 1481 of 1000 on page 2\n",
      "Processing 1482 of 1000 on page 2\n",
      "Processing 1483 of 1000 on page 2\n",
      "Processing 1484 of 1000 on page 2\n",
      "Processing 1485 of 1000 on page 2\n",
      "Processing 1486 of 1000 on page 2\n",
      "Processing 1487 of 1000 on page 2\n",
      "Processing 1488 of 1000 on page 2\n",
      "Processing 1489 of 1000 on page 2\n",
      "Processing 1490 of 1000 on page 2\n",
      "Processing 1491 of 1000 on page 2\n",
      "Processing 1492 of 1000 on page 2\n",
      "Processing 1493 of 1000 on page 2\n",
      "Processing 1494 of 1000 on page 2\n",
      "Processing 1495 of 1000 on page 2\n",
      "Processing 1496 of 1000 on page 2\n",
      "Processing 1497 of 1000 on page 2\n",
      "Processing 1498 of 1000 on page 2\n",
      "Processing 1499 of 1000 on page 2\n",
      "Processing 1500 of 1000 on page 2\n",
      "Processing 1501 of 1000 on page 2\n",
      "Processing 1502 of 1000 on page 2\n",
      "Processing 1503 of 1000 on page 2\n",
      "Processing 1504 of 1000 on page 2\n",
      "Processing 1505 of 1000 on page 2\n",
      "Processing 1506 of 1000 on page 2\n",
      "Processing 1507 of 1000 on page 2\n",
      "Processing 1508 of 1000 on page 2\n",
      "Processing 1509 of 1000 on page 2\n",
      "Processing 1510 of 1000 on page 2\n",
      "Processing 1511 of 1000 on page 2\n",
      "Processing 1512 of 1000 on page 2\n",
      "Processing 1513 of 1000 on page 2\n",
      "Processing 1514 of 1000 on page 2\n",
      "Processing 1515 of 1000 on page 2\n",
      "Processing 1516 of 1000 on page 2\n",
      "Processing 1517 of 1000 on page 2\n",
      "Processing 1518 of 1000 on page 2\n",
      "Processing 1519 of 1000 on page 2\n",
      "Processing 1520 of 1000 on page 2\n",
      "Processing 1521 of 1000 on page 2\n",
      "Processing 1522 of 1000 on page 2\n",
      "Processing 1523 of 1000 on page 2\n",
      "Processing 1524 of 1000 on page 2\n",
      "Processing 1525 of 1000 on page 2\n",
      "Processing 1526 of 1000 on page 2\n",
      "Processing 1527 of 1000 on page 2\n",
      "Processing 1528 of 1000 on page 2\n",
      "Processing 1529 of 1000 on page 2\n",
      "Processing 1530 of 1000 on page 2\n",
      "Processing 1531 of 1000 on page 2\n",
      "Processing 1532 of 1000 on page 2\n",
      "Processing 1533 of 1000 on page 2\n",
      "Processing 1534 of 1000 on page 2\n",
      "Processing 1535 of 1000 on page 2\n",
      "Processing 1536 of 1000 on page 2\n",
      "Processing 1537 of 1000 on page 2\n",
      "Processing 1538 of 1000 on page 2\n",
      "Processing 1539 of 1000 on page 2\n",
      "Processing 1540 of 1000 on page 2\n",
      "Processing 1541 of 1000 on page 2\n",
      "Processing 1542 of 1000 on page 2\n",
      "Processing 1543 of 1000 on page 2\n",
      "Processing 1544 of 1000 on page 2\n",
      "Processing 1545 of 1000 on page 2\n",
      "Processing 1546 of 1000 on page 2\n",
      "Processing 1547 of 1000 on page 2\n",
      "Processing 1548 of 1000 on page 2\n",
      "Processing 1549 of 1000 on page 2\n",
      "Processing 1550 of 1000 on page 2\n",
      "Processing 1551 of 1000 on page 2\n",
      "Processing 1552 of 1000 on page 2\n",
      "Processing 1553 of 1000 on page 2\n",
      "Processing 1554 of 1000 on page 2\n",
      "Processing 1555 of 1000 on page 2\n",
      "Processing 1556 of 1000 on page 2\n",
      "Processing 1557 of 1000 on page 2\n",
      "Processing 1558 of 1000 on page 2\n",
      "Processing 1559 of 1000 on page 2\n",
      "Processing 1560 of 1000 on page 2\n",
      "Processing 1561 of 1000 on page 2\n",
      "Processing 1562 of 1000 on page 2\n",
      "Processing 1563 of 1000 on page 2\n",
      "Processing 1564 of 1000 on page 2\n",
      "Processing 1565 of 1000 on page 2\n",
      "Processing 1566 of 1000 on page 2\n",
      "Processing 1567 of 1000 on page 2\n",
      "Processing 1568 of 1000 on page 2\n",
      "Processing 1569 of 1000 on page 2\n",
      "Processing 1570 of 1000 on page 2\n",
      "Processing 1571 of 1000 on page 2\n",
      "Processing 1572 of 1000 on page 2\n",
      "Processing 1573 of 1000 on page 2\n",
      "Processing 1574 of 1000 on page 2\n",
      "Processing 1575 of 1000 on page 2\n",
      "Processing 1576 of 1000 on page 2\n",
      "Processing 1577 of 1000 on page 2\n",
      "Processing 1578 of 1000 on page 2\n",
      "Processing 1579 of 1000 on page 2\n",
      "Processing 1580 of 1000 on page 2\n",
      "Processing 1581 of 1000 on page 2\n",
      "Processing 1582 of 1000 on page 2\n",
      "Processing 1583 of 1000 on page 2\n",
      "Processing 1584 of 1000 on page 2\n",
      "Processing 1585 of 1000 on page 2\n",
      "Processing 1586 of 1000 on page 2\n",
      "Processing 1587 of 1000 on page 2\n",
      "Processing 1588 of 1000 on page 2\n",
      "Processing 1589 of 1000 on page 2\n",
      "Processing 1590 of 1000 on page 2\n",
      "Processing 1591 of 1000 on page 2\n",
      "Processing 1592 of 1000 on page 2\n",
      "Processing 1593 of 1000 on page 2\n",
      "Processing 1594 of 1000 on page 2\n",
      "Processing 1595 of 1000 on page 2\n",
      "Processing 1596 of 1000 on page 2\n",
      "Processing 1597 of 1000 on page 2\n",
      "Processing 1598 of 1000 on page 2\n",
      "Processing 1599 of 1000 on page 2\n",
      "Processing 1600 of 1000 on page 2\n",
      "Processing 1601 of 1000 on page 2\n",
      "Processing 1602 of 1000 on page 2\n",
      "Processing 1603 of 1000 on page 2\n",
      "Processing 1604 of 1000 on page 2\n",
      "Processing 1605 of 1000 on page 2\n",
      "Processing 1606 of 1000 on page 2\n",
      "Processing 1607 of 1000 on page 2\n",
      "Processing 1608 of 1000 on page 2\n",
      "Processing 1609 of 1000 on page 2\n",
      "Processing 1610 of 1000 on page 2\n",
      "Processing 1611 of 1000 on page 2\n",
      "Processing 1612 of 1000 on page 2\n",
      "Processing 1613 of 1000 on page 2\n",
      "Processing 1614 of 1000 on page 2\n",
      "Processing 1615 of 1000 on page 2\n",
      "Processing 1616 of 1000 on page 2\n",
      "Processing 1617 of 1000 on page 2\n",
      "Processing 1618 of 1000 on page 2\n",
      "Processing 1619 of 1000 on page 2\n",
      "Processing 1620 of 1000 on page 2\n",
      "Processing 1621 of 1000 on page 2\n",
      "Processing 1622 of 1000 on page 2\n",
      "Processing 1623 of 1000 on page 2\n",
      "Processing 1624 of 1000 on page 2\n",
      "Processing 1625 of 1000 on page 2\n",
      "Processing 1626 of 1000 on page 2\n",
      "Processing 1627 of 1000 on page 2\n",
      "Processing 1628 of 1000 on page 2\n",
      "Processing 1629 of 1000 on page 2\n",
      "Processing 1630 of 1000 on page 2\n",
      "Processing 1631 of 1000 on page 2\n",
      "Processing 1632 of 1000 on page 2\n",
      "Processing 1633 of 1000 on page 2\n",
      "Processing 1634 of 1000 on page 2\n",
      "Processing 1635 of 1000 on page 2\n",
      "Processing 1636 of 1000 on page 2\n",
      "Processing 1637 of 1000 on page 2\n",
      "Processing 1638 of 1000 on page 2\n",
      "Processing 1639 of 1000 on page 2\n",
      "Processing 1640 of 1000 on page 2\n",
      "Processing 1641 of 1000 on page 2\n",
      "Processing 1642 of 1000 on page 2\n",
      "Processing 1643 of 1000 on page 2\n",
      "Processing 1644 of 1000 on page 2\n",
      "Processing 1645 of 1000 on page 2\n",
      "Processing 1646 of 1000 on page 2\n",
      "Processing 1647 of 1000 on page 2\n",
      "Processing 1648 of 1000 on page 2\n",
      "Processing 1649 of 1000 on page 2\n",
      "Processing 1650 of 1000 on page 2\n",
      "Processing 1651 of 1000 on page 2\n",
      "Processing 1652 of 1000 on page 2\n",
      "Processing 1653 of 1000 on page 2\n",
      "Processing 1654 of 1000 on page 2\n",
      "Processing 1655 of 1000 on page 2\n",
      "Processing 1656 of 1000 on page 2\n",
      "Processing 1657 of 1000 on page 2\n",
      "Processing 1658 of 1000 on page 2\n",
      "Processing 1659 of 1000 on page 2\n",
      "Processing 1660 of 1000 on page 2\n",
      "Processing 1661 of 1000 on page 2\n",
      "Processing 1662 of 1000 on page 2\n",
      "Processing 1663 of 1000 on page 2\n",
      "Processing 1664 of 1000 on page 2\n",
      "Processing 1665 of 1000 on page 2\n",
      "Processing 1666 of 1000 on page 2\n",
      "Processing 1667 of 1000 on page 2\n",
      "Processing 1668 of 1000 on page 2\n",
      "Processing 1669 of 1000 on page 2\n",
      "Processing 1670 of 1000 on page 2\n",
      "Processing 1671 of 1000 on page 2\n",
      "Processing 1672 of 1000 on page 2\n",
      "Processing 1673 of 1000 on page 2\n",
      "Processing 1674 of 1000 on page 2\n",
      "Processing 1675 of 1000 on page 2\n",
      "Processing 1676 of 1000 on page 2\n",
      "Processing 1677 of 1000 on page 2\n",
      "Processing 1678 of 1000 on page 2\n",
      "Processing 1679 of 1000 on page 2\n",
      "Processing 1680 of 1000 on page 2\n",
      "Processing 1681 of 1000 on page 2\n",
      "Processing 1682 of 1000 on page 2\n",
      "Processing 1683 of 1000 on page 2\n",
      "Processing 1684 of 1000 on page 2\n",
      "Processing 1685 of 1000 on page 2\n",
      "Processing 1686 of 1000 on page 2\n",
      "Processing 1687 of 1000 on page 2\n",
      "Processing 1688 of 1000 on page 2\n",
      "Processing 1689 of 1000 on page 2\n",
      "Processing 1690 of 1000 on page 2\n",
      "Processing 1691 of 1000 on page 2\n",
      "Processing 1692 of 1000 on page 2\n",
      "Processing 1693 of 1000 on page 2\n",
      "Processing 1694 of 1000 on page 2\n",
      "Processing 1695 of 1000 on page 2\n",
      "Processing 1696 of 1000 on page 2\n",
      "Processing 1697 of 1000 on page 2\n",
      "Processing 1698 of 1000 on page 2\n",
      "Processing 1699 of 1000 on page 2\n",
      "Processing 1700 of 1000 on page 2\n",
      "Processing 1701 of 1000 on page 2\n",
      "Processing 1702 of 1000 on page 2\n",
      "Processing 1703 of 1000 on page 2\n",
      "Processing 1704 of 1000 on page 2\n",
      "Processing 1705 of 1000 on page 2\n",
      "Processing 1706 of 1000 on page 2\n",
      "Processing 1707 of 1000 on page 2\n",
      "Processing 1708 of 1000 on page 2\n",
      "Processing 1709 of 1000 on page 2\n",
      "Processing 1710 of 1000 on page 2\n",
      "Processing 1711 of 1000 on page 2\n",
      "Processing 1712 of 1000 on page 2\n",
      "Processing 1713 of 1000 on page 2\n",
      "Processing 1714 of 1000 on page 2\n",
      "Processing 1715 of 1000 on page 2\n",
      "Processing 1716 of 1000 on page 2\n",
      "Processing 1717 of 1000 on page 2\n",
      "Processing 1718 of 1000 on page 2\n",
      "Processing 1719 of 1000 on page 2\n",
      "Processing 1720 of 1000 on page 2\n",
      "Processing 1721 of 1000 on page 2\n",
      "Processing 1722 of 1000 on page 2\n",
      "Processing 1723 of 1000 on page 2\n",
      "Processing 1724 of 1000 on page 2\n",
      "Processing 1725 of 1000 on page 2\n",
      "Processing 1726 of 1000 on page 2\n",
      "Processing 1727 of 1000 on page 2\n",
      "Processing 1728 of 1000 on page 2\n",
      "Processing 1729 of 1000 on page 2\n",
      "Processing 1730 of 1000 on page 2\n",
      "Processing 1731 of 1000 on page 2\n",
      "Processing 1732 of 1000 on page 2\n",
      "Processing 1733 of 1000 on page 2\n",
      "Processing 1734 of 1000 on page 2\n",
      "Processing 1735 of 1000 on page 2\n",
      "Processing 1736 of 1000 on page 2\n",
      "Processing 1737 of 1000 on page 2\n",
      "Processing 1738 of 1000 on page 2\n",
      "Processing 1739 of 1000 on page 2\n",
      "Processing 1740 of 1000 on page 2\n",
      "Processing 1741 of 1000 on page 2\n",
      "Processing 1742 of 1000 on page 2\n",
      "Processing 1743 of 1000 on page 2\n",
      "Processing 1744 of 1000 on page 2\n",
      "Processing 1745 of 1000 on page 2\n",
      "Processing 1746 of 1000 on page 2\n",
      "Processing 1747 of 1000 on page 2\n",
      "Processing 1748 of 1000 on page 2\n",
      "Processing 1749 of 1000 on page 2\n",
      "Processing 1750 of 1000 on page 2\n",
      "Processing 1751 of 1000 on page 2\n",
      "Processing 1752 of 1000 on page 2\n",
      "Processing 1753 of 1000 on page 2\n",
      "Processing 1754 of 1000 on page 2\n",
      "Processing 1755 of 1000 on page 2\n",
      "Processing 1756 of 1000 on page 2\n",
      "Processing 1757 of 1000 on page 2\n",
      "Processing 1758 of 1000 on page 2\n",
      "Processing 1759 of 1000 on page 2\n",
      "Processing 1760 of 1000 on page 2\n",
      "Processing 1761 of 1000 on page 2\n",
      "Processing 1762 of 1000 on page 2\n",
      "Processing 1763 of 1000 on page 2\n",
      "Processing 1764 of 1000 on page 2\n",
      "Processing 1765 of 1000 on page 2\n",
      "Processing 1766 of 1000 on page 2\n",
      "Processing 1767 of 1000 on page 2\n",
      "Processing 1768 of 1000 on page 2\n",
      "Processing 1769 of 1000 on page 2\n",
      "Processing 1770 of 1000 on page 2\n",
      "Processing 1771 of 1000 on page 2\n",
      "Processing 1772 of 1000 on page 2\n",
      "Processing 1773 of 1000 on page 2\n",
      "Processing 1774 of 1000 on page 2\n",
      "Processing 1775 of 1000 on page 2\n",
      "Processing 1776 of 1000 on page 2\n",
      "Processing 1777 of 1000 on page 2\n",
      "Processing 1778 of 1000 on page 2\n",
      "Processing 1779 of 1000 on page 2\n",
      "Processing 1780 of 1000 on page 2\n",
      "Processing 1781 of 1000 on page 2\n",
      "Processing 1782 of 1000 on page 2\n",
      "Processing 1783 of 1000 on page 2\n",
      "Processing 1784 of 1000 on page 2\n",
      "Processing 1785 of 1000 on page 2\n",
      "Processing 1786 of 1000 on page 2\n",
      "Processing 1787 of 1000 on page 2\n",
      "Processing 1788 of 1000 on page 2\n",
      "Processing 1789 of 1000 on page 2\n",
      "Processing 1790 of 1000 on page 2\n",
      "Processing 1791 of 1000 on page 2\n",
      "Processing 1792 of 1000 on page 2\n",
      "Processing 1793 of 1000 on page 2\n",
      "Processing 1794 of 1000 on page 2\n",
      "Processing 1795 of 1000 on page 2\n",
      "Processing 1796 of 1000 on page 2\n",
      "Processing 1797 of 1000 on page 2\n",
      "Processing 1798 of 1000 on page 2\n",
      "Processing 1799 of 1000 on page 2\n",
      "Processing 1800 of 1000 on page 2\n",
      "Processing 1801 of 1000 on page 2\n",
      "Processing 1802 of 1000 on page 2\n",
      "Processing 1803 of 1000 on page 2\n",
      "Processing 1804 of 1000 on page 2\n",
      "Processing 1805 of 1000 on page 2\n",
      "Processing 1806 of 1000 on page 2\n",
      "Processing 1807 of 1000 on page 2\n",
      "Processing 1808 of 1000 on page 2\n",
      "Processing 1809 of 1000 on page 2\n",
      "Processing 1810 of 1000 on page 2\n",
      "Processing 1811 of 1000 on page 2\n",
      "Processing 1812 of 1000 on page 2\n",
      "Processing 1813 of 1000 on page 2\n",
      "Processing 1814 of 1000 on page 2\n",
      "Processing 1815 of 1000 on page 2\n",
      "Processing 1816 of 1000 on page 2\n",
      "Processing 1817 of 1000 on page 2\n",
      "Processing 1818 of 1000 on page 2\n",
      "Processing 1819 of 1000 on page 2\n",
      "Processing 1820 of 1000 on page 2\n",
      "Processing 1821 of 1000 on page 2\n",
      "Processing 1822 of 1000 on page 2\n",
      "Processing 1823 of 1000 on page 2\n",
      "Processing 1824 of 1000 on page 2\n",
      "Processing 1825 of 1000 on page 2\n",
      "Processing 1826 of 1000 on page 2\n",
      "Processing 1827 of 1000 on page 2\n",
      "Processing 1828 of 1000 on page 2\n",
      "Processing 1829 of 1000 on page 2\n",
      "Processing 1830 of 1000 on page 2\n",
      "Processing 1831 of 1000 on page 2\n",
      "Processing 1832 of 1000 on page 2\n",
      "Processing 1833 of 1000 on page 2\n",
      "Processing 1834 of 1000 on page 2\n",
      "Processing 1835 of 1000 on page 2\n",
      "Processing 1836 of 1000 on page 2\n",
      "Processing 1837 of 1000 on page 2\n",
      "Processing 1838 of 1000 on page 2\n",
      "Processing 1839 of 1000 on page 2\n",
      "Processing 1840 of 1000 on page 2\n",
      "Processing 1841 of 1000 on page 2\n",
      "Processing 1842 of 1000 on page 2\n",
      "Processing 1843 of 1000 on page 2\n",
      "Processing 1844 of 1000 on page 2\n",
      "Processing 1845 of 1000 on page 2\n",
      "Processing 1846 of 1000 on page 2\n",
      "Processing 1847 of 1000 on page 2\n",
      "Processing 1848 of 1000 on page 2\n",
      "Processing 1849 of 1000 on page 2\n",
      "Processing 1850 of 1000 on page 2\n",
      "Processing 1851 of 1000 on page 2\n",
      "Processing 1852 of 1000 on page 2\n",
      "Processing 1853 of 1000 on page 2\n",
      "Processing 1854 of 1000 on page 2\n",
      "Processing 1855 of 1000 on page 2\n",
      "Processing 1856 of 1000 on page 2\n",
      "Processing 1857 of 1000 on page 2\n",
      "Processing 1858 of 1000 on page 2\n",
      "Processing 1859 of 1000 on page 2\n",
      "Processing 1860 of 1000 on page 2\n",
      "Processing 1861 of 1000 on page 2\n",
      "Processing 1862 of 1000 on page 2\n",
      "Processing 1863 of 1000 on page 2\n",
      "Processing 1864 of 1000 on page 2\n",
      "Processing 1865 of 1000 on page 2\n",
      "Processing 1866 of 1000 on page 2\n",
      "Processing 1867 of 1000 on page 2\n",
      "Processing 1868 of 1000 on page 2\n",
      "Processing 1869 of 1000 on page 2\n",
      "Processing 1870 of 1000 on page 2\n",
      "Processing 1871 of 1000 on page 2\n",
      "Processing 1872 of 1000 on page 2\n",
      "Processing 1873 of 1000 on page 2\n",
      "Processing 1874 of 1000 on page 2\n",
      "Processing 1875 of 1000 on page 2\n",
      "Processing 1876 of 1000 on page 2\n",
      "Processing 1877 of 1000 on page 2\n",
      "Processing 1878 of 1000 on page 2\n",
      "Processing 1879 of 1000 on page 2\n",
      "Processing 1880 of 1000 on page 2\n",
      "Processing 1881 of 1000 on page 2\n",
      "Processing 1882 of 1000 on page 2\n",
      "Processing 1883 of 1000 on page 2\n",
      "Processing 1884 of 1000 on page 2\n",
      "Processing 1885 of 1000 on page 2\n",
      "Processing 1886 of 1000 on page 2\n",
      "Processing 1887 of 1000 on page 2\n",
      "Processing 1888 of 1000 on page 2\n",
      "Processing 1889 of 1000 on page 2\n",
      "Processing 1890 of 1000 on page 2\n",
      "Processing 1891 of 1000 on page 2\n",
      "Processing 1892 of 1000 on page 2\n",
      "Processing 1893 of 1000 on page 2\n",
      "Processing 1894 of 1000 on page 2\n",
      "Processing 1895 of 1000 on page 2\n",
      "Processing 1896 of 1000 on page 2\n",
      "Processing 1897 of 1000 on page 2\n",
      "Processing 1898 of 1000 on page 2\n",
      "Processing 1899 of 1000 on page 2\n",
      "Processing 1900 of 1000 on page 2\n",
      "Processing 1901 of 1000 on page 2\n",
      "Processing 1902 of 1000 on page 2\n",
      "Processing 1903 of 1000 on page 2\n",
      "Processing 1904 of 1000 on page 2\n",
      "Processing 1905 of 1000 on page 2\n",
      "Processing 1906 of 1000 on page 2\n",
      "Processing 1907 of 1000 on page 2\n",
      "Processing 1908 of 1000 on page 2\n",
      "Processing 1909 of 1000 on page 2\n",
      "Processing 1910 of 1000 on page 2\n",
      "Processing 1911 of 1000 on page 2\n",
      "Processing 1912 of 1000 on page 2\n",
      "Processing 1913 of 1000 on page 2\n",
      "Processing 1914 of 1000 on page 2\n",
      "Processing 1915 of 1000 on page 2\n",
      "Processing 1916 of 1000 on page 2\n",
      "Processing 1917 of 1000 on page 2\n",
      "Processing 1918 of 1000 on page 2\n",
      "Processing 1919 of 1000 on page 2\n",
      "Processing 1920 of 1000 on page 2\n",
      "Processing 1921 of 1000 on page 2\n",
      "Processing 1922 of 1000 on page 2\n",
      "Processing 1923 of 1000 on page 2\n",
      "Processing 1924 of 1000 on page 2\n",
      "Processing 1925 of 1000 on page 2\n",
      "Processing 1926 of 1000 on page 2\n",
      "Processing 1927 of 1000 on page 2\n",
      "Processing 1928 of 1000 on page 2\n",
      "Processing 1929 of 1000 on page 2\n",
      "Processing 1930 of 1000 on page 2\n",
      "Processing 1931 of 1000 on page 2\n",
      "Processing 1932 of 1000 on page 2\n",
      "Processing 1933 of 1000 on page 2\n",
      "Processing 1934 of 1000 on page 2\n",
      "Processing 1935 of 1000 on page 2\n",
      "Processing 1936 of 1000 on page 2\n",
      "Processing 1937 of 1000 on page 2\n",
      "Processing 1938 of 1000 on page 2\n",
      "Processing 1939 of 1000 on page 2\n",
      "Processing 1940 of 1000 on page 2\n",
      "Processing 1941 of 1000 on page 2\n",
      "Processing 1942 of 1000 on page 2\n",
      "Processing 1943 of 1000 on page 2\n",
      "Processing 1944 of 1000 on page 2\n",
      "Processing 1945 of 1000 on page 2\n",
      "Processing 1946 of 1000 on page 2\n",
      "Processing 1947 of 1000 on page 2\n",
      "Processing 1948 of 1000 on page 2\n",
      "Processing 1949 of 1000 on page 2\n",
      "Processing 1950 of 1000 on page 2\n",
      "Processing 1951 of 1000 on page 2\n",
      "Processing 1952 of 1000 on page 2\n",
      "Processing 1953 of 1000 on page 2\n",
      "Processing 1954 of 1000 on page 2\n",
      "Processing 1955 of 1000 on page 2\n",
      "Processing 1956 of 1000 on page 2\n",
      "Processing 1957 of 1000 on page 2\n",
      "Processing 1958 of 1000 on page 2\n",
      "Processing 1959 of 1000 on page 2\n",
      "Processing 1960 of 1000 on page 2\n",
      "Processing 1961 of 1000 on page 2\n",
      "Processing 1962 of 1000 on page 2\n",
      "Processing 1963 of 1000 on page 2\n",
      "Processing 1964 of 1000 on page 2\n",
      "Processing 1965 of 1000 on page 2\n",
      "Processing 1966 of 1000 on page 2\n",
      "Processing 1967 of 1000 on page 2\n",
      "Processing 1968 of 1000 on page 2\n",
      "Processing 1969 of 1000 on page 2\n",
      "Processing 1970 of 1000 on page 2\n",
      "Processing 1971 of 1000 on page 2\n",
      "Processing 1972 of 1000 on page 2\n",
      "Processing 1973 of 1000 on page 2\n",
      "Processing 1974 of 1000 on page 2\n",
      "Processing 1975 of 1000 on page 2\n",
      "Processing 1976 of 1000 on page 2\n",
      "Processing 1977 of 1000 on page 2\n",
      "Processing 1978 of 1000 on page 2\n",
      "Processing 1979 of 1000 on page 2\n",
      "Processing 1980 of 1000 on page 2\n",
      "Processing 1981 of 1000 on page 2\n",
      "Processing 1982 of 1000 on page 2\n",
      "Processing 1983 of 1000 on page 2\n",
      "Processing 1984 of 1000 on page 2\n",
      "Processing 1985 of 1000 on page 2\n",
      "Processing 1986 of 1000 on page 2\n",
      "Processing 1987 of 1000 on page 2\n",
      "Processing 1988 of 1000 on page 2\n",
      "Processing 1989 of 1000 on page 2\n",
      "Processing 1990 of 1000 on page 2\n",
      "Processing 1991 of 1000 on page 2\n",
      "Processing 1992 of 1000 on page 2\n",
      "Processing 1993 of 1000 on page 2\n",
      "Processing 1994 of 1000 on page 2\n",
      "Processing 1995 of 1000 on page 2\n",
      "Processing 1996 of 1000 on page 2\n",
      "Processing 1997 of 1000 on page 2\n",
      "Processing 1998 of 1000 on page 2\n",
      "Processing 1999 of 1000 on page 2\n",
      "Processing 2000 of 1000 on page 2\n",
      "Processing 2001 of 153 on page 3\n",
      "Processing 2002 of 153 on page 3\n",
      "Processing 2003 of 153 on page 3\n",
      "Processing 2004 of 153 on page 3\n",
      "Processing 2005 of 153 on page 3\n",
      "Processing 2006 of 153 on page 3\n",
      "Processing 2007 of 153 on page 3\n",
      "Processing 2008 of 153 on page 3\n",
      "Processing 2009 of 153 on page 3\n",
      "Processing 2010 of 153 on page 3\n",
      "Processing 2011 of 153 on page 3\n",
      "Processing 2012 of 153 on page 3\n",
      "Processing 2013 of 153 on page 3\n",
      "Processing 2014 of 153 on page 3\n",
      "Processing 2015 of 153 on page 3\n",
      "Processing 2016 of 153 on page 3\n",
      "Processing 2017 of 153 on page 3\n",
      "Processing 2018 of 153 on page 3\n",
      "Processing 2019 of 153 on page 3\n",
      "Processing 2020 of 153 on page 3\n",
      "Processing 2021 of 153 on page 3\n",
      "Processing 2022 of 153 on page 3\n",
      "Processing 2023 of 153 on page 3\n",
      "Processing 2024 of 153 on page 3\n",
      "Processing 2025 of 153 on page 3\n",
      "Processing 2026 of 153 on page 3\n",
      "Processing 2027 of 153 on page 3\n",
      "Processing 2028 of 153 on page 3\n",
      "Processing 2029 of 153 on page 3\n",
      "Processing 2030 of 153 on page 3\n",
      "Processing 2031 of 153 on page 3\n",
      "Processing 2032 of 153 on page 3\n",
      "Processing 2033 of 153 on page 3\n",
      "Processing 2034 of 153 on page 3\n",
      "Processing 2035 of 153 on page 3\n",
      "Processing 2036 of 153 on page 3\n",
      "Processing 2037 of 153 on page 3\n",
      "Processing 2038 of 153 on page 3\n",
      "Processing 2039 of 153 on page 3\n",
      "Processing 2040 of 153 on page 3\n",
      "Processing 2041 of 153 on page 3\n",
      "Processing 2042 of 153 on page 3\n",
      "Processing 2043 of 153 on page 3\n",
      "Processing 2044 of 153 on page 3\n",
      "Processing 2045 of 153 on page 3\n",
      "Processing 2046 of 153 on page 3\n",
      "Processing 2047 of 153 on page 3\n",
      "Processing 2048 of 153 on page 3\n",
      "Processing 2049 of 153 on page 3\n",
      "Processing 2050 of 153 on page 3\n",
      "Processing 2051 of 153 on page 3\n",
      "Processing 2052 of 153 on page 3\n",
      "Processing 2053 of 153 on page 3\n",
      "Processing 2054 of 153 on page 3\n",
      "Processing 2055 of 153 on page 3\n",
      "Processing 2056 of 153 on page 3\n",
      "Processing 2057 of 153 on page 3\n",
      "Processing 2058 of 153 on page 3\n",
      "Processing 2059 of 153 on page 3\n",
      "Processing 2060 of 153 on page 3\n",
      "Processing 2061 of 153 on page 3\n",
      "Processing 2062 of 153 on page 3\n",
      "Processing 2063 of 153 on page 3\n",
      "Processing 2064 of 153 on page 3\n",
      "Processing 2065 of 153 on page 3\n",
      "Processing 2066 of 153 on page 3\n",
      "Processing 2067 of 153 on page 3\n",
      "Processing 2068 of 153 on page 3\n",
      "Processing 2069 of 153 on page 3\n",
      "Processing 2070 of 153 on page 3\n",
      "Processing 2071 of 153 on page 3\n",
      "Processing 2072 of 153 on page 3\n",
      "Processing 2073 of 153 on page 3\n",
      "Processing 2074 of 153 on page 3\n",
      "Processing 2075 of 153 on page 3\n",
      "Processing 2076 of 153 on page 3\n",
      "Processing 2077 of 153 on page 3\n",
      "Processing 2078 of 153 on page 3\n",
      "Processing 2079 of 153 on page 3\n",
      "Processing 2080 of 153 on page 3\n",
      "Processing 2081 of 153 on page 3\n",
      "Processing 2082 of 153 on page 3\n",
      "Processing 2083 of 153 on page 3\n",
      "Processing 2084 of 153 on page 3\n",
      "Processing 2085 of 153 on page 3\n",
      "Processing 2086 of 153 on page 3\n",
      "Processing 2087 of 153 on page 3\n",
      "Processing 2088 of 153 on page 3\n",
      "Processing 2089 of 153 on page 3\n",
      "Processing 2090 of 153 on page 3\n",
      "Processing 2091 of 153 on page 3\n",
      "Processing 2092 of 153 on page 3\n",
      "Processing 2093 of 153 on page 3\n",
      "Processing 2094 of 153 on page 3\n",
      "Processing 2095 of 153 on page 3\n",
      "Processing 2096 of 153 on page 3\n",
      "Processing 2097 of 153 on page 3\n",
      "Processing 2098 of 153 on page 3\n",
      "Processing 2099 of 153 on page 3\n",
      "Processing 2100 of 153 on page 3\n",
      "Processing 2101 of 153 on page 3\n",
      "Processing 2102 of 153 on page 3\n",
      "Processing 2103 of 153 on page 3\n",
      "Processing 2104 of 153 on page 3\n",
      "Processing 2105 of 153 on page 3\n",
      "Processing 2106 of 153 on page 3\n",
      "Processing 2107 of 153 on page 3\n",
      "Processing 2108 of 153 on page 3\n",
      "Processing 2109 of 153 on page 3\n",
      "Processing 2110 of 153 on page 3\n",
      "Processing 2111 of 153 on page 3\n",
      "Processing 2112 of 153 on page 3\n",
      "Processing 2113 of 153 on page 3\n",
      "Processing 2114 of 153 on page 3\n",
      "Processing 2115 of 153 on page 3\n",
      "Processing 2116 of 153 on page 3\n",
      "Processing 2117 of 153 on page 3\n",
      "Processing 2118 of 153 on page 3\n",
      "Processing 2119 of 153 on page 3\n",
      "Processing 2120 of 153 on page 3\n",
      "Processing 2121 of 153 on page 3\n",
      "Processing 2122 of 153 on page 3\n",
      "Processing 2123 of 153 on page 3\n",
      "Processing 2124 of 153 on page 3\n",
      "Processing 2125 of 153 on page 3\n",
      "Processing 2126 of 153 on page 3\n",
      "Processing 2127 of 153 on page 3\n",
      "Processing 2128 of 153 on page 3\n",
      "Processing 2129 of 153 on page 3\n",
      "Processing 2130 of 153 on page 3\n",
      "Processing 2131 of 153 on page 3\n",
      "Processing 2132 of 153 on page 3\n",
      "Processing 2133 of 153 on page 3\n",
      "Processing 2134 of 153 on page 3\n",
      "Processing 2135 of 153 on page 3\n",
      "Processing 2136 of 153 on page 3\n",
      "Processing 2137 of 153 on page 3\n",
      "Processing 2138 of 153 on page 3\n",
      "Processing 2139 of 153 on page 3\n",
      "Processing 2140 of 153 on page 3\n",
      "Processing 2141 of 153 on page 3\n",
      "Processing 2142 of 153 on page 3\n",
      "Processing 2143 of 153 on page 3\n",
      "Processing 2144 of 153 on page 3\n",
      "Processing 2145 of 153 on page 3\n",
      "Processing 2146 of 153 on page 3\n",
      "Processing 2147 of 153 on page 3\n",
      "Processing 2148 of 153 on page 3\n",
      "Processing 2149 of 153 on page 3\n",
      "Processing 2150 of 153 on page 3\n",
      "Processing 2151 of 153 on page 3\n",
      "Processing 2152 of 153 on page 3\n",
      "Processing 2153 of 153 on page 3\n",
      "usgs_elev_tables load completed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Has appx 2,150 HUCs to process, but this section goes quickly.\n",
    "\n",
    "sf.execute_sql('DROP TABLE IF EXISTS derived.usgs_elev_table;')\n",
    "\n",
    "uet_usecols = ['location_id', 'HydroID', 'dem_adj_elevation', 'nws_lid', 'levpa_id']\n",
    "\n",
    "paginator = S3_CLIENT.get_paginator('list_objects')\n",
    "operation_parameters = {'Bucket': FIM_BUCKET,\n",
    "                        'Prefix': f'{HAND_DATASETS_DPATH}/',\n",
    "                        'Delimiter': '/'}\n",
    "page_iterator = paginator.paginate(**operation_parameters)\n",
    "page_count = 0\n",
    "for page in page_iterator:\n",
    "    \n",
    "    prefix_objects = page['CommonPrefixes']\n",
    "    for i, prefix_obj in enumerate(prefix_objects):\n",
    "        print(f\"Processing {(i+1) + (1000 * page_count)} of\"\n",
    "              f\"{len(prefix_objects) * page_count} on page {page_count + 1} (1000 per page)\")\n",
    "        huc_prefix = prefix_obj.get(\"Prefix\")\n",
    "        usgs_elev_table_key = f'{huc_prefix}usgs_elev_table.csv'\n",
    "        try:\n",
    "            uet = S3_CLIENT.get_object(\n",
    "                Bucket=FIM_BUCKET, \n",
    "                Key=usgs_elev_table_key\n",
    "            )['Body']\n",
    "            uet_df = pd.read_csv(uet, header=0, usecols=uet_usecols)\n",
    "            uet_df['fim_version'] = PUBLIC_FIM_VERSION\n",
    "            uet_df[COLUMN_NAME_MODEL_VERSION] = FIM_MODEL_VERSION\n",
    "            uet_df.to_sql(\n",
    "                con=VIZ_DB_ENGINE,\n",
    "                dtype={\n",
    "                    \"location_id\": Text(),\n",
    "                    \"nws_data_huc\": Text()\n",
    "                },\n",
    "                schema='derived',\n",
    "                name='usgs_elev_table',\n",
    "                index=False, \n",
    "                if_exists='append'\n",
    "            )\n",
    "        except Exception as e:\n",
    "            if \"NoSuchKey\" in str(e):\n",
    "                pass\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "    page_count += 1\n",
    "                                     \n",
    "                \n",
    "print(\"usgs_elev_tables load completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9951d778-5d75-4550-a63d-1dee833417f9",
   "metadata": {},
   "source": [
    "<h2>8 - Recreate derived.hydrotable_staggered</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33026eb6-fbc7-47f7-a44b-f9cea8f0911b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hydrotable reloaded - started\n",
      "Processing 1 of 0 on page 1 (1000 per page)\n",
      "Processing 1002 of 1000 on page 2 (1000 per page)\n",
      "Processing 2003 of 2000 on page 3 (1000 per page)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 46\u001b[0m\n\u001b[1;32m     41\u001b[0m ht \u001b[38;5;241m=\u001b[39m S3_CLIENT\u001b[38;5;241m.\u001b[39mget_object(\n\u001b[1;32m     42\u001b[0m     Bucket\u001b[38;5;241m=\u001b[39mFIM_BUCKET,\n\u001b[1;32m     43\u001b[0m     Key\u001b[38;5;241m=\u001b[39mhydro_table_key\n\u001b[1;32m     44\u001b[0m )[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBody\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# print(\"...Reading with pandas...\")\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m ht_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mht\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mht_usecols\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# print('...Writing to db...')\u001b[39;00m\n\u001b[1;32m     48\u001b[0m ht_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfim_version\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m PUBLIC_FIM_VERSION\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/io/parsers/readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m     (\n\u001b[1;32m   1920\u001b[0m         index,\n\u001b[1;32m   1921\u001b[0m         columns,\n\u001b[1;32m   1922\u001b[0m         col_dict,\n\u001b[0;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2053\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/botocore/response.py:99\u001b[0m, in \u001b[0;36mStreamingBody.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Read at most amt bytes from the stream.\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \n\u001b[1;32m     96\u001b[0m \u001b[38;5;124;03mIf the amt argument is omitted, read all data.\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 99\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m URLLib3ReadTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;66;03m# TODO: the url will be None as urllib3 isn't setting it yet\u001b[39;00m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeoutError(endpoint_url\u001b[38;5;241m=\u001b[39me\u001b[38;5;241m.\u001b[39murl, error\u001b[38;5;241m=\u001b[39me)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/urllib3/response.py:949\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[1;32m    947\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[0;32m--> 949\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    951\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/urllib3/response.py:873\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    870\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    872\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 873\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    874\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m    875\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[1;32m    876\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[1;32m    883\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/urllib3/response.py:856\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1()\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/http/client.py:466\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    465\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 466\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/ssl.py:1307\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1304\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1305\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1306\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/ssl.py:1163\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1163\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1165\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Takes appx 5.75 to 6 hrs to run\n",
    "\n",
    "print(\"hydrotable reloaded - started\")\n",
    "start_dt = datetime.now()\n",
    "\n",
    "sf.execute_sql('DROP TABLE IF EXISTS derived.hydrotable;')\n",
    "sql = '''\n",
    "SELECT distinct LPAD(huc8::text, 8, '0') as huc8 FROM derived.featureid_huc_crosswalk WHERE huc8 is not null;\n",
    "'''\n",
    "df = sf.sql_to_dataframe(sql)\n",
    "ht_usecols = ['HydroID', 'feature_id', 'stage', 'discharge_cms']\n",
    "\n",
    "paginator = S3_CLIENT.get_paginator('list_objects')\n",
    "operation_parameters = {'Bucket': FIM_BUCKET,\n",
    "                        'Prefix': f'{HAND_DATASETS_DPATH}/',\n",
    "                        'Delimiter': '/'}\n",
    "page_iterator = paginator.paginate(**operation_parameters)\n",
    "page_count = 0\n",
    "for page in page_iterator:\n",
    "\n",
    "    prefix_objects = page['CommonPrefixes']\n",
    "    for i, prefix_obj in enumerate(prefix_objects):\n",
    "        \n",
    "        print(f\"Processing {(i+1) + (1000 * page_count)} of\"\n",
    "              f\"{len(prefix_objects) * page_count} on page {page_count + 1} (1000 per page)\")        \n",
    "        \n",
    "        print(f\"Processing {(i+1) + (1000 * page_count)} of\"\n",
    "              f\" {len(prefix_objects) * page_count} on page {page_count + 1} (1000 per page)\")\n",
    "        branch_prefix = f'{prefix_obj.get(\"Prefix\")}branches/0/'\n",
    "        branch_files_result = S3_CLIENT.list_objects(\n",
    "            Bucket=FIM_BUCKET, \n",
    "            Prefix=branch_prefix, \n",
    "            Delimiter='/'\n",
    "        )\n",
    "        hydro_table_key = None\n",
    "        for content_obj in branch_files_result.get('Contents'):\n",
    "            branch_file_prefix = content_obj['Key']\n",
    "            if 'hydroTable' in branch_file_prefix:\n",
    "                hydro_table_key = branch_file_prefix\n",
    "\n",
    "        if hydro_table_key:\n",
    "            # print(f\"Found usgs_elev_table and hydroTable in {branch_prefix}\")\n",
    "            try:\n",
    "                # print(\"...Fetching csvs...\")\n",
    "                ht = S3_CLIENT.get_object(\n",
    "                    Bucket=FIM_BUCKET,\n",
    "                    Key=hydro_table_key\n",
    "                )['Body']\n",
    "                # print(\"...Reading with pandas...\")\n",
    "                ht_df = pd.read_csv(ht, header=0, usecols=ht_usecols)\n",
    "                # print('...Writing to db...')\n",
    "                ht_df['fim_version'] = PUBLIC_FIM_VERSION\n",
    "                ht_df[COLUMN_NAME_MODEL_VERSION] = FIM_MODEL_VERSION\n",
    "                ht_df.to_sql(\n",
    "                    con=VIZ_DB_ENGINE, \n",
    "                    schema='derived',\n",
    "                    name='hydrotable',\n",
    "                    index=False,\n",
    "                    if_exists='append'\n",
    "                )\n",
    "            except Exception as e:\n",
    "                raise e\n",
    "                print(f'Fetch failed: {e}')\n",
    "                \n",
    "        page_count += 1\n",
    "                \n",
    "                \n",
    "end_dt = datetime.now()\n",
    "time_duration = end_dt - start_dt\n",
    "print(\"hydrotable reload done\")\n",
    "print(f\"... duration was  {str(time_duration).split('.')[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc764e4-7e67-4b51-a100-6021e31416cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"hydrotable_staggered started\")\n",
    "\n",
    "start_dt = datetime.now()\n",
    "\n",
    "sql = '''\n",
    "DROP TABLE IF EXISTS derived.hydrotable_staggered;\n",
    "SELECT\n",
    "    et.location_id,\n",
    "    ht.feature_id,\n",
    "    (stage + et.dem_adj_elevation) * 3.28084 as elevation_ft,\n",
    "    LEAD((stage + et.dem_adj_elevation) * 3.28084) OVER (PARTITION BY ht.feature_id ORDER BY ht.feature_id, stage) as next_elevation_ft,\n",
    "    discharge_cms * 35.3147 as discharge_cfs,\n",
    "    LEAD(discharge_cms * 35.3147) OVER (PARTITION BY ht.feature_id ORDER BY ht.feature_id, stage) as next_discharge_cfs\n",
    "INTO derived.hydrotable_staggered\n",
    "FROM derived.hydrotable AS ht\n",
    "JOIN derived.usgs_elev_table AS et ON ht.\"HydroID\" = et.\"HydroID\" AND et.location_id IS NOT NULL;\n",
    "'''\n",
    "sf.execute_sql(sql)\n",
    "\n",
    "print(\"hydrotable_staggered reload done\")\n",
    "end_dt = datetime.now()\n",
    "time_duration = end_dt - start_dt\n",
    "print(f\"... duration was  {str(time_duration).split('.')[0]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f21c03-ca36-402c-a69b-396162720f66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# we don't need the hydrotable anymore as it has been reloaded and adjusted above in hydrotable_staggered\n",
    "sf.execute_sql('DROP TABLE IF EXISTS derived.hydrotable;')\n",
    "print(\"Done dropping derived.hydrotable, post hydrotable_staggered load\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16ad30a-f0d6-4fba-8a9f-059d3d11ff6a",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2>9 - Recreate derived.usgs_rating_curves_staggered</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461f20b4-8442-4c0e-9114-ea62764cd789",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Aug 16, 2024 - done for 4.4.0.0 (4.5.2.11)\n",
    "\n",
    "# TODO: Aug 2024: Change this to a backup without indexes and not rename\n",
    "# Aug 27, 2024: This needs to be redone so we don't rename tables, it messes up indexes and index names when we use _to_sql commands later\n",
    "\n",
    "# sf.execute_sql(f'ALTER TABLE IF EXISTS derived.usgs_rating_curves RENAME TO usgs_rating_curves_{OLD_FIM_TAG};')\n",
    "# sf.execute_sql(f'ALTER TABLE IF EXISTS derived.usgs_rating_curves_staggered RENAME TO usgs_rating_curves_staggered_{OLD_FIM_TAG};')\n",
    "# print(\"usgs rating curve tables renamed and cleaned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b752fce5-c8d7-4064-92d7-842c22d1723e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "sql = '''\n",
    "    DROP TABLE IF EXISTS derived.usgs_rating_curves;\n",
    "    DROP TABLE IF EXISTS derived.usgs_rating_curves_staggered;\n",
    "'''\n",
    "sf.execute_sql(sql)\n",
    "\n",
    "print(\"Done dropping usgs_rating_curves and usgs_rating_curves_staggered\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a0cc18-de82-4bd8-bd4d-c3ad021a1dec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run the script to load the usgs_rating_curve.csv. Exact duration not yet known. Appx 30 min (??)\n",
    "\n",
    "start_dt = datetime.now()\n",
    "event = {\n",
    "    'target_table': 'derived.usgs_rating_curves',\n",
    "    'target_cols': ['location_id', 'flow', 'stage', 'navd88_datum', 'elevation_navd88'],\n",
    "    'file': f'{QA_DATASETS_DPATH}/usgs_rating_curves.csv',\n",
    "    'bucket': FIM_BUCKET,\n",
    "    'reference_time': '2023-08-23 00:00:00',\n",
    "    'keep_flows_at_or_above': 0,\n",
    "    'iteration_index': 0\n",
    "}\n",
    "\n",
    "sf.execute_db_ingest(event, None)\n",
    "\n",
    "print(\"done loading usgs_rating_curves\")\n",
    "end_dt = datetime.now()\n",
    "time_duration = end_dt - start_dt\n",
    "print(f\"... duration was  {str(time_duration).split('.')[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4548d5e8-303e-44e0-a8a5-7b1947214b4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Takes under a minute\n",
    "print(\"Starting usgs_rating_curves_staggered build based on usgs_rating_curve table\")\n",
    "\n",
    "sql = '''\n",
    "SELECT \n",
    "    location_id,\n",
    "    flow as discharge_cfs, \n",
    "    LEAD(flow) OVER (PARTITION BY location_id ORDER BY location_id, stage) as next_discharge_cfs,\n",
    "    stage,\n",
    "    navd88_datum,\n",
    "    elevation_navd88 as elevation_ft,\n",
    "    LEAD(elevation_navd88) OVER (PARTITION BY location_id ORDER BY location_id, stage) as next_elevation_ft\n",
    "INTO derived.usgs_rating_curves_staggered\n",
    "FROM derived.usgs_rating_curves;\n",
    "'''\n",
    "\n",
    "sf.execute_sql(sql)\n",
    "\n",
    "print(\"Done loading usgs_rating_curves_staggered\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a77875-f569-4410-888c-b132714a7c9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# usgs_rating_curves is a temp table and is loaded with some changes into the usgs_rating_curves_staggered\n",
    "sf.execute_sql('DROP TABLE IF EXISTS derived.usgs_rating_curves;')\n",
    "print(\"Done dropping derived.usgs_rating_curves, post loading usgs_rating_curves_staggered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e926c7a-9d1c-41ea-a9b3-5fc688994157",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2>10 - UPDATE SRC SKILL METRICS IN DB</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c472f8-0557-4864-8536-1814f3ac4286",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Already run for 4.4.0.0 (4.5.2.11)\n",
    "\n",
    "'''\n",
    "Be Very Careful to just rename tables. If they have indexes, the index will now point to the new\n",
    "table names but maintain the original index name. Those index names can really mess stuff up.\n",
    "Best to never rename unless you rename indexes as well. This particular on is ok. \n",
    "Note: When various '\"to_sql\" tools are run which have GIST indexes, this index column name issue\n",
    "will be the problem.\n",
    "\n",
    "Why Drop instead of Truncate? if the schema changes for the incoming, truncate will have column\n",
    "missmatches.\n",
    "\n",
    "We really should be backing up indexes and constraints as well.\n",
    "\n",
    "'''\n",
    "\n",
    "# TODO: Aug 2024: Change this away from \"rename\" to copy / drop. \n",
    "# sf.execute_sql(f'ALTER TABLE IF EXISTS derived.src_skill_temp RENAME TO src_skill_temp_{OLD_FIM_TAG};')\n",
    "# sf.execute_sql(f'ALTER TABLE IF EXISTS derived.src_skill RENAME TO src_skill_{OLD_FIM_TAG};')\n",
    "\n",
    "# print(\"src_skill and src_skill_temps db renamed\")\n",
    "\n",
    "\n",
    "# TODO: Rob Aug 2024: change this to backup of table and not rename as it messses with indexes\n",
    "# Don't need a copy of the reference src_skill table , so just drop it.\n",
    "new_table_name = f\"derived.src_skill_temp_{OLD_FIM_TAG}\"\n",
    "sql = f'''\n",
    "   CREATE TABLE IF NOT EXISTS {new_table_name} AS TABLE derived.src_skill_temp;\n",
    "'''\n",
    "\n",
    "\n",
    "#print(\"src_skill and src_skill_temps db renamed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a249854-b9cd-4ccb-afea-96c35708515d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prep the dbs for the new load\n",
    "#sf.execute_sql('DROP TABLE IF EXISTS derived.src_skill_temp;')\n",
    "#sf.execute_sql('DROP TABLE IF EXISTS reference.src_skill;', db_type='egis')\n",
    "#print(\"Done dropping src_skill and src_skill_temp tables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24f509a-90df-492a-a0d4-aa9eab8e7041",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the src_skill_temp table\n",
    "start_dt = datetime.now()\n",
    "\n",
    "event = {\n",
    "    'target_table': 'derived.src_skill_temp',\n",
    "    'target_cols': None,  # This means \"all\"\n",
    "    'file': f'{QA_DATASETS_DPATH}/agg_nwm_recurr_flow_elev_stats_location_id.csv',\n",
    "    'bucket': FIM_BUCKET,\n",
    "    'reference_time': '2023-08-23 00:00:00',\n",
    "    'keep_flows_at_or_above': 0,\n",
    "    'iteration_index': 0,\n",
    "    'db_type': 'viz'\n",
    "}\n",
    "\n",
    "execute_db_ingest(event, None)\n",
    "print(\"Done loading derived.src_skill_temp table\")\n",
    "end_dt = datetime.now()\n",
    "time_duration = end_dt - start_dt\n",
    "print(f\"... duration was  {str(time_duration).split('.')[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5639447-9a5a-4add-9c36-ebd945a5a8d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load into src_skill table adding geometry to it from external.usgs_gage. Yes.. more/less straight from WRDS tables\n",
    "# Some recs appear to be in error in the csv. location id = 394220106431500 (those are dropped below)\n",
    "\n",
    "start_dt = datetime.now()\n",
    "\n",
    "sf.execute_sql('DROP TABLE IF EXISTS derived.src_skill;')\n",
    "\n",
    "sql = f\"\"\"\n",
    "SELECT\n",
    "\t(row_number() OVER ())::int as oid,\n",
    "\tgage.name,\n",
    "\tLPAD(skill.location_id::text, 8, '0') as location_id,\n",
    "\tskill.nrmse,\n",
    "\tskill.mean_abs_y_diff_ft,\n",
    "\tskill.mean_y_diff_ft,\n",
    "\tskill.percent_bias,\n",
    "    '{PUBLIC_FIM_VERSION}' as {COLUMN_NAME_FIM_VERSION},\n",
    "    '{FIM_MODEL_VERSION}' as {COLUMN_NAME_MODEL_VERSION},\n",
    "\tgage.geo_point as geom\n",
    "INTO derived.src_skill\n",
    "FROM derived.src_skill_temp skill\n",
    "JOIN external.usgs_gage AS gage ON LPAD(gage.usgs_gage_id::text, 8, '0') = LPAD(skill.location_id::text, 8, '0')\n",
    "\"\"\"\n",
    "\n",
    "sf.execute_sql(sql)\n",
    "\n",
    "print(\"Done loading derived.src_skill table\")\n",
    "end_dt = datetime.now()\n",
    "time_duration = end_dt - start_dt\n",
    "print(f\"... duration was  {str(time_duration).split('.')[0]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa32cd0-6edc-4441-9be3-f5bcba499ad0",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h4>Then export the derived.src_skill table and import it into the EGIS reference.src_skill table</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df23345-47a4-490a-b10f-2020e3a2a68e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "sf.move_data_from_viz_to_egis(\"derived.src_skill\", \"reference.src_skill\")\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118c126e-00b8-4c7f-8a8d-0bb427f413f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2>11 - UPDATE FIM PERFORMANCE METRICS IN DB</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b291a461-e36f-4049-84b6-dd59cd1c4ad2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Make copies of current dbs for 4.4.0.0 (4.5.2.11)\n",
    "# DONE: for 4.4.0.0 (4.5.2.11)\n",
    "\n",
    "# NOTE: Aug 2024: The problem with not droppign them and rebuilding them with indexes, is that if the table schema\n",
    "# changes it is not reflected\n",
    "\n",
    "\n",
    "# Points\n",
    "new_table_name = f\"reference.fim_performance_points_{OLD_FIM_TAG}\"\n",
    "sql = f'''\n",
    "    CREATE TABLE IF NOT EXISTS {new_table_name} AS TABLE reference.fim_performance_points;\n",
    "'''\n",
    "sf.execute_sql(sql, db_type='egis')\n",
    "print(f\"fim_performance_points copied to {new_table_name} if it does not already exists\")\n",
    "\n",
    "\n",
    "# Catchments\n",
    "new_table_name = f\"reference.fim_performance_catchments_{OLD_FIM_TAG}\"\n",
    "sql = f'''\n",
    "   CREATE TABLE IF NOT EXISTS {new_table_name} AS TABLE reference.fim_performance_catchments;\n",
    "'''\n",
    "sf.execute_sql(sql, db_type='egis')\n",
    "print(f\"fim_performance_catchments copied to {new_table_name} if it does not already exists\")\n",
    "\n",
    "\n",
    "# Polys\n",
    "new_table_name = f\"reference.fim_performance_polys_{OLD_FIM_TAG}\"\n",
    "sql = f'''\n",
    "   CREATE TABLE IF NOT EXISTS {new_table_name} AS TABLE reference.fim_performance_polys;\n",
    "'''\n",
    "sf.execute_sql(sql, db_type='egis')\n",
    "print(f\"fim_performance_polys copied to {new_table_name} if it does not already exists\")\n",
    "\n",
    "print(\"Done making backups of the FIM performance tables\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fcdfab-3936-4a0a-8ab9-605e9a64f9bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# clean up tables for new load\n",
    "\n",
    "# TODO: Aug 2024: Add postgresql if / else. Truncate \"if exists\" doesn't exist. :)\n",
    "\n",
    "table_names = [\n",
    "    \"reference.fim_performance_points\",\n",
    "    \"reference.fim_performance_polys\",\n",
    "    \"reference.fim_performance_catchments\"\n",
    "]\n",
    "\n",
    "for tb_name in table_names:\n",
    "    sql = f\"TRUNCATE TABLE {tb_name}\"\n",
    "#    print(sql)\n",
    "    sf.execute_sql(sql,db_type='egis')\n",
    "\n",
    "\n",
    "print(f\"All fim_performance tables trunated if they exist\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ca5581",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the new fim performance tables\n",
    "\n",
    "start_dt = datetime.now()\n",
    "\n",
    "# os.environ['EGIS_DB_HOST'] =''  #TI DB\n",
    "\n",
    "db_type = \"egis\"\n",
    "db_engine = sf.get_db_engine(db_type)\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Define bucket and parent directories.\n",
    "bucket = \"hydrovis-ti-deployment-us-east-1\"\n",
    "\n",
    "# file_handles = ['fim_performance_points.csv']\n",
    "# file_handles = ['fim_performance_points.csv', 'fim_performance_polys.csv', 'fim_performance_catchments_dissolved.csv']\n",
    "# file_handles = ['fim_performance_points.csv', 'fim_performance_polys.csv']\n",
    "file_handles = ['fim_performance_catchments.csv']\n",
    "\n",
    "for file_handle in file_handles:\n",
    "\n",
    "    print(\"Reading file...\")\n",
    "    # df = pd.read_csv(local_download_path)\n",
    "    file_to_download = f\"{QA_DATASETS_DPATH}/{file_handle}\"\n",
    "    df = s3_sf.download_S3_csv_files_to_df_from_list(FIM_BUCKET, [file_to_download], True)\n",
    "    print(\"File read.\")\n",
    "\n",
    "    # Rename headers.\n",
    "\n",
    "    if file_handle == 'fim_performance_points.csv':\n",
    "        df = df.rename(columns={'Unnamed: 0': 'oid', 'geometry': 'geom'})\n",
    "    else:\n",
    "        df = df.rename(columns={'Unnamed: 0': 'oid', 'geometry': 'geom', 'huc':'huc8'})\n",
    "\n",
    "    print(df.dtypes)\n",
    "    # Convert all field names to lowercase (needed for ArcGIS Pro).\n",
    "    df.columns = df.columns.str.lower()\n",
    "\n",
    "    # Enforce data types on df before loading in DB (TODO: need to create special cases for each layer).\n",
    "    if file_handle == 'fim_performance_points.csv':\n",
    "        df = df.astype({'huc': 'str'})\n",
    "    else:\n",
    "        df = df.astype({'huc8': 'str'})\n",
    "    df = df.fillna(0)\n",
    "    try:\n",
    "        df = df.astype({'feature_id': 'int'})\n",
    "        df = df.astype({'feature_id': 'str'})\n",
    "        df = df.astype({'oid': 'int'})\n",
    "    except KeyError:  # If there is no feature_id field\n",
    "        pass\n",
    "    try:\n",
    "        df = df.astype({'nwm_seg': 'int'})\n",
    "        df = df.astype({'nwm_seg': 'str'})\n",
    "    except KeyError:  # If there is no nwm_seg field\n",
    "        pass\n",
    "    try:\n",
    "        df = df.astype({'usgs_gage': 'int'})\n",
    "        df = df.astype({'usgs_gage': 'str'})\n",
    "    except KeyError:  # If there is no usgs_gage field\n",
    "        pass\n",
    "\n",
    "    # zfill HUC8 field.\n",
    "    if file_handle == 'fim_performance_points.csv':\n",
    "        df['huc'] = df['huc'].apply(lambda x: x.zfill(8))\n",
    "    else:\n",
    "        df['huc8'] = df['huc8'].apply(lambda x: x.zfill(8))\n",
    "\n",
    "    df['version'] = PUBLIC_FIM_VERSION\n",
    "    df[COLUMN_NAME_MODEL_VERSION] = FIM_MODEL_VERSION\n",
    "\n",
    "    # Upload df to database.\n",
    "    stripped_layer_name = file_handle.replace(\".csv\", \"\")\n",
    "    table_name = \"reference.\" + stripped_layer_name\n",
    "    print(\"Loading data into DB...\")\n",
    "\n",
    "    # Chunk load data into DB\n",
    "\n",
    "    if file_handle in ['fim_performance_catchments.csv']:\n",
    "\n",
    "        print(\"Chunk loading...\")\n",
    "        # Create list of df chunks\n",
    "        n = 10000  # chunk row size\n",
    "        list_df = [df[i:i+n] for i in range(0,df.shape[0],n)]\n",
    "        # geometry = 'MULTIPOLYGON'\n",
    "        # Load the first chunk into the DB as a new table\n",
    "        first_chunk_df = list_df[0]\n",
    "        print(first_chunk_df.shape[0])\n",
    "\n",
    "        first_chunk_df.to_sql(\n",
    "            name=stripped_layer_name, \n",
    "            con=db_engine, \n",
    "            schema='reference',\n",
    "            if_exists='replace', \n",
    "            index=False,\n",
    "            dtype={'oid': sqlalchemy.types.Integer(),\n",
    "                   'version': sqlalchemy.types.String(),\n",
    "                   'geom': Geometry('MULTIPOLYGON', srid=3857)\n",
    "                  }\n",
    "        )\n",
    "        # Load remaining chunks into newly created table\n",
    "\n",
    "        for remaining_chunk_df in list_df[1:]:\n",
    "            print(remaining_chunk_df.shape[0])\n",
    "            remaining_chunk_df.to_sql(\n",
    "                name=stripped_layer_name,\n",
    "                con=db_engine,\n",
    "                schema='reference',\n",
    "                if_exists='append',\n",
    "                index=False,\n",
    "                dtype={'oid': sqlalchemy.types.Integer(),\n",
    "                       'version': sqlalchemy.types.String(),\n",
    "                       'geom': Geometry('MULTIPOLYGON', srid=3857)\n",
    "                      }\n",
    "            )\n",
    "    else:\n",
    "        if 'points' in stripped_layer_name: geometry = 'POINT'\n",
    "        if 'polys' in stripped_layer_name: geometry = 'POLYGON'\n",
    "        # print(\"GEOMETRY\")\n",
    "        # print(geometry)\n",
    "        df.to_sql(\n",
    "            name=stripped_layer_name,\n",
    "            con=db_engine,\n",
    "            schema='reference',\n",
    "            if_exists='replace',\n",
    "            index=False,\n",
    "            dtype={'oid': sqlalchemy.types.Integer(),\n",
    "                   'version': sqlalchemy.types.String(),\n",
    "                   'geom': Geometry(geometry, srid=3857)\n",
    "                  }\n",
    "        )\n",
    "\n",
    "    print(f\">>> {file_handle} downloaded and loaded\")\n",
    "\n",
    "    # deleted the downloaded file that was just processed.\n",
    "    # if os.path.exists(local_download_path):\n",
    "\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "end_dt = datetime.now()\n",
    "time_duration = end_dt - start_dt\n",
    "# print(\"All FIM Performance files loaded\")\n",
    "print(f\"... duration was  {str(time_duration).split('.')[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9309a4b-1d94-4af2-90dc-57a08c4add68",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2>12 - CatFIM (Stage-Based and Flow-Based)</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7938fcb2-313c-4cf8-8274-d34d80bdf7bf",
   "metadata": {},
   "source": [
    "<h4>Function to load CatFIM Data (Non Public)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f37c81-d105-4c38-aa49-b4aef40a7543",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "''' Function to load CatFIM data (for any flow / stage / library / sites but non public)'''\n",
    "\n",
    "\n",
    "def load_catfim_table(catfim_type):\n",
    "\n",
    "    '''\n",
    "    Inputs:\n",
    "        - catfim_type: name identififer for the set, such as \"flow_based_catfim\" or \"flow_based_catfim_sites\", etc\n",
    "              Sometimes the file_handle name can be the name of the s3 file (without extension) and/or the table\n",
    "              name.\n",
    "              Options: flow_based_catfim, flow_based_catfim_sites, stage_based_catfim, stage_based_catfim_sites\n",
    "    '''\n",
    "\n",
    "    db_type = \"egis\"\n",
    "    db_engine = sf.get_db_engine(db_type)\n",
    "    src_crs = \"3857\"\n",
    "\n",
    "    # --------------------------------------\n",
    "    # Drop the original Db if already in place\n",
    "    table_name = catfim_type  # yes, dup variable for now\n",
    "\n",
    "    sf.execute_sql(f\"DROP TABLE IF EXISTS reference.{table_name};\", db_type=db_type)\n",
    "    print(f\"Dropping reference.{table_name} table if it existed\")\n",
    "    print(\"\")\n",
    "\n",
    "    # --------------------------------------\n",
    "    # Get the data from S3 and load it into a df\n",
    "    if catfim_type in ['flow_based_catfim', 'stage_based_catfim']:\n",
    "        file_to_download = f\"{QA_DATASETS_DPATH}/{catfim_type}_library.csv\"\n",
    "    else:\n",
    "        file_to_download = f\"{QA_DATASETS_DPATH}/{catfim_type}.csv\"\n",
    "\n",
    "    # print(f\"Downloading {file_to_download} ... \")\n",
    "\n",
    "    df = s3_sf.download_S3_csv_files_to_df_from_list(FIM_BUCKET, [file_to_download], True)\n",
    "    num_recs = len(df)\n",
    "    print(f\"File read. {num_recs} records to load\")\n",
    "\n",
    "    # --------------------------------------\n",
    "    # Adjusting Columns and data\n",
    "    # Rename headers. All files this name\n",
    "    df = df.rename(columns={'Unnamed: 0': 'oid',\n",
    "                            'geometry': 'geom',\n",
    "                            'huc': 'huc8'})\n",
    "\n",
    "    # Convert all field names to lowercase (needed for ArcGIS Pro).\n",
    "    df.columns = df.columns.str.lower()\n",
    "\n",
    "    # Enforce data types on df before loading in DB (TODO: need to create special cases for each layer).\n",
    "    df = df.astype({'huc8': 'str'})\n",
    "    df = df.fillna(0)\n",
    "    try:\n",
    "        df = df.astype({'feature_id': 'int'})\n",
    "        df = df.astype({'feature_id': 'str'})\n",
    "    except KeyError:  # If there is no feature_id field\n",
    "        pass\n",
    "    try:\n",
    "        df = df.astype({'nwm_seg': 'int'})\n",
    "        df = df.astype({'nwm_seg': 'str'})\n",
    "    except KeyError:  # If there is no nwm_seg field\n",
    "        pass\n",
    "    try:\n",
    "        df = df.astype({'usgs_gage': 'int'})\n",
    "        df = df.astype({'usgs_gage': 'str'})\n",
    "    except KeyError:  # If there is no usgs_gage field\n",
    "        pass\n",
    "\n",
    "    # zfill HUC8 field.\n",
    "    df['huc8'] = df['huc8'].apply(lambda x: x.zfill(8))\n",
    "\n",
    "    if '_sites' in catfim_type:\n",
    "        df = df.astype({'nws_data_rfc_forecast_point': 'str'})\n",
    "        df = df.astype({'nws_data_rfc_defined_fcst_point': 'str'})\n",
    "        df = df.astype({'nws_data_riverpoint': 'str'})\n",
    "\n",
    "\n",
    "    # As of Nov 1, 2024: Ignore the incoming \"version\" from dataset\n",
    "    # df['version'] = PUBLIC_FIM_VERSION\n",
    "    df[COLUMN_NAME_FIM_VERSION] = PUBLIC_FIM_VERSION\n",
    "    df[COLUMN_NAME_MODEL_VERSION] = FIM_MODEL_VERSION\n",
    "\n",
    "    # --------------------------------------\n",
    "    # Load to DB\n",
    "    # Chunk load data into DB\n",
    "    if catfim_type in ['flow_based_catfim', 'stage_based_catfim']:\n",
    "\n",
    "        # Create list of df chunks\n",
    "        n = 1000  # chunk row size\n",
    "        print(f\"Chunk loading... into {table_name} -- {n} records at a time\")\n",
    "        print(\"\")\n",
    "        chunk_df = [df[i:i+n] for i in range(0, df.shape[0], n)]\n",
    "\n",
    "        # Load the first chunk into the DB as a new table\n",
    "        first_chunk_df = chunk_df[0]\n",
    "        num_chunks = len(chunk_df)\n",
    "\n",
    "        print(f\" ... loading chunk 1 of {num_chunks}\")\n",
    "\n",
    "        first_chunk_df.to_sql(\n",
    "            name=table_name,\n",
    "            con=db_engine,\n",
    "            schema='reference',\n",
    "            if_exists='replace',\n",
    "            index=False,\n",
    "            dtype={'oid': sqlalchemy.types.Integer(),\n",
    "                   'geom': Geometry('MULTIPOLYGON', srid=src_crs)}\n",
    "        )\n",
    "\n",
    "        # Load remaining chunks into newly created table\n",
    "        ctr = 1  # Already loaded one\n",
    "        for remaining_chunk in chunk_df[1:]:\n",
    "            # print(remaining_chunk.shape[0])\n",
    "            ctr += 1\n",
    "            print(f\" ... loading chunk {ctr} of {num_chunks}\")\n",
    "            remaining_chunk.to_sql(\n",
    "                        name=table_name,\n",
    "                        con=db_engine,\n",
    "                        schema='reference',\n",
    "                        if_exists='append',\n",
    "                        index=False,\n",
    "                        dtype={'oid': sqlalchemy.types.Integer(),\n",
    "                               'geom': Geometry('MULTIPOLYGON', srid=src_crs)\n",
    "                              }\n",
    "                    )\n",
    "        # end for\n",
    "    else:  # sites tables\n",
    "        print(f\"Loading data into {table_name} ...\")\n",
    "\n",
    "        df.to_sql(\n",
    "            name=table_name,\n",
    "            con=db_engine,\n",
    "            schema='reference',\n",
    "            if_exists='replace',\n",
    "            index=False,\n",
    "            dtype={'oid': sqlalchemy.types.Integer(),\n",
    "                   'geom': Geometry('POINT', srid=src_crs)}\n",
    "        )\n",
    "\n",
    "    # This should auto create a gist index against the geometry column\n",
    "    # if that index name already exists, the upload will fail, the index can not pre-exist\n",
    "    # Best to drop the table before loading.\n",
    "\n",
    "    # return\n",
    "\n",
    "print(\"load_catfim_table function loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca12f850-8826-4177-884a-04f461496d7b",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h3>12.a - Backup old DBs and prepare new databases (but not the \"public\" FIM 10/30 db's)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9577edb-4aa6-423b-819e-df8c922c7ec2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This covers both Stage Based and Flow Based (but not the \"public\" catfim db's)\n",
    "\n",
    "# The \"Public\" db backups ana loads are in cells lower (12.d and higher)\n",
    "\n",
    "# DONE for 4.4.0.0.  (4.5.2.11)\n",
    "\n",
    "# # print(\"Starting Data Backups and table drops for stage and flow based catfim\")\n",
    "# db_names = [\"stage_based_catfim\", \"stage_based_catfim_sites\",\n",
    "#             \"flow_based_catfim\", \"flow_based_catfim_sites\"]\n",
    "\n",
    "# for db_name in db_names:\n",
    "#     new_table_name = f\"reference.{db_name}_{OLD_FIM_TAG}\"\n",
    "#     sql = f'''\n",
    "#         CREATE TABLE IF NOT EXISTS {new_table_name} AS TABLE reference.{db_name};\n",
    "#     '''\n",
    "#     sf.execute_sql(sql, db_type='egis')\n",
    "#     print(f\"{db_name} copied to {new_table_name} if it does not already exist\")\n",
    "\n",
    "\n",
    "# Aug 2024: Now we can drop the tables as we don't have any indexes on them at this time other than the gist geom index.\n",
    "# By dropping them, we can auto adjust the tables schema. (don't truncate)\n",
    "\n",
    "# for db_name in db_names:\n",
    "#     sf.execute_sql(f\"DROP TABLE IF EXISTS reference.{db_name};\", db_type='egis')\n",
    "#     print(f\"reference.{db_name} table dropped if it existed\")\n",
    "\n",
    "\n",
    "# print(\"Data Backups of flow based catfim are complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0af91cf-5a87-4c0b-a7ae-54d6809dd152",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h3>12.b - Updated Flow and Stage Based CatFIM Data (Non Public)</h3>\n",
    "\n",
    "<h3>AUG 2024: IMPORTANT NOTE:</h3>\n",
    "The stage based catfim (library) csv has grown to appx 10 GiB. Our current notebook, hv-vpp-ti-viz-notebook only has 15 GiB memory.\n",
    "Running tool can easily overwhelm the notebook server and freeze it up forcing a reboot.\n",
    "Sometimes when the notebook instance comes back up, it no longer has ths swap system in place. You will need most of the memory\n",
    "and some swap to load it.  Keep an eye a \"terminal\" windows and keep entering `free -h` to keep an eye on it's usage.\n",
    "</br>\n",
    "We will need to review to see if we want to:\n",
    "\n",
    "1. Upgrade this notebook server with more memory (and harddrive space would be good)\n",
    "\n",
    "2. Change the load of the catfim library (non sites) data to another system. Maybe we can load it via a lambda to an EC2 or something?\n",
    "\n",
    "3. Get the FIM Team to break it to smaller pieces, but watch carefully for the OID system (unique id for all records)\n",
    "\n",
    "**When you are done running this script, Please restart this kernal as it does not appear to be releasing all memory. (memory leak?)**\n",
    "\n",
    "\n",
    "Also looks like Tyler has some notebooks where he was moving this into a lambda load? We need to look into that\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c55cd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"Starting of CatFIM data\")\n",
    "\n",
    "# catfim_types = ['flow_based_catfim', 'flow_based_catfim_sites']\n",
    "# catfim_types =  ['stage_based_catfim', 'stage_based_catfim_sites']\n",
    "catfim_types = ['stage_based_catfim_sites']\n",
    "# catfim_types = ['stage_based_catfim']\n",
    "\n",
    "start_dt = datetime.now()\n",
    "\n",
    "for catfim_type in catfim_types:\n",
    "    print(f\"Loading {catfim_type} data\")\n",
    "    load_catfim_table(catfim_type)\n",
    "\n",
    "print(\"\")\n",
    "end_dt = datetime.now()\n",
    "time_duration = end_dt - start_dt\n",
    "print(f\"... duration was  {str(time_duration).split('.')[0]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d1a57d-089c-4042-98d8-6893d7a45acf",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h3>12.c - CatFIM Backup old \"public\" FIM 10 / 30 DBs and prepare new databases</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cf108d-7360-48a9-a4a2-81b23b9e51b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This covers ONLY Catfim public FIM 10/30 for both flow based and stage based\n",
    "'''\n",
    "\n",
    "''' DONE for 4.4.0.0.  (4.5.2.11)'''\n",
    "\n",
    "# db_name_appendix = f\"{OLD_FIM_TAG}_fim_10\"\n",
    "\n",
    "# print(\"Starting Data Backups and table drops for stage and flow based PUBLIC catfim\")\n",
    "# # db_names = [\"stage_based_catfim_public\", \"stage_based_catfim_sites_public\",\n",
    "# #              \"flow_based_catfim_public\", \"flow_based_catfim_sites_public\"]\n",
    "\n",
    "# # stage_based_catfim_sites_public didn't exist for fim 10 but should have in TI (does in other enviros likely)\n",
    "# db_names = [\"stage_based_catfim_public\", \n",
    "#              \"flow_based_catfim_public\", \"flow_based_catfim_sites_public\"]\n",
    "\n",
    "# for db_name in db_names:\n",
    "#     new_table_name = f\"reference.{db_name}_{db_name_appendix}\"\n",
    "#     sql = f\"CREATE TABLE IF NOT EXISTS {new_table_name} AS TABLE reference.{db_name}\"\n",
    "#     sf.execute_sql(sql, db_type='egis')\n",
    "#     print(f\"{db_name} copied to {new_table_name} if it does not already exist\")\n",
    "\n",
    "    \n",
    "# # Aug 2024: Now we can drop the tables as we don't have any indexes on them at this time other than the gist geom index.\n",
    "# # By dropping them, we can auto adjust the tables schema. (don't truncate)\n",
    "\n",
    "# for db_name in db_names:\n",
    "#     sf.execute_sql(f\"DROP TABLE IF EXISTS reference.{db_name};\", db_type='egis')\n",
    "#     print(f\"reference.{db_name} table dropped if it existed\")\n",
    "\n",
    "# print(\"Data Backups of flow based catfim are complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb73b36-056f-47b2-90cd-38b26171723a",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h3>12.d - Load CatFIM \"public\" FIM 30 DBs</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd38c00d-22ad-476e-ab05-cf293e5bbc15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"Loading CatFIM Public datasets (FIM 30)\")\n",
    "\n",
    "catfim_types = [\"stage_based_catfim\", \"stage_based_catfim_sites\",\n",
    "                \"flow_based_catfim\", \"flow_based_catfim_sites\"]\n",
    "\n",
    "__public_fim_release = \"fim_30\"  # The new fim public release being loaded (ie. fim_10, fim_30, fim_60..)\n",
    "\n",
    "start_dt = datetime.now()\n",
    "\n",
    "for catfim_type in catfim_types:\n",
    "    print(\"\")\n",
    "    sql = f'''\n",
    "    DROP TABLE IF EXISTS reference.{catfim_type}_public;\n",
    "\n",
    "    SELECT\n",
    "        catfim.*,\n",
    "        '{__public_fim_release}' as public_fim_release\n",
    "    INTO reference.{catfim_type}_public\n",
    "    FROM reference.{catfim_type} as catfim\n",
    "    JOIN reference.public_fim_domain as fim_domain ON ST_Intersects(catfim.geom, fim_domain.geom)\n",
    "    '''\n",
    "    print(sf.execute_sql(sql, db_type='egis'))\n",
    "    print(f\"public {__public_fim_release} data load for {catfim_type} is complete\")\n",
    "\n",
    "# what about indexes again?\n",
    "\n",
    "# for db_name in db_names:\n",
    "#     new_table_name = f\"reference.{db_name}_{db_name_appendix}\"\n",
    "#     sql = f\"CREATE TABLE IF NOT EXISTS {new_table_name} AS TABLE reference.{db_name}\"\n",
    "#     sf.execute_sql(sql, db_type='egis')\n",
    "#     print(f\"{db_name} copied to {new_table_name} if it does not already exist\")\n",
    "\n",
    "print(\"\")\n",
    "end_dt = datetime.now()\n",
    "time_duration = end_dt - start_dt\n",
    "print(f\"... duration was  {str(time_duration).split('.')[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb36b537-f957-457a-9bfa-64d944f85599",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2>13 - Clear the HAND Cache</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d1ab20-4148-4d1d-bbb6-a2c3983ba145",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "TRUNCATE TABLE fim_cache.hand_hydrotable_cached;\n",
    "TRUNCATE TABLE fim_cache.hand_hydrotable_cached_max;\n",
    "TRUNCATE TABLE fim_cache.hand_hydrotable_cached_geo;\n",
    "TRUNCATE TABLE fim_cache.hand_hydrotable_cached_zero_stage;\n",
    "\"\"\"\n",
    "sf.execute_sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166702e8-60d2-485e-a5bc-23b475841e5a",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2>14 - SAVE TO REPO (AND REDEPLOY TO TI WITH NEW VERSION VARIABLE IN TERRAFORM ??)</h2>\n",
    "\n",
    "Oct 21, 2024: We don''t have a system per-say to update for Terraform, but we now have github hooks\n",
    "built right into JupyterHub. We need to figure out how to work with multiple branches and \"getting latest\"\n",
    "but this gives us source control management now.\n",
    "\n",
    "\n",
    "Note from Rob: While, un-elegant, there so much quick evolution here that I recommend we even keep seperate named load scripts in GIT\n",
    "ie) one for FIM Version 4.4.0.0 and one for 4.5.2.11, etc. So many changes for each edition and very fast script changes WIP may \n",
    "make it smarter to keep each script seperately (ie. 4.4.0.0, 4.5.2.11, etc)\n",
    "\n",
    "<h4>Make sure to Publish the changes to git and add a PR</h4>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
