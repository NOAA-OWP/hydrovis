{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68fce701-8ba2-400d-b662-dca6439ef9b0",
   "metadata": {},
   "source": [
    "### Notes - Oct 21, 2024 ###\n",
    "This is a copy from 10.FIM Version 4.5.2.11 which included hand data loads plus ras2fim data. \n",
    "We will remove all ras2fim stuff here knowing that sometimes ras2fim will be uploaded on its own.\n",
    "However.. when ras2fim is loaded, some steps here will need to be re-run. Those steps will be\n",
    "duplicated when we build our next ras2fim load. This hand release does not have a ras2fim update so\n",
    "we will keep the one in place.\n",
    "</br></br>\n",
    "All code in here will be reviewed and adjusted as the loads progress. Consider each step to be\n",
    "WIP until you see a load date below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e1ee2b5-b109-49e8-8255-f06449b44ee6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: geopandas in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.22 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from geopandas) (1.26.4)\n",
      "Requirement already satisfied: pyogrio>=0.7.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from geopandas) (0.10.0)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from geopandas) (24.2)\n",
      "Requirement already satisfied: pandas>=1.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from geopandas) (2.2.3)\n",
      "Requirement already satisfied: pyproj>=3.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from geopandas) (3.7.0)\n",
      "Requirement already satisfied: shapely>=2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from geopandas) (2.0.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas>=1.4.0->geopandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas>=1.4.0->geopandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas>=1.4.0->geopandas) (2024.2)\n",
      "Requirement already satisfied: certifi in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pyogrio>=0.7.2->geopandas) (2024.8.30)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.4.0->geopandas) (1.16.0)\n",
      "Requirement already satisfied: xarray in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (2024.10.0)\n",
      "Requirement already satisfied: numpy>=1.24 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from xarray) (1.26.4)\n",
      "Requirement already satisfied: packaging>=23.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from xarray) (24.2)\n",
      "Requirement already satisfied: pandas>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from xarray) (2.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas>=2.1->xarray) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas>=2.1->xarray) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas>=2.1->xarray) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=2.1->xarray) (1.16.0)\n",
      "Requirement already satisfied: geoalchemy2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (0.16.0)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from geoalchemy2) (2.0.36)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from geoalchemy2) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from SQLAlchemy>=1.4->geoalchemy2) (4.12.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from SQLAlchemy>=1.4->geoalchemy2) (3.1.1)\n",
      "Requirement already satisfied: rioxarray in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (0.18.1)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from rioxarray) (24.2)\n",
      "Requirement already satisfied: rasterio>=1.3.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from rioxarray) (1.4.2)\n",
      "Requirement already satisfied: xarray>=2024.7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from rioxarray) (2024.10.0)\n",
      "Requirement already satisfied: pyproj>=3.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from rioxarray) (3.7.0)\n",
      "Requirement already satisfied: numpy>=1.23 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from rioxarray) (1.26.4)\n",
      "Requirement already satisfied: certifi in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pyproj>=3.3->rioxarray) (2024.8.30)\n",
      "Requirement already satisfied: affine in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from rasterio>=1.3.7->rioxarray) (2.4.0)\n",
      "Requirement already satisfied: attrs in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from rasterio>=1.3.7->rioxarray) (23.2.0)\n",
      "Requirement already satisfied: click>=4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from rasterio>=1.3.7->rioxarray) (8.1.7)\n",
      "Requirement already satisfied: cligj>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from rasterio>=1.3.7->rioxarray) (0.7.2)\n",
      "Requirement already satisfied: click-plugins in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from rasterio>=1.3.7->rioxarray) (1.1.1)\n",
      "Requirement already satisfied: pyparsing in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from rasterio>=1.3.7->rioxarray) (3.2.0)\n",
      "Requirement already satisfied: pandas>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from xarray>=2024.7.0->rioxarray) (2.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas>=2.1->xarray>=2024.7.0->rioxarray) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas>=2.1->xarray>=2024.7.0->rioxarray) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas>=2.1->xarray>=2024.7.0->rioxarray) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=2.1->xarray>=2024.7.0->rioxarray) (1.16.0)\n",
      "Requirement already satisfied: s3fs in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (0.4.2)\n",
      "Requirement already satisfied: botocore>=1.12.91 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from s3fs) (1.35.63)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from s3fs) (2024.10.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore>=1.12.91->s3fs) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore>=1.12.91->s3fs) (2.9.0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore>=1.12.91->s3fs) (2.2.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore>=1.12.91->s3fs) (1.16.0)\n",
      "Requirement already satisfied: python-dotenv in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (1.0.1)\n",
      "*** All loaded ***\n"
     ]
    }
   ],
   "source": [
    "# Cell to manually pip reload a packages that the Jupyter engine not retained\n",
    "# !pip install numpy\n",
    "!pip install geopandas\n",
    "# !pip install pyarrow\n",
    "!pip install xarray\n",
    "!pip install geoalchemy2\n",
    "# !pip install contextily\n",
    "!pip install rioxarray\n",
    "\n",
    "!pip install s3fs # to get a newer version, old one on system\n",
    "!pip install python-dotenv\n",
    "print(\"*** All loaded ***\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b656259",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "option set\n",
      "imports loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import codecs\n",
    "import csv\n",
    "\n",
    "import sys\n",
    "\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import boto3\n",
    "import geopandas as gpd\n",
    "import json\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "import sqlalchemy\n",
    "import xarray as xr\n",
    "\n",
    "from geopandas import GeoDataFrame\n",
    "from io import StringIO\n",
    "from geoalchemy2 import Geometry\n",
    "from shapely import wkt\n",
    "from shapely.geometry import Polygon\n",
    "from sqlalchemy.exc import DataError   # yes, reduntant, fix it later\n",
    "from sqlalchemy.types import Text    # yes, reduntant, fix it later\n",
    "\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "import helper_functions.shared_functions as sf\n",
    "import helper_functions.s3_shared_functions as s3_sf\n",
    "\n",
    "from helper_functions.viz_classes import database\n",
    "\n",
    "pd.set_option(\"max_info_rows\", 1000000) # override  \n",
    "print(\"option set\")\n",
    "\n",
    "print(\"imports loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fae5ca0f-e582-4ff2-89b6-087285433a97",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aws_keys are at /home/ec2-user/SageMaker/AWS_keys.env\n",
      "aws_keys loaded\n"
     ]
    }
   ],
   "source": [
    "# Load AWS Keys\n",
    "from dotenv import load_dotenv\n",
    "aws_keys_path = os.path.join(Path.home(),\"SageMaker\", \"AWS_keys.env\")\n",
    "print(f\"aws_keys are at {aws_keys_path}\")\n",
    "load_dotenv(aws_keys_path)\n",
    "\n",
    "TI_ACCESS_KEY = os.environ['WF_TI_ACCESS_KEY']\n",
    "TI_SECRET_KEY = os.environ['WF_TI_SECRET_KEY']\n",
    "TI_TOKEN = os.environ['WF_TI_TOKEN']\n",
    "\n",
    "# I updated the file but it is not being honored in the enviro values\n",
    "\n",
    "# print(TI_ACCESS_KEY)\n",
    "# print(TI_SECRET_KEY)\n",
    "# print(TI_TOKEN)\n",
    "\n",
    "print(\"aws_keys loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e944eff3-6023-48f4-9f57-27304a240447",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Variables loaded\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# we won't load this into any tables at this time\n",
    "# The phrase of FIM 5.1.0 will be embedded in config files\n",
    "#PU0LIC_FIM_VERSION = \"FIM 5.1.0\"\n",
    "HAND_MODEL_VERSION = \"4.5.11.1\"\n",
    "\n",
    "HAND_ROOT_DPATH = \"fim/hand_4_5_11_1\"\n",
    "HAND_DATASETS_DPATH = f\"{HAND_ROOT_DPATH}/hand_datasets\"\n",
    "QA_DATASETS_DPATH = f\"{HAND_ROOT_DPATH}/qa_datasets\"\n",
    "\n",
    "FIM_BUCKET = \"hydrovis-ti-deployment-us-east-1\"\n",
    "PIPELINE_ARN = 'arn:aws:states:us-east-1:526904826677:stateMachine:hv-vpp-ti-viz-pipeline'\n",
    "\n",
    "COLUMN_NAME_MODEL_VERSION = \"model_version\"\n",
    "\n",
    "# Sometimes these credential values get updated. To find the latest correct values, go to your AWS Console log page and click on the \"Access Key\"\n",
    "# link to get the latest valid set. Using the \"AWS environment variables\" values.\n",
    "# If this is not set correctly, you will get an HTTP error 400 when you call S3 lower.\n",
    "# You might also see an error of 'An error occurred (NoSuchKey) when calling the GetObject operation:\n",
    "# The specified key does not exist.\" the creds are not correct\"\n",
    "\n",
    "S3_CLIENT = boto3.client(\"s3\")\n",
    "STEPFUNCTION_CLIENT = boto3.client('stepfunctions')\n",
    "VIZ_DB_ENGINE = sf.get_db_engine('viz')\n",
    "\n",
    "print(\"Global Variables loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5827eb-3ca3-4b29-8ee7-f7c4f3c2c736",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2>1 - UPLOAD FIM4 HYDRO ID/FEATURE ID CROSSWALK</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96a49f2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "FIM_CROSSWALK_FPATH = os.path.join(HAND_DATASETS_DPATH, \"crosswalk_table.csv\")\n",
    "\n",
    "print(f\"Getting column name from {FIM_CROSSWALK_FPATH}\")\n",
    "\n",
    "data = S3_CLIENT.get_object(Bucket=FIM_BUCKET, Key=FIM_CROSSWALK_FPATH)\n",
    "d_reader = csv.DictReader(codecs.getreader(\"utf-8\")(data[\"Body\"]))\n",
    "headers = d_reader.fieldnames\n",
    "\n",
    "\n",
    "header_str = \"(\"\n",
    "for header in headers:\n",
    "    header_str += header\n",
    "    if header in ['hand_id', 'hydro_id', 'lake_id']:\n",
    "        header_str += ' integer,'\n",
    "    elif header in ['branch_id', 'feature_id']:\n",
    "        header_str += ' bigint,'\n",
    "    else:\n",
    "        header_str += ' TEXT,'\n",
    "header_str = header_str[:-1] + \")\"\n",
    "print(header_str)\n",
    "\n",
    "db = database(db_type=\"viz\")\n",
    "with db.get_db_connection() as conn, conn.cursor() as cur:\n",
    "    \n",
    "    print(f\"Deleting/Creating derived.fim4_featureid_crosswalk using columns {header_str}\")\n",
    "    sql = f\"DROP TABLE IF EXISTS derived.fim4_featureid_crosswalk; CREATE TABLE derived.fim4_featureid_crosswalk {header_str};\"\n",
    "    cur.execute(sql)\n",
    "    conn.commit()\n",
    "    \n",
    "    # TODO: Nov: Drop the other 2 tables? No. ignore featureid_huc_crosswalk and featureid_huc_crosswalk_ak (not ours)\n",
    "    \n",
    "\n",
    "    print(f\"Importing {FIM_CROSSWALK_FPATH} to derived.fim4_featureid_crosswalk\")\n",
    "    sql = f\"\"\"\n",
    "        SELECT aws_s3.table_import_from_s3(\n",
    "           'derived.fim4_featureid_crosswalk',\n",
    "           '', \n",
    "           '(format csv, HEADER true)',\n",
    "           (SELECT aws_commons.create_s3_uri(\n",
    "               '{FIM_BUCKET}',\n",
    "               '{FIM_CROSSWALK_FPATH}',\n",
    "               'us-east-1'\n",
    "                ) AS s3_uri\n",
    "            ),\n",
    "            aws_commons.create_aws_credentials('{TI_ACCESS_KEY}', '{TI_SECRET_KEY}', '{TI_TOKEN}')\n",
    "           );\n",
    "        \"\"\"\n",
    "    cur.execute(sql)\n",
    "    conn.commit()\n",
    "\n",
    "    \n",
    "    print(f\"Adding {COLUMN_NAME_MODEL_VERSION} column to derived.fim4_featureid_crosswalk\")\n",
    "    sql = f\"ALTER TABLE derived.fim4_featureid_crosswalk ADD COLUMN IF NOT EXISTS {COLUMN_NAME_MODEL_VERSION} text DEFAULT '{HAND_MODEL_VERSION}';\"\n",
    "    cur.execute(sql)\n",
    "    conn.commit()\n",
    "    \n",
    "    print(\"Adding feature id index to derived.fim4_featureid_crosswalk\")\n",
    "    # Drop it already exists\n",
    "    sql = \"DROP INDEX IF EXISTS derived.fim4_crosswalk_feature_id\"\n",
    "    cur.execute(sql)\n",
    "    conn.commit()    \n",
    "    sql = \"CREATE INDEX fim4_crosswalk_feature_id ON derived.fim4_featureid_crosswalk USING btree (feature_id)\"\n",
    "    cur.execute(sql)\n",
    "    conn.commit()\n",
    "\n",
    "    print(\"Adding hydro id index to derived.fim4_featureid_crosswalk\")\n",
    "    # Drop it already exists\n",
    "    sql = \"DROP INDEX IF EXISTS derived.fim4_crosswalk_hydro_id\"\n",
    "    cur.execute(sql)\n",
    "    conn.commit()    \n",
    "    sql = \"CREATE INDEX fim4_crosswalk_hydro_id ON derived.fim4_featureid_crosswalk USING btree (hydro_id)\"\n",
    "    cur.execute(sql)\n",
    "    conn.commit()\n",
    "\n",
    "print(\"\")\n",
    "print(\"... Estimated time to completion is just a few mins\")\n",
    "print(\"Successully loaded derived.fim4_featureid_crosswalk and updated it\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c7d50b-0885-4bec-8094-7efe3d762da4",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2>2 - UPDATE FIM HAND PROCESSING LAMBDA ENV VARIABLE WITH NEW FIM PREFIX</h2>\n",
    "\n",
    "https://us-east-1.console.aws.amazon.com/lambda/home?region=us-east-1#/functions/hv-vpp-ti-viz-hand-fim-processing?tab=configure\n",
    "\n",
    "Lambda name: hv-vpp-ti-viz-hand-fim-processing\n",
    "\n",
    "In the Configuration Tab, click on the `Environment variables` (left menu)\n",
    "- change the `FIM_VERSION` to latest publie version being used. (numerics only): ie: 5.1.0\n",
    "- change the `HAND_VERSION` to latest HAND model version being used. (numerics only): ie: 4.5.11.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d71e761-9421-49fc-aaed-aafb33b7de0a",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2>3 - UPDATE FIM DATA PREP LAMBDA ENV VARIABLE WITH NEW FIM VERSION AND MEMORY</h2>\n",
    "\n",
    "https://us-east-1.console.aws.amazon.com/lambda/home?region=us-east-1#/functions/hv-vpp-ti-viz-fim-data-prep?tab=code\n",
    "\n",
    "Lambda name: hv-vpp-ti-viz-hand-fim-processing\n",
    "\n",
    "In the `Configuration` Tab, click on the `Environment variables` (left menu):\n",
    "- change the `FIM_VERSION` to the latest fim model version. \n",
    "<br>\n",
    "ie) 4.5.11.1\n",
    "<br><br>\n",
    "<b>Then:</b> Still in the Configuration Tab, now click on the `General Configuration` (left menu), followed \n",
    "by the `edit` button on the far right side, to get into the `General Configuration` page details.\n",
    "<br>Change (if they are not already there)\n",
    "<br>Memory (text field) to 4096 (MB)  and\n",
    "<br>Emphermeral Storage tp 1024 (MB)\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052f15eb-c883-4c27-9dd0-eb9c77ae7594",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2>4 - UPDATE RAS2FIM DATA (inc ras2fim boundaries) IN DB</h2>\n",
    "\n",
    "As of Oct 2024, we have a new fim (hand) release covered in this file, but ras2fim does not have a new\n",
    "release. ras2fim will likely be loaded as new datasets become available. \n",
    "\n",
    "***The code for ras2fim is removed here from the 4.5.2.11 set and will rebuilt as it''s own new separate load script when that happens.***\n",
    "\n",
    "However, we will have a few modifications for ras2fim data (not a reload) to help bring in the new\n",
    "fim_version and model_version format of just \"2.0\" columns. Those changes are included here.\n",
    "\n",
    "The Fim Version field needs to be deleted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc0a44c-8b46-4db5-a9a0-c65e0bf1e20b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Update \"geocurves\" to update the \"fim_version\" field to \"FIM 5.1.0:\n",
    "\n",
    "print(\"Updating geocurves table to model_version of 2.0 to follow the new versioning system and drop fim_version\")\n",
    "\n",
    "sf.execute_sql(\"UPDATE ras2fim.geocurves SET model_version = '2.0'\", db_type=\"viz\")\n",
    "\n",
    "# Drop the \"fim_version\" field.\n",
    "sf.execute_sql(\"ALTER TABLE ras2fim.geocurves DROP COLUMN fim_version\", db_type=\"viz\")\n",
    "\n",
    "print(\"Updating done for geocurves\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6d68b8-ecf2-406c-aa5d-43fe53707862",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2>5 - Run AEP FIM Pipelines.</h2>\n",
    "Updated Documentation from Tyler Early 2024: This can be done in a couple of diferent ways.\n",
    "\n",
    "1) One option is to use the pipeline_input code created below by Corey to start the AEP pipelines directly from this notebook.<br>\n",
    "   However, those pipeline_input dictionaries may very well be be out of date, pending more recent updates to the pipelines.<br?\n",
    "\n",
    "\n",
    "2) The other option, which I prefer, is to setup a manual test event in the initialize_pipeline lambda function to trigger an AEP pipeline like this:</b>\n",
    "{\n",
    "  \"configuration\": \"reference\",\n",
    "  \"products_to_run\": \"static_nwm_aep_inundation_extent_library\",\n",
    "  \"invoke_step_function\": false\n",
    "}\n",
    "\n",
    "Using this test event will produce the pipeline instructions, printing any errors that come up, and you can simply change the invoke_step_function flag to True when you're ready to actually invoke a pipeline run (which you can monitor/manage in the step function gui). You will need to manually update the static_nwm_aep_inundation_extent_library.yml product config file to only run 1 aep configuration at a time, and work through the configs as the pipelines finish (takes about an hour each). I've also found that the fim_data_prep lambda function needs to be temporarilly increased to ~4,500mb of memory to run these pipelines. It's also worth noting that these are very resource intesive pipelines, as FIM is calculated for every reach in the nation. AWS costs can amount to hundreds or even thousands of dollars by running these pipelines, so use responsibly.\n",
    "\n",
    "A couple other important notes:\n",
    "- These AEP configurations write data directly to the aep_fim schema in the egis RDS database, instead of the viz database.\n",
    "- <b>You'll need to dump the aep_fim schema after that is complete for backup / deployment into other environments.</b>\n",
    "- This process has not been tested with new NWM 3.0 Recurrence Flows, and a good thorough audit / QC check of output data is warranted, given those changes and the recent updates to the pipelines.\n",
    "\n",
    "***Note: You can start each of these 6 one right after the other. Maybe someday we can create a block that just starts all 6 at once.***\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a698067",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Aug 6, 2024: Note: This was created after all intervals were created, so only HW was tested against\n",
    "\n",
    "def get_aep_pipeline_input(stage_interval):\n",
    "    pipeline_input = {\n",
    "      \"configuration\": \"reference\",\n",
    "      \"job_type\": \"auto\",\n",
    "      \"data_type\": \"channel\",\n",
    "      \"keep_raw\": False,\n",
    "      \"reference_time\": datetime.now().strftime('%Y-%m-%d 00:00:00'),\n",
    "      \"configuration_data_flow\": {\n",
    "        \"db_max_flows\": [],\n",
    "        \"db_ingest_groups\": [],\n",
    "        \"python_preprocessing\": []\n",
    "      },\n",
    "      \"pipeline_products\": [\n",
    "        {\n",
    "          \"product\": \"static_nwm_aep_inundation_extent_library\",\n",
    "          \"configuration\": \"reference\",\n",
    "          \"product_type\": \"fim\",\n",
    "          \"run\": True,\n",
    "          \"fim_configs\": [\n",
    "            {\n",
    "              \"name\": f\"rf_{stage_interval}_inundation\",\n",
    "              \"target_table\": f\"aep_fim.rf_{stage_interval}_inundation\",\n",
    "              \"fim_type\": \"hand\",\n",
    "              \"sql_file\": f\"rf_{stage_interval}_inundation\"\n",
    "            }\n",
    "          ],\n",
    "          \"services\": [\n",
    "            \"static_nwm_aep_inundation_extent_library_noaa\"\n",
    "          ],\n",
    "          \"raster_outputs\": {\n",
    "            \"output_bucket\": \"\",\n",
    "            \"output_raster_workspaces\": []\n",
    "          },\n",
    "          \"postprocess_sql\": [],\n",
    "          \"product_summaries\": [],\n",
    "          \"python_preprocesing_dependent\": False\n",
    "        }\n",
    "      ],\n",
    "      \"sql_rename_dict\": {},\n",
    "      \"logging_info\": {\n",
    "          \"Timestamp\": int(datetime.now().timestamp())\n",
    "      }\n",
    "    }\n",
    "\n",
    "    return pipeline_input\n",
    "\n",
    "print(\"function: get_aep_pipeline_input loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d6ee69",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#### 2 Year Flow\n",
    "pipeline_input = get_aep_pipeline_input(\"2\")\n",
    "\n",
    "# notice, slightly different object name\n",
    "pipeline_name = f\"hv_ti_data_loads_sage_aep_2_{datetime.now().strftime('%Y%m%dT%H%M')}\"\n",
    "\n",
    "STEPFUNCTION_CLIENT.start_execution(\n",
    "    stateMachineArn = PIPELINE_ARN,\n",
    "    name = pipeline_name,\n",
    "    input= json.dumps(pipeline_input)\n",
    ")\n",
    "\n",
    "print(\"AEP : 2 year flows ie: rf_2_inundation kicked off. Can take 15 - 45 mins.\")\n",
    "print(f\"Step Function Pipeline : hv-vpp-ti-viz-pipeline : Pipeline run name started: {pipeline_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f89d9a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#### 5 Year Flow\n",
    "pipeline_input = get_aep_pipeline_input(\"5\")\n",
    "\n",
    "# notice, slightly different object name\n",
    "pipeline_name = f\"hv_ti_data_loads_sage_aep_5_{datetime.now().strftime('%Y%m%dT%H%M')}\"\n",
    "\n",
    "STEPFUNCTION_CLIENT.start_execution(\n",
    "    stateMachineArn = PIPELINE_ARN,\n",
    "    name = pipeline_name,\n",
    "    input= json.dumps(pipeline_input)\n",
    ")\n",
    "\n",
    "print(f\"AEP : 5 year flows ie: rf_5_inundation kicked off. Can take 15 - 45 mins.\")\n",
    "print(f\"Step Function Pipeline : hv-vpp-ti-viz-pipeline : Pipeline run name started: {pipeline_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791d1a8b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#### 10 Year Flow\n",
    "pipeline_input = get_aep_pipeline_input(\"10\")\n",
    "\n",
    "# notice, slightly different object name\n",
    "pipeline_name = f\"hv_ti_data_loads_sage_aep_10_{datetime.now().strftime('%Y%m%dT%H%M')}\"\n",
    "\n",
    "STEPFUNCTION_CLIENT.start_execution(\n",
    "    stateMachineArn = PIPELINE_ARN,\n",
    "    name = pipeline_name,\n",
    "    input= json.dumps(pipeline_input)\n",
    ")\n",
    "\n",
    "print(f\"AEP : 10 year flows ie: rf_10_inundation kicked off. Can take 15 - 45 mins.\")\n",
    "print(f\"Step Function Pipeline : hv-vpp-ti-viz-pipeline : Pipeline run name started: {pipeline_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb87128",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#### 25 Year Flow\n",
    "pipeline_input = get_aep_pipeline_input(\"25\")\n",
    "\n",
    "# notice, slightly different object name\n",
    "pipeline_name = f\"hv_ti_data_loads_sage_aep_25_{datetime.now().strftime('%Y%m%dT%H%M')}\"\n",
    "\n",
    "STEPFUNCTION_CLIENT.start_execution(\n",
    "    stateMachineArn = PIPELINE_ARN,\n",
    "    name = pipeline_name,\n",
    "    input= json.dumps(pipeline_input)\n",
    ")\n",
    "\n",
    "print(f\"AEP : 25 year flows ie: rf_25_inundation kicked off. Can take 15 - 45 mins.\")\n",
    "print(f\"Step Function Pipeline : hv-vpp-ti-viz-pipeline : Pipeline run name started: {pipeline_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4832e4e0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#### 50 Year Flow\n",
    "pipeline_input = get_aep_pipeline_input(\"50\")\n",
    "\n",
    "# notice, slightly different object name\n",
    "pipeline_name = f\"hv_ti_data_loads_sage_aep_50_{datetime.now().strftime('%Y%m%dT%H%M')}\"\n",
    "\n",
    "STEPFUNCTION_CLIENT.start_execution(\n",
    "    stateMachineArn = PIPELINE_ARN,\n",
    "    name = pipeline_name,\n",
    "    input= json.dumps(pipeline_input)\n",
    ")\n",
    "\n",
    "print(f\"AEP : 50 year flows ie: rf_50_inundation kicked off. Can take 15 - 45 mins.\")\n",
    "print(f\"Step Function Pipeline : hv-vpp-ti-viz-pipeline : Pipeline run name started: {pipeline_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187e83bc-ebbe-4615-a046-e0ef7b09ad3b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#### HW (High Water) Flow\n",
    "pipeline_input = get_aep_pipeline_input(\"high_water\")\n",
    "\n",
    "# notice, slightly different object name\n",
    "pipeline_name = f\"hv_ti_data_loads_sage_aep_hw_{datetime.now().strftime('%Y%m%dT%H%M')}\"\n",
    "\n",
    "STEPFUNCTION_CLIENT.start_execution(\n",
    "     stateMachineArn = PIPELINE_ARN,\n",
    "     name = pipeline_name,\n",
    "     input= json.dumps(pipeline_input)\n",
    ")\n",
    "\n",
    "print(f\"AEP : High Water year flows ie: rf_hw_inundation kicked off. Can take 15 - 45 mins.\")\n",
    "print(f\"Step Function Pipeline : hv-vpp-ti-viz-pipeline : Pipeline run name started: {pipeline_name}\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c810767-2f5d-46b2-860c-5d5c549f2e2a",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h3>IMPORTANT: Return hv-vpp-ti-viz-fim-data-prep Lambda memory to 2048mb</h3>\n",
    "\n",
    "https://us-east-1.console.aws.amazon.com/lambda/home?region=us-east-1#/functions/hv-vpp-ti-viz-fim-data-prep?tab=code\n",
    "\n",
    "Lambda name: hv-vpp-ti-viz-hand-fim-processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fa6c78-efc2-4dbe-ae9a-55b3c4842492",
   "metadata": {},
   "source": [
    "<h2>6 - RUN CATCHMENT WORKFLOWS 2 CONFIGS AT A TIME. CHECK FOR STEP FUNCTION FINISHING BEFORE STARTING NEW ONE</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7495759",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_catchment_pipepline_input(branch_key):\n",
    "\n",
    "    catchment_name_key = f\"catchments_{branch_key}_branches\"\n",
    "\n",
    "    pipeline_input = {\n",
    "      \"configuration\": \"reference\",\n",
    "      \"job_type\": \"auto\",\n",
    "      \"data_type\": \"channel\",\n",
    "      \"keep_raw\": False,\n",
    "      \"reference_time\": datetime.now().strftime('%Y-%m-%d 00:00:00'),\n",
    "      \"configuration_data_flow\": {\n",
    "        \"db_max_flows\": [],\n",
    "        \"db_ingest_groups\": [],\n",
    "        \"python_preprocessing\": []\n",
    "      },\n",
    "      \"pipeline_products\": [\n",
    "        {\n",
    "          \"product\": f\"static_hand_{catchment_name_key}\",\n",
    "          \"configuration\": \"reference\",\n",
    "          \"product_type\": \"fim\",\n",
    "          \"run\": True,\n",
    "          \"fim_configs\": [\n",
    "            {\n",
    "              \"name\": f\"{catchment_name_key}\",\n",
    "              \"target_table\": f\"fim_catchments.{catchment_name_key}\",\n",
    "              \"fim_type\": \"hand\",\n",
    "              \"sql_file\": f\"{catchment_name_key}\"\n",
    "            }\n",
    "          ],\n",
    "          \"services\": [\n",
    "            f\"static_hand_{catchment_name_key}_noaa\"\n",
    "          ],\n",
    "          \"raster_outputs\": {\n",
    "            \"output_bucket\": \"\",\n",
    "            \"output_raster_workspaces\": []\n",
    "          },\n",
    "          \"postprocess_sql\": [],\n",
    "          \"product_summaries\": [],\n",
    "          \"python_preprocesing_dependent\": False\n",
    "        },\n",
    "        {\n",
    "          \"product\": f\"static_hand_{catchment_name_key}_hi\",\n",
    "          \"configuration\": \"reference\",\n",
    "          \"product_type\": \"fim\",\n",
    "          \"run\": True,\n",
    "          \"fim_configs\": [\n",
    "            {\n",
    "              \"name\": f\"{catchment_name_key}_hi\",\n",
    "              \"target_table\": f\"fim_catchments.{catchment_name_key}_hi\",\n",
    "              \"fim_type\": \"hand\",\n",
    "              \"sql_file\": f\"{catchment_name_key}_hi\"\n",
    "            }\n",
    "          ],\n",
    "          \"services\": [\n",
    "            f\"static_hand_{catchment_name_key}_hi_noaa\"\n",
    "          ],\n",
    "          \"raster_outputs\": {\n",
    "            \"output_bucket\": \"\",\n",
    "            \"output_raster_workspaces\": []\n",
    "          },\n",
    "          \"postprocess_sql\": [],\n",
    "          \"product_summaries\": [],\n",
    "          \"python_preprocesing_dependent\": False\n",
    "        },\n",
    "        {\n",
    "          \"product\": f\"static_hand_{catchment_name_key}_prvi\",\n",
    "          \"configuration\": \"reference\",\n",
    "          \"product_type\": \"fim\",\n",
    "          \"run\": True,\n",
    "          \"fim_configs\": [\n",
    "            {\n",
    "              \"name\": f\"{catchment_name_key}_prvi\",\n",
    "              \"target_table\": f\"fim_catchments.{catchment_name_key}_prvi\",\n",
    "              \"fim_type\": \"hand\",\n",
    "              \"sql_file\": f\"{catchment_name_key}_prvi\"\n",
    "            }\n",
    "          ],\n",
    "          \"services\": [\n",
    "            f\"static_hand_{catchment_name_key}_prvi_noaa\"\n",
    "          ],\n",
    "          \"raster_outputs\": {\n",
    "            \"output_bucket\": \"\",\n",
    "            \"output_raster_workspaces\": []\n",
    "          },\n",
    "          \"postprocess_sql\": [],\n",
    "          \"product_summaries\": [],\n",
    "          \"python_preprocesing_dependent\": False\n",
    "        }\n",
    "      ],\n",
    "      \"sql_rename_dict\": {},\n",
    "      \"logging_info\": {\n",
    "          \"Timestamp\": int(datetime.now().timestamp())\n",
    "      }\n",
    "    }\n",
    "\n",
    "    return pipeline_input\n",
    "\n",
    "print(\"get_catchment_pipepline_input loaded\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fa1947-39af-49b4-8970-579ec4001d1c",
   "metadata": {},
   "source": [
    "### 6a - Branch 0 Catchments. Wait until it is done before kicking off the next GMS (Level Path) catchments load a bit lower. ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77d6e5b-f5fb-422f-a6c3-0f53743cd631",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Npte: The three db's here were renamed from:\n",
    "# \"branch_0_catchments\", \"branch_0_catchments_hi\", \"branch_0_catchments_prvi\",\n",
    "\n",
    "sf.execute_sql('''\n",
    "TRUNCATE \n",
    "    fim_catchments.catchments_0_branches,\n",
    "    fim_catchments.catchments_0_branches_hi,\n",
    "    fim_catchments.catchments_0_branches_prvi;\n",
    "''', db_type=\"egis\")\n",
    "\n",
    "print(\"Catchment Truncation for Branch 0 Done\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2806b8-5b9f-435c-b0ae-a28f9224b938",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "pipeline_name = f\"hv_ti_data_loads_catchments_branch_0_{datetime.now().strftime('%Y%m%dT%H%M')}\"\n",
    "\n",
    "pipeline_input = get_catchment_pipepline_input(\"0\")\n",
    "# print(pipeline_input)\n",
    "\n",
    "STEPFUNCTION_CLIENT.start_execution(\n",
    "    stateMachineArn=PIPELINE_ARN,\n",
    "    name=pipeline_name,\n",
    "    input=json.dumps(pipeline_input)\n",
    ")\n",
    "\n",
    "print(\"Catchments Branch 0 load kicked off. Takes appx 25 mins (depending on other processess) \"\n",
    "      f\"Step Function Pipeline : hv-vpp-ti-viz-pipeline : Run Name - {pipeline_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c75c4b7-bf13-4e8c-a33e-261219ce4338",
   "metadata": {},
   "source": [
    "### 6b - GMS (Level Paths / non branch 0) catchments ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea2a5be-9936-4abe-832a-8582f3c3af64",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "sf.execute_sql('''\n",
    "TRUNCATE\n",
    "    fim_catchments.catchments_gms_branches,\n",
    "    fim_catchments.catchments_gms_branches_hi,\n",
    "    fim_catchments.catchments_gms_branches_prvi;\n",
    "''', db_type=\"egis\")\n",
    "\n",
    "print(\"Catchment Truncation for GMS (Level Path) Branches Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae8917f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "pipeline_name = f\"hv_ti_data_loads_catchments_gms_{datetime.now().strftime('%Y%m%dT%H%M')}\"\n",
    "pipeline_input = get_catchment_pipepline_input(\"gms\")\n",
    "# print(pipeline_input)\n",
    "\n",
    "STEPFUNCTION_CLIENT.start_execution(\n",
    "    stateMachineArn=PIPELINE_ARN,\n",
    "    name=pipeline_name,\n",
    "    input=json.dumps(pipeline_input)\n",
    ")\n",
    "\n",
    "print(\"Catchments GMS Branches (Level Paths / non branch 0) load kicked off.\"\n",
    "      \" Takes appx 25 mins (depending on other processess)\")\n",
    "print(f\" Step Function Pipeline : hv-vpp-ti-viz-pipeline : Run Name - {pipeline_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fb7850-5be1-4127-86ba-bbfde273424a",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2>7 - Recreate derived.usgs_elev_table</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd0c144-5a20-4899-b946-a6dd9ae49b53",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Has appx 2,150 HUCs to process, but this section goes quickly.\n",
    "\n",
    "sf.execute_sql('DROP TABLE IF EXISTS derived.usgs_elev_table;')\n",
    "\n",
    "uet_usecols = ['location_id', 'HydroID', 'dem_adj_elevation', 'nws_lid', 'levpa_id']\n",
    "\n",
    "paginator = S3_CLIENT.get_paginator('list_objects')\n",
    "operation_parameters = {'Bucket': FIM_BUCKET,\n",
    "                        'Prefix': f'{HAND_DATASETS_DPATH}/',\n",
    "                        'Delimiter': '/'}\n",
    "page_iterator = paginator.paginate(**operation_parameters)\n",
    "page_count = 0\n",
    "num_pages = paginator.num_pages\n",
    "\n",
    "for page in page_iterator:\n",
    "\n",
    "    prefix_objects = page['CommonPrefixes']\n",
    "    for i, prefix_obj in enumerate(prefix_objects):\n",
    "        rec_num  = (i + 1) + (1000 * page_count)\n",
    "        print(f\"Processing rec number {rec_num} : \"\n",
    "              f\" On page {page_count} of {num_pages} (up to 1000 per page)\")\n",
    "        huc_prefix = prefix_obj.get(\"Prefix\")\n",
    "        usgs_elev_table_key = f'{huc_prefix}usgs_elev_table.csv'\n",
    "        try:\n",
    "            uet = S3_CLIENT.get_object(\n",
    "                Bucket=FIM_BUCKET, \n",
    "                Key=usgs_elev_table_key\n",
    "            )['Body']\n",
    "            uet_df = pd.read_csv(uet, header=0, usecols=uet_usecols)\n",
    "            # uet_df['fim_version'] = PUBLIC_FIM_VERSION\n",
    "            uet_df[COLUMN_NAME_MODEL_VERSION] = HAND_MODEL_VERSION\n",
    "            uet_df.to_sql(\n",
    "                con=VIZ_DB_ENGINE,\n",
    "                dtype={\n",
    "                    \"location_id\": Text(),\n",
    "                    \"nws_data_huc\": Text()\n",
    "                },\n",
    "                schema='derived',\n",
    "                name='usgs_elev_table',\n",
    "                index=False, \n",
    "                if_exists='append'\n",
    "            )\n",
    "        except Exception as e:\n",
    "            if \"NoSuchKey\" in str(e):\n",
    "                pass\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "    page_count += 1\n",
    "\n",
    "print(\"usgs_elev_tables load completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9951d778-5d75-4550-a63d-1dee833417f9",
   "metadata": {},
   "source": [
    "<h2>8 - Recreate derived.hydrotable_staggered</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33026eb6-fbc7-47f7-a44b-f9cea8f0911b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Takes appx 5.75 to 6 hrs to run\n",
    "\n",
    "# ******  WIP TESTS to see if we can speed it up    ****\n",
    "\n",
    "# Can not use Multi proc as there can be collisions to the db.\n",
    "\n",
    "# print(\"hydrotable reloaded - started\")\n",
    "# start_dt = datetime.now()\n",
    "\n",
    "# sf.execute_sql('DROP TABLE IF EXISTS derived.hydrotable;')\n",
    "# sql = '''\n",
    "# SELECT distinct LPAD(huc8::text, 8, '0') as huc8 FROM derived.featureid_huc_crosswalk WHERE huc8 is not null;\n",
    "# '''\n",
    "# df = sf.sql_to_dataframe(sql)\n",
    "# ht_usecols = ['HydroID', 'feature_id', 'stage', 'discharge_cms']\n",
    "\n",
    "# paginator = S3_CLIENT.get_paginator('list_objects')\n",
    "# operation_parameters = {'Bucket': FIM_BUCKET,\n",
    "#                         'Prefix': f'{HAND_DATASETS_DPATH}/',\n",
    "#                         'Delimiter': '/'}\n",
    "# page_iterator = paginator.paginate(**operation_parameters)\n",
    "\n",
    "# # print(len(page_iterator.items))\n",
    "\n",
    "# page_count = 0\n",
    "\n",
    "# # Incomplete tests for improving performance\n",
    "# number_of_recs_test = 50\n",
    "# test_done = False\n",
    "# df = pd.DataFrame()\n",
    "# is_last_pg_chunck = False\n",
    "# save_chunk_to_db = False\n",
    "# num_files_per_df = 20\n",
    "\n",
    "# for page in page_iterator:\n",
    "\n",
    "#     if test_done:\n",
    "#         break\n",
    "        \n",
    "#     prefix_objects = page['CommonPrefixes']\n",
    "#     # num_of_rec_in_pg = len(prefix_objects)\n",
    "#     # print(f\"num of records in page is {num_of_rec_in_pg}\")\n",
    "\n",
    "#     for i, prefix_obj in enumerate(prefix_objects):\n",
    "\n",
    "   \n",
    "#         if (i + 1) > number_of_recs_test:\n",
    "#             print(\"test done\")\n",
    "#             test_done = True\n",
    "#             break\n",
    "            \n",
    "#         total_recs_in_page = len(prefix_obj)\n",
    "#         print(f\"Total recs in set is {total_recs_in_page}\")\n",
    "              \n",
    "#         # test override for total recs\n",
    "#         total_recs_in_page = 50\n",
    "            \n",
    "#         rec_num  = (i + 1) + (1000 * page_count)\n",
    "#         print(f\"Processing rec number {rec_num} : \"\n",
    "#               f\" On page  {page_count + 1}   of (number of pages unknown) - (up to 1000 per page)\")\n",
    "\n",
    "#         branch_prefix = f'{prefix_obj.get(\"Prefix\")}branches/0/'\n",
    "#         branch_files_result = S3_CLIENT.list_objects(\n",
    "#             Bucket=FIM_BUCKET, \n",
    "#             Prefix=branch_prefix, \n",
    "#             Delimiter='/'\n",
    "#         )\n",
    "#         hydro_table_key = None\n",
    "#         for content_obj in branch_files_result.get('Contents'):\n",
    "#             branch_file_prefix = content_obj['Key']\n",
    "#             if 'hydroTable' in branch_file_prefix:\n",
    "#                 hydro_table_key = branch_file_prefix\n",
    "\n",
    "#         if hydro_table_key:\n",
    "                \n",
    "#             # print(f\"Found usgs_elev_table and hydroTable in {branch_prefix}\")\n",
    "#             try:\n",
    "#                 # print(\"...Fetching csvs...\")\n",
    "#                 ht = S3_CLIENT.get_object(\n",
    "#                     Bucket=FIM_BUCKET,\n",
    "#                     Key=hydro_table_key\n",
    "#                 )['Body']\n",
    "#                 # print(\"...Reading with pandas...\")\n",
    "#                 ht_df = pd.read_csv(ht, header=0, usecols=ht_usecols)\n",
    "                \n",
    "#                 mod_num = (i + 1) % num_files_per_df\n",
    "#                 print(\"========================\")\n",
    "#                 print(f\"mod is {mod_num}\")\n",
    "#                 print(f\"+++ current length of ht_df is {len(ht_df)}\")\n",
    "#                 print(f\"+++ current length of df is {len(df)}\")\n",
    "\n",
    "#                 # No matter what, append to the df (or start a new one if length is zero)\n",
    "#                 # which is likely a new set\n",
    "#                 if len(df) == 0:\n",
    "#                     df = ht_df  # sets up the schema\n",
    "#                 else:\n",
    "#                     df = pd.concat([df, ht_df])\n",
    "#                     print(\"... concating db's\")\n",
    "                    \n",
    "#                 print(f\"++++ new length of df is  {len(df)}\")\n",
    "                \n",
    "#                 if mod_num == 0:\n",
    "#                     print(\" ***** Saving df to database\")\n",
    "#                     save_chunk_to_db = True\n",
    "#                 elif (i + 1 >= total_recs_in_page):\n",
    "#                     print(\"*** last set of recs in this page set, saving df to database\")\n",
    "#                     save_chunk_to_db = True\n",
    "#                 else:\n",
    "#                     # not saving to Db yet, just build up larger sets with 20 csv's in them \n",
    "#                     # before loading to db.\n",
    "#                     continue  # not one of the sets of twenty\n",
    "\n",
    "#                 # if we got here, just save the db\n",
    "#                 print(\"\")\n",
    "#                 print(\".... saving dataframe to db\")\n",
    "#                 df[COLUMN_NAME_MODEL_VERSION] = HAND_MODEL_VERSION\n",
    "#                 df.to_sql(\n",
    "#                     con=VIZ_DB_ENGINE, \n",
    "#                     schema='derived',\n",
    "#                     name='hydrotable',\n",
    "#                     index=False,\n",
    "#                     if_exists='append',\n",
    "#                     method='multi'\n",
    "#                 )\n",
    "#                 save_chunk_to_db = False\n",
    "#                 df = pd.DataFrame()\n",
    "\n",
    "#             except Exception as e:\n",
    "#                 raise e\n",
    "#                 print(f'Fetch failed: {e}')\n",
    "\n",
    "#     page_count += 1\n",
    "\n",
    "# end_dt = datetime.now()\n",
    "# time_duration = end_dt - start_dt\n",
    "# print(\"hydrotable reload done\")\n",
    "# print(f\"... duration was  {str(time_duration).split('.')[0]}\")\n",
    "\n",
    "# '''\n",
    "# Timing tests..\n",
    "# length of 50  (groups of 20)\n",
    "#  - one rec downloaded and to_sql at a time =  5:40\n",
    "#  - Group into sets of 20 recs, then send to DF, make sure we don't miss the last one. = still 5:36\n",
    "#  - add the method=\"multi\" arg  = same and tended to freeze\n",
    "# '''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329dae61-d69f-4865-bdb6-e99a7442b0be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hydrotable reloaded - started\n",
      "Processing rec number 1 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 2 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 3 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 4 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 5 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 6 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 7 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 8 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 9 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 10 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 11 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 12 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 13 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 14 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 15 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 16 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 17 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 18 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 19 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 20 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 21 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 22 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 23 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 24 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 25 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 26 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 27 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 28 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 29 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 30 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 31 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 32 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 33 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 34 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 35 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 36 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 37 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 38 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 39 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 40 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 41 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 42 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 43 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 44 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 45 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 46 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 47 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 48 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 49 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 50 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 51 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 52 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 53 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 54 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 55 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 56 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 57 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 58 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 59 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 60 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 61 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 62 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 63 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 64 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 65 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 66 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 67 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 68 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 69 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 70 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 71 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 72 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 73 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 74 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 75 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 76 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 77 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 78 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 79 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 80 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 81 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 82 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 83 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 84 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 85 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 86 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 87 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 88 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 89 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 90 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 91 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 92 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 93 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 94 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 95 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 96 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 97 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 98 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 99 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 100 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 101 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 102 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 103 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 104 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 105 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 106 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 107 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 108 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 109 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 110 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 111 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 112 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 113 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 114 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 115 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 116 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 117 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 118 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 119 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 120 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 121 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 122 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 123 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 124 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 125 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 126 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 127 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 128 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 129 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 130 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 131 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 132 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 133 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 134 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 135 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 136 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 137 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 138 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 139 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 140 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 141 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 142 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 143 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 144 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 145 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 146 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 147 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 148 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 149 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 150 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 151 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 152 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 153 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 154 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 155 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 156 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 157 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 158 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 159 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 160 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 161 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 162 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 163 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 164 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 165 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 166 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 167 :  On page  1 (appx 2150 recs in total)\n",
      "Processing rec number 168 :  On page  1 (appx 2150 recs in total)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Note: This version finished in 1hr 49m but had to be babysat, clicking on the screen very 10 - 15 mins\n",
    "\n",
    "print(\"hydrotable reloaded - started\")\n",
    "start_dt = datetime.now()\n",
    "\n",
    "sf.execute_sql('DROP TABLE IF EXISTS derived.hydrotable;')\n",
    "sql = '''\n",
    "SELECT distinct LPAD(huc8::text, 8, '0') as huc8 FROM derived.featureid_huc_crosswalk WHERE huc8 is not null;\n",
    "'''\n",
    "df = sf.sql_to_dataframe(sql)\n",
    "ht_usecols = ['HydroID', 'feature_id', 'stage', 'discharge_cms']\n",
    "\n",
    "paginator = S3_CLIENT.get_paginator('list_objects')\n",
    "operation_parameters = {'Bucket': FIM_BUCKET,\n",
    "                        'Prefix': f'{HAND_DATASETS_DPATH}/',\n",
    "                        'Delimiter': '/'}\n",
    "page_iterator = paginator.paginate(**operation_parameters)\n",
    "\n",
    "# print(len(page_iterator.items))\n",
    "page_count = 0\n",
    "\n",
    "for page in page_iterator:\n",
    "\n",
    "    prefix_objects = page['CommonPrefixes']\n",
    "    # num_of_rec_in_pg = len(prefix_objects)\n",
    "    # print(f\"num of records in page is {num_of_rec_in_pg}\")\n",
    "\n",
    "    for i, prefix_obj in enumerate(prefix_objects):\n",
    "\n",
    "        # print(\"========================\")\n",
    "        total_recs_in_page = len(prefix_objects)\n",
    "\n",
    "        rec_num  = (i + 1) + (1000 * page_count)\n",
    "        print(f\"Processing rec number {rec_num} : \"\n",
    "              f\" On page  {page_count + 1} (appx 2150 recs in total)\")\n",
    "\n",
    "        branch_prefix = f'{prefix_obj.get(\"Prefix\")}branches/0/'\n",
    "        branch_files_result = S3_CLIENT.list_objects(\n",
    "            Bucket=FIM_BUCKET, \n",
    "            Prefix=branch_prefix, \n",
    "            Delimiter='/'\n",
    "        )\n",
    "        hydro_table_key = None\n",
    "        for content_obj in branch_files_result.get('Contents'):\n",
    "            branch_file_prefix = content_obj['Key']\n",
    "            if 'hydroTable' in branch_file_prefix:\n",
    "                hydro_table_key = branch_file_prefix\n",
    "\n",
    "        if hydro_table_key:\n",
    "\n",
    "            # print(f\"Found usgs_elev_table and hydroTable in {branch_prefix}\")\n",
    "            try:\n",
    "                # print(\"...Fetching csvs...\")\n",
    "                ht = S3_CLIENT.get_object(\n",
    "                    Bucket=FIM_BUCKET,\n",
    "                    Key=hydro_table_key\n",
    "                )['Body']\n",
    "                # print(\"... Reading with pandas ...\")\n",
    "                ht_df = pd.read_csv(ht, header=0, usecols=ht_usecols)\n",
    "\n",
    "                # print(\"... saving dataframe to db\")\n",
    "                ht_df[COLUMN_NAME_MODEL_VERSION] = HAND_MODEL_VERSION\n",
    "                ht_df.to_sql(\n",
    "                    con=VIZ_DB_ENGINE,\n",
    "                    schema='derived',\n",
    "                    name='hydrotable',\n",
    "                    index=False,\n",
    "                    if_exists='append',\n",
    "                    method='multi'\n",
    "                )\n",
    "                # print(\"... saved to db\")\n",
    "\n",
    "            except Exception as e:\n",
    "                raise e\n",
    "                print(f'Fetch failed: {e}')\n",
    "\n",
    "    page_count += 1\n",
    "\n",
    "end_dt = datetime.now()\n",
    "time_duration = end_dt - start_dt\n",
    "print(\"***************\")\n",
    "print(\"hydrotable reload done\")\n",
    "print(f\"... duration was  {str(time_duration).split('.')[0]}\")\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc764e4-7e67-4b51-a100-6021e31416cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"hydrotable_staggered started\")\n",
    "\n",
    "start_dt = datetime.now()\n",
    "\n",
    "sql = '''\n",
    "DROP TABLE IF EXISTS derived.hydrotable_staggered;\n",
    "SELECT\n",
    "    et.location_id,\n",
    "    ht.feature_id,\n",
    "    (stage + et.dem_adj_elevation) * 3.28084 as elevation_ft,\n",
    "    LEAD((stage + et.dem_adj_elevation) * 3.28084) OVER (PARTITION BY ht.feature_id ORDER BY ht.feature_id, stage) as next_elevation_ft,\n",
    "    discharge_cms * 35.3147 as discharge_cfs,\n",
    "    LEAD(discharge_cms * 35.3147) OVER (PARTITION BY ht.feature_id ORDER BY ht.feature_id, stage) as next_discharge_cfs\n",
    "INTO derived.hydrotable_staggered\n",
    "FROM derived.hydrotable AS ht\n",
    "JOIN derived.usgs_elev_table AS et ON ht.\"HydroID\" = et.\"HydroID\" AND et.location_id IS NOT NULL;\n",
    "'''\n",
    "sf.execute_sql(sql)\n",
    "\n",
    "print(\"hydrotable_staggered reload done\")\n",
    "end_dt = datetime.now()\n",
    "time_duration = end_dt - start_dt\n",
    "print(f\"... duration was  {str(time_duration).split('.')[0]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f21c03-ca36-402c-a69b-396162720f66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# we don't need the hydrotable anymore as it has been reloaded and adjusted above in hydrotable_staggered\n",
    "sf.execute_sql('DROP TABLE IF EXISTS derived.hydrotable;')\n",
    "print(\"Done dropping derived.hydrotable, post hydrotable_staggered load\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16ad30a-f0d6-4fba-8a9f-059d3d11ff6a",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2>9 - Recreate derived.usgs_rating_curves_staggered</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461f20b4-8442-4c0e-9114-ea62764cd789",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Aug 16, 2024 - done for 4.4.0.0 (4.5.2.11)\n",
    "\n",
    "# TODO: Aug 2024: Change this to a backup without indexes and not rename\n",
    "# Aug 27, 2024: This needs to be redone so we don't rename tables, it messes up indexes and index names when we use _to_sql commands later\n",
    "\n",
    "# sf.execute_sql(f'ALTER TABLE IF EXISTS derived.usgs_rating_curves RENAME TO usgs_rating_curves_{OLD_FIM_TAG};')\n",
    "# sf.execute_sql(f'ALTER TABLE IF EXISTS derived.usgs_rating_curves_staggered RENAME TO usgs_rating_curves_staggered_{OLD_FIM_TAG};')\n",
    "# print(\"usgs rating curve tables renamed and cleaned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b752fce5-c8d7-4064-92d7-842c22d1723e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "sql = '''\n",
    "    DROP TABLE IF EXISTS derived.usgs_rating_curves;\n",
    "    DROP TABLE IF EXISTS derived.usgs_rating_curves_staggered;\n",
    "'''\n",
    "sf.execute_sql(sql)\n",
    "\n",
    "print(\"Done dropping usgs_rating_curves and usgs_rating_curves_staggered\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a0cc18-de82-4bd8-bd4d-c3ad021a1dec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run the script to load the usgs_rating_curve.csv. Exact duration not yet known. Appx 30 min (??)\n",
    "\n",
    "start_dt = datetime.now()\n",
    "event = {\n",
    "    'target_table': 'derived.usgs_rating_curves',\n",
    "    'target_cols': ['location_id', 'flow', 'stage', 'navd88_datum', 'elevation_navd88'],\n",
    "    'file': f'{QA_DATASETS_DPATH}/usgs_rating_curves.csv',\n",
    "    'bucket': FIM_BUCKET,\n",
    "    'reference_time': '2023-08-23 00:00:00',\n",
    "    'keep_flows_at_or_above': 0,\n",
    "    'iteration_index': 0\n",
    "}\n",
    "\n",
    "sf.execute_db_ingest(event, None)\n",
    "\n",
    "print(\"done loading usgs_rating_curves\")\n",
    "end_dt = datetime.now()\n",
    "time_duration = end_dt - start_dt\n",
    "print(f\"... duration was  {str(time_duration).split('.')[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4548d5e8-303e-44e0-a8a5-7b1947214b4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Takes under a minute\n",
    "print(\"Starting usgs_rating_curves_staggered build based on usgs_rating_curve table\")\n",
    "\n",
    "sql = '''\n",
    "SELECT \n",
    "    location_id,\n",
    "    flow as discharge_cfs, \n",
    "    LEAD(flow) OVER (PARTITION BY location_id ORDER BY location_id, stage) as next_discharge_cfs,\n",
    "    stage,\n",
    "    navd88_datum,\n",
    "    elevation_navd88 as elevation_ft,\n",
    "    LEAD(elevation_navd88) OVER (PARTITION BY location_id ORDER BY location_id, stage) as next_elevation_ft\n",
    "INTO derived.usgs_rating_curves_staggered\n",
    "FROM derived.usgs_rating_curves;\n",
    "'''\n",
    "\n",
    "sf.execute_sql(sql)\n",
    "\n",
    "print(\"Done loading usgs_rating_curves_staggered\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a77875-f569-4410-888c-b132714a7c9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# usgs_rating_curves is a temp table and is loaded with some changes into the usgs_rating_curves_staggered\n",
    "sf.execute_sql('DROP TABLE IF EXISTS derived.usgs_rating_curves;')\n",
    "print(\"Done dropping derived.usgs_rating_curves, post loading usgs_rating_curves_staggered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e926c7a-9d1c-41ea-a9b3-5fc688994157",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2>10 - UPDATE SRC SKILL METRICS IN DB</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24f509a-90df-492a-a0d4-aa9eab8e7041",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the src_skill_temp table\n",
    "start_dt = datetime.now()\n",
    "\n",
    "sf.eecute_sql('DROP TABLE IF EXISTS derived.src_skill_temp;', db_type='viz')\n",
    "\n",
    "file_handle = 'agg_nwm_recurr_flow_elev_stats_location_id.csv'\n",
    "\n",
    "print(\"Reading file...\")\n",
    "# df = pd.read_csv(local_download_path)\n",
    "file_to_download = f\"{QA_DATASETS_DPATH}/{file_handle}\"\n",
    "df = s3_sf.download_S3_csv_files_to_df_from_list(FIM_BUCKET, [file_to_download], True)\n",
    "print(f\"File read. {len(df)} records found\")\n",
    "\n",
    "db_type = \"viz\"\n",
    "db_engine = sf.get_db_engine(db_type)\n",
    "\n",
    "df.to_sql(\n",
    "    name='src_skill_temp',\n",
    "    con=db_engine,\n",
    "    schema='derived',\n",
    "    if_exists='replace',\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(\"Done loading derived.src_skill_temp table\")\n",
    "end_dt = datetime.now()\n",
    "time_duration = end_dt - start_dt\n",
    "print(f\"... duration was  {str(time_duration).split('.')[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5639447-9a5a-4add-9c36-ebd945a5a8d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load into src_skill table adding geometry to it from external.usgs_gage. Yes.. more/less straight from WRDS tables\n",
    "# Some recs appear to be in error in the csv. location id = 394220106431500 (those are dropped below)\n",
    "\n",
    "start_dt = datetime.now()\n",
    "\n",
    "sf.execute_sql('DROP TABLE IF EXISTS derived.src_skill;', db_type='viz')\n",
    "\n",
    "sql = f'''\n",
    "SELECT\n",
    "    (row_number() OVER ())::int as oid,\n",
    "    gage.name,\n",
    "    LPAD(skill_temp.location_id::text, 8, '0') as location_id,\n",
    "    skill_temp.nrmse,\n",
    "    skill_temp.mean_abs_y_diff_ft,\n",
    "    skill_temp.mean_y_diff_ft,\n",
    "    skill_temp.percent_bias,\n",
    "    '{HAND_MODEL_VERSION}' as {COLUMN_NAME_MODEL_VERSION},\n",
    "    gage.geo_point as geom\n",
    "INTO derived.src_skill\n",
    "FROM derived.src_skill_temp skill_temp\n",
    "JOIN external.usgs_gage AS gage ON LPAD(gage.usgs_gage_id::text, 8, '0') = LPAD(skill_temp.location_id::text, 8, '0')\n",
    "'''\n",
    "\n",
    "sf.execute_sql(sql)\n",
    "\n",
    "print(\"Done loading derived.src_skill table\")\n",
    "end_dt = datetime.now()\n",
    "time_duration = end_dt - start_dt\n",
    "print(f\"... duration was  {str(time_duration).split('.')[0]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa32cd0-6edc-4441-9be3-f5bcba499ad0",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h4>Then export the derived.src_skill table and import it into the EGIS reference.src_skill table</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df23345-47a4-490a-b10f-2020e3a2a68e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "sf.move_data_from_viz_to_egis(\"derived.src_skill\", \"reference.src_skill\")\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118c126e-00b8-4c7f-8a8d-0bb427f413f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2>11 - UPDATE FIM PERFORMANCE METRICS IN DB</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fcdfab-3936-4a0a-8ab9-605e9a64f9bd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# clean up tables for new load\n",
    "\n",
    "table_names = [\n",
    "    \"reference.fim_performance_points\",\n",
    "    \"reference.fim_performance_polys\",\n",
    "    \"reference.fim_performance_catchments\"\n",
    "]\n",
    "\n",
    "for tb_name in table_names:\n",
    "    sql = f\"TRUNCATE TABLE {tb_name}\"\n",
    "#    print(sql)\n",
    "    sf.execute_sql(sql,db_type='egis')\n",
    "\n",
    "\n",
    "print(f\"All fim_performance tables trunated if they exist\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ca5581",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the new fim performance tables \n",
    "\n",
    "start_dt = datetime.now()  # appx 35 min\n",
    "\n",
    "# os.environ['EGIS_DB_HOST'] =''  #TI DB\n",
    "\n",
    "db_type = \"egis\"\n",
    "db_engine = sf.get_db_engine(db_type)\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Define bucket and parent directories.\n",
    "bucket = \"hydrovis-ti-deployment-us-east-1\"\n",
    "\n",
    "# file_handles = ['fim_performance_points.csv']\n",
    "#file_handles = ['fim_performance_points.csv', 'fim_performance_polys.csv', 'fim_performance_catchments.csv']\n",
    "# file_handles = ['fim_performance_points.csv', 'fim_performance_polys.csv']\n",
    "file_handles = ['fim_performance_catchments.csv']\n",
    "\n",
    "for file_handle in file_handles:\n",
    "\n",
    "    # df = pd.read_csv(local_download_path)\n",
    "    # print(f\"Reading file {file_to_download}\")\n",
    "    print(\"\")\n",
    "    file_to_download = f\"{QA_DATASETS_DPATH}/{file_handle}\"\n",
    "    # ie) /fim/hand_4_5_11_1/qa_datasets/fim_performance_catchments.csv\n",
    "    df = s3_sf.download_S3_csv_files_to_df_from_list(FIM_BUCKET, [file_to_download], True)\n",
    "    print(\"... File read.\")\n",
    "\n",
    "    # drop it for now, we will rebuild it\n",
    "    if 'oid' in df.columns:\n",
    "        df = df.drop('oid', axis=1)\n",
    "\n",
    "    # Rename headers.\n",
    "    if file_handle == 'fim_performance_points.csv':\n",
    "        df = df.rename(columns={'Unnamed: 0': 'oid', 'geometry': 'geom'})\n",
    "    else:\n",
    "        df = df.rename(columns={'Unnamed: 0': 'oid', 'geometry': 'geom', 'huc':'huc8'})\n",
    "\n",
    "    # print(df.dtypes)\n",
    "    # Convert all field names to lowercase (needed for ArcGIS Pro).\n",
    "    df.columns = df.columns.str.lower()\n",
    "\n",
    "    # Enforce data types on df before loading in DB (TODO: need to create special cases for each layer).\n",
    "    if file_handle == 'fim_performance_points.csv':\n",
    "        df = df.astype({'huc': 'str'})\n",
    "    else:\n",
    "        df = df.astype({'huc8': 'str'})\n",
    "    df = df.fillna(0)\n",
    "    try:\n",
    "        df = df.astype({'feature_id': 'int'})\n",
    "        df = df.astype({'feature_id': 'str'})\n",
    "        df = df.astype({'oid': 'int'})\n",
    "    except KeyError:  # If there is no feature_id field\n",
    "        pass\n",
    "    try:\n",
    "        df = df.astype({'nwm_seg': 'int'})\n",
    "        df = df.astype({'nwm_seg': 'str'})\n",
    "    except KeyError:  # If there is no nwm_seg field\n",
    "        pass\n",
    "    try:\n",
    "        df = df.astype({'usgs_gage': 'int'})\n",
    "        df = df.astype({'usgs_gage': 'str'})\n",
    "    except KeyError:  # If there is no usgs_gage field\n",
    "        pass\n",
    "\n",
    "    # zfill HUC8 field.\n",
    "    if file_handle == 'fim_performance_points.csv':\n",
    "        df['huc'] = df['huc'].apply(lambda x: x.zfill(8))\n",
    "    else:\n",
    "        df['huc8'] = df['huc8'].apply(lambda x: x.zfill(8))\n",
    "\n",
    "    # add model version column\n",
    "    df[COLUMN_NAME_MODEL_VERSION] = HAND_MODEL_VERSION\n",
    "\n",
    "    if \"version\" in df:\n",
    "        df = df.drop(\"version\", axis=1)\n",
    "\n",
    "    # Upload df to database.\n",
    "    stripped_layer_name = file_handle.replace(\".csv\", \"\")\n",
    "    table_name = \"reference.\" + stripped_layer_name\n",
    "    print(\"... Loading data into DB\")\n",
    "\n",
    "    # Chunk load data into DB\n",
    "\n",
    "    if file_handle in ['fim_performance_catchments.csv']:\n",
    "\n",
    "        print(\"... Chunk loading\")\n",
    "        # Create list of df chunks\n",
    "        n = 10000  # chunk row size\n",
    "        list_df = [df[i:i+n] for i in range(0,df.shape[0],n)]\n",
    "        # geometry = 'MULTIPOLYGON'\n",
    "        # Load the first chunk into the DB as a new table\n",
    "        first_chunk_df = list_df[0]\n",
    "        print(first_chunk_df.shape[0])\n",
    "\n",
    "        first_chunk_df.to_sql(\n",
    "            name=stripped_layer_name,\n",
    "            con=db_engine,\n",
    "            schema='reference',\n",
    "            if_exists='replace',\n",
    "            index=False,\n",
    "            dtype={'oid': sqlalchemy.types.Integer(),\n",
    "                   'version': sqlalchemy.types.String(),\n",
    "                   'geom': Geometry('MULTIPOLYGON', srid=3857)\n",
    "                  }\n",
    "        )\n",
    "        # Load remaining chunks into newly created table\n",
    "\n",
    "        for remaining_chunk_df in list_df[1:]:\n",
    "            print(remaining_chunk_df.shape[0])\n",
    "            remaining_chunk_df.to_sql(\n",
    "                name=stripped_layer_name,\n",
    "                con=db_engine,\n",
    "                schema='reference',\n",
    "                if_exists='append',\n",
    "                index=False,\n",
    "                dtype={'oid': sqlalchemy.types.Integer(),\n",
    "                       'version': sqlalchemy.types.String(),\n",
    "                       'geom': Geometry('MULTIPOLYGON', srid=3857)\n",
    "                      }\n",
    "            )\n",
    "    else:\n",
    "        if 'points' in stripped_layer_name: geometry='POINT'\n",
    "        if 'polys' in stripped_layer_name: geometry='POLYGON'\n",
    "        # print(\"GEOMETRY\")\n",
    "        # print(geometry)\n",
    "        df.to_sql(\n",
    "            name=stripped_layer_name,\n",
    "            con=db_engine,\n",
    "            schema='reference',\n",
    "            if_exists='replace',\n",
    "            index=False,\n",
    "            dtype={'oid': sqlalchemy.types.Integer(),\n",
    "                   'version': sqlalchemy.types.String(),\n",
    "                   'geom': Geometry(geometry, srid=3857)\n",
    "                  }\n",
    "        )\n",
    "\n",
    "    print(f\"... >>> {file_handle} downloaded and loaded\")\n",
    "    print(\"\")\n",
    "\n",
    "    # deleted the downloaded file that was just processed.\n",
    "    # if os.path.exists(local_download_path):\n",
    "\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "end_dt = datetime.now()\n",
    "time_duration = end_dt - start_dt\n",
    "# print(\"All FIM Performance files loaded\")\n",
    "print(f\"... duration was  {str(time_duration).split('.')[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9309a4b-1d94-4af2-90dc-57a08c4add68",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2>12 - CatFIM (Stage-Based and Flow-Based)</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7938fcb2-313c-4cf8-8274-d34d80bdf7bf",
   "metadata": {},
   "source": [
    "<h4>Function to load CatFIM Data (Non Public)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f37c81-d105-4c38-aa49-b4aef40a7543",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "''' Function to load CatFIM data (for any flow / stage / library / sites but non public)'''\n",
    "\n",
    "\n",
    "def load_catfim_table(catfim_type):\n",
    "\n",
    "    '''\n",
    "    Inputs:\n",
    "        - catfim_type: name identififer for the set, such as \"flow_based_catfim\" or \"flow_based_catfim_sites\", etc\n",
    "              Sometimes the file_handle name can be the name of the s3 file (without extension) and/or the table\n",
    "              name.\n",
    "              Options: flow_based_catfim, flow_based_catfim_sites, stage_based_catfim, stage_based_catfim_sites\n",
    "    '''\n",
    "\n",
    "    db_type = \"egis\"\n",
    "    db_engine = sf.get_db_engine(db_type)\n",
    "    src_crs = \"3857\"\n",
    "\n",
    "    # --------------------------------------\n",
    "    # Drop the original Db if already in place\n",
    "    table_name = catfim_type  # yes, dup variable for now\n",
    "\n",
    "    sf.execute_sql(f\"DROP TABLE IF EXISTS reference.{table_name};\", db_type=db_type)\n",
    "    print(f\"Dropping reference.{table_name} table if it existed\")\n",
    "    print(\"\")\n",
    "\n",
    "    # --------------------------------------\n",
    "    # Get the data from S3 and load it into a df\n",
    "    if catfim_type in ['flow_based_catfim', 'stage_based_catfim']:\n",
    "        file_to_download = f\"{QA_DATASETS_DPATH}/{catfim_type}_library.csv\"\n",
    "    else:\n",
    "        file_to_download = f\"{QA_DATASETS_DPATH}/{catfim_type}.csv\"\n",
    "\n",
    "    # print(f\"Downloading {file_to_download} ... \")\n",
    "\n",
    "    df = s3_sf.download_S3_csv_files_to_df_from_list(FIM_BUCKET, [file_to_download], True)\n",
    "    num_recs = len(df)\n",
    "    print(f\"File read. {num_recs} records to load\")\n",
    "\n",
    "    # --------------------------------------\n",
    "    # Adjusting Columns and data\n",
    "    # Rename headers. All files this name\n",
    "    df = df.rename(columns={'Unnamed: 0': 'oid',\n",
    "                            'geometry': 'geom',\n",
    "                            'huc': 'huc8'})\n",
    "\n",
    "    # Convert all field names to lowercase (needed for ArcGIS Pro).\n",
    "    df.columns = df.columns.str.lower()\n",
    "\n",
    "    # Enforce data types on df before loading in DB (TODO: need to create special cases for each layer).\n",
    "    df = df.astype({'huc8': 'str'})\n",
    "    df = df.fillna(0)\n",
    "    try:\n",
    "        df = df.astype({'feature_id': 'int'})\n",
    "        df = df.astype({'feature_id': 'str'})\n",
    "    except KeyError:  # If there is no feature_id field\n",
    "        pass\n",
    "    try:\n",
    "        df = df.astype({'nwm_seg': 'int'})\n",
    "        df = df.astype({'nwm_seg': 'str'})\n",
    "    except KeyError:  # If there is no nwm_seg field\n",
    "        pass\n",
    "    try:\n",
    "        df = df.astype({'usgs_gage': 'int'})\n",
    "        df = df.astype({'usgs_gage': 'str'})\n",
    "    except KeyError:  # If there is no usgs_gage field\n",
    "        pass\n",
    "\n",
    "    # zfill HUC8 field.\n",
    "    df['huc8'] = df['huc8'].apply(lambda x: x.zfill(8))\n",
    "\n",
    "    if '_sites' in catfim_type:\n",
    "        df = df.astype({'nws_data_rfc_forecast_point': 'str'})\n",
    "        df = df.astype({'nws_data_rfc_defined_fcst_point': 'str'})\n",
    "        df = df.astype({'nws_data_riverpoint': 'str'})\n",
    "\n",
    "\n",
    "    # As of Nov 1, 2024: Ignore the incoming \"version\" from dataset\n",
    "    # df['version'] = PUBLIC_FIM_VERSION\n",
    "    df[COLUMN_NAME_FIM_VERSION] = PUBLIC_FIM_VERSION\n",
    "    df[COLUMN_NAME_MODEL_VERSION] = FIM_MODEL_VERSION\n",
    "\n",
    "    # --------------------------------------\n",
    "    # Load to DB\n",
    "    # Chunk load data into DB\n",
    "    if catfim_type in ['flow_based_catfim', 'stage_based_catfim']:\n",
    "\n",
    "        # Create list of df chunks\n",
    "        n = 1000  # chunk row size\n",
    "        print(f\"Chunk loading... into {table_name} -- {n} records at a time\")\n",
    "        print(\"\")\n",
    "        chunk_df = [df[i:i+n] for i in range(0, df.shape[0], n)]\n",
    "\n",
    "        # Load the first chunk into the DB as a new table\n",
    "        first_chunk_df = chunk_df[0]\n",
    "        num_chunks = len(chunk_df)\n",
    "\n",
    "        print(f\" ... loading chunk 1 of {num_chunks}\")\n",
    "\n",
    "        first_chunk_df.to_sql(\n",
    "            name=table_name,\n",
    "            con=db_engine,\n",
    "            schema='reference',\n",
    "            if_exists='replace',\n",
    "            index=False,\n",
    "            dtype={'oid': sqlalchemy.types.Integer(),\n",
    "                   'geom': Geometry('MULTIPOLYGON', srid=src_crs)}\n",
    "        )\n",
    "\n",
    "        # Load remaining chunks into newly created table\n",
    "        ctr = 1  # Already loaded one\n",
    "        for remaining_chunk in chunk_df[1:]:\n",
    "            # print(remaining_chunk.shape[0])\n",
    "            ctr += 1\n",
    "            print(f\" ... loading chunk {ctr} of {num_chunks}\")\n",
    "            remaining_chunk.to_sql(\n",
    "                        name=table_name,\n",
    "                        con=db_engine,\n",
    "                        schema='reference',\n",
    "                        if_exists='append',\n",
    "                        index=False,\n",
    "                        dtype={'oid': sqlalchemy.types.Integer(),\n",
    "                               'geom': Geometry('MULTIPOLYGON', srid=src_crs)\n",
    "                              }\n",
    "                    )\n",
    "        # end for\n",
    "    else:  # sites tables\n",
    "        print(f\"Loading data into {table_name} ...\")\n",
    "\n",
    "        df.to_sql(\n",
    "            name=table_name,\n",
    "            con=db_engine,\n",
    "            schema='reference',\n",
    "            if_exists='replace',\n",
    "            index=False,\n",
    "            dtype={'oid': sqlalchemy.types.Integer(),\n",
    "                   'geom': Geometry('POINT', srid=src_crs)}\n",
    "        )\n",
    "\n",
    "    # This should auto create a gist index against the geometry column\n",
    "    # if that index name already exists, the upload will fail, the index can not pre-exist\n",
    "    # Best to drop the table before loading.\n",
    "\n",
    "    # return\n",
    "\n",
    "print(\"load_catfim_table function loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca12f850-8826-4177-884a-04f461496d7b",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h3>12.a - Backup old DBs and prepare new databases (but not the \"public\" FIM 10/30 db's)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9577edb-4aa6-423b-819e-df8c922c7ec2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This covers both Stage Based and Flow Based (but not the \"public\" catfim db's)\n",
    "\n",
    "# The \"Public\" db backups ana loads are in cells lower (12.d and higher)\n",
    "\n",
    "# DONE for 4.4.0.0.  (4.5.2.11)\n",
    "\n",
    "# # print(\"Starting Data Backups and table drops for stage and flow based catfim\")\n",
    "# db_names = [\"stage_based_catfim\", \"stage_based_catfim_sites\",\n",
    "#             \"flow_based_catfim\", \"flow_based_catfim_sites\"]\n",
    "\n",
    "# for db_name in db_names:\n",
    "#     new_table_name = f\"reference.{db_name}_{OLD_FIM_TAG}\"\n",
    "#     sql = f'''\n",
    "#         CREATE TABLE IF NOT EXISTS {new_table_name} AS TABLE reference.{db_name};\n",
    "#     '''\n",
    "#     sf.execute_sql(sql, db_type='egis')\n",
    "#     print(f\"{db_name} copied to {new_table_name} if it does not already exist\")\n",
    "\n",
    "\n",
    "# Aug 2024: Now we can drop the tables as we don't have any indexes on them at this time other than the gist geom index.\n",
    "# By dropping them, we can auto adjust the tables schema. (don't truncate)\n",
    "\n",
    "# for db_name in db_names:\n",
    "#     sf.execute_sql(f\"DROP TABLE IF EXISTS reference.{db_name};\", db_type='egis')\n",
    "#     print(f\"reference.{db_name} table dropped if it existed\")\n",
    "\n",
    "\n",
    "# print(\"Data Backups of flow based catfim are complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0af91cf-5a87-4c0b-a7ae-54d6809dd152",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h3>12.b - Updated Flow and Stage Based CatFIM Data (Non Public)</h3>\n",
    "\n",
    "<h3>AUG 2024: IMPORTANT NOTE:</h3>\n",
    "The stage based catfim (library) csv has grown to appx 10 GiB. Our current notebook, hv-vpp-ti-viz-notebook only has 15 GiB memory.\n",
    "Running tool can easily overwhelm the notebook server and freeze it up forcing a reboot.\n",
    "Sometimes when the notebook instance comes back up, it no longer has ths swap system in place. You will need most of the memory\n",
    "and some swap to load it.  Keep an eye a \"terminal\" windows and keep entering `free -h` to keep an eye on it's usage.\n",
    "</br>\n",
    "We will need to review to see if we want to:\n",
    "\n",
    "1. Upgrade this notebook server with more memory (and harddrive space would be good)\n",
    "\n",
    "2. Change the load of the catfim library (non sites) data to another system. Maybe we can load it via a lambda to an EC2 or something?\n",
    "\n",
    "3. Get the FIM Team to break it to smaller pieces, but watch carefully for the OID system (unique id for all records)\n",
    "\n",
    "**When you are done running this script, Please restart this kernal as it does not appear to be releasing all memory. (memory leak?)**\n",
    "\n",
    "\n",
    "Also looks like Tyler has some notebooks where he was moving this into a lambda load? We need to look into that\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c55cd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"Starting of CatFIM data\")\n",
    "\n",
    "# catfim_types = ['flow_based_catfim', 'flow_based_catfim_sites']\n",
    "# catfim_types =  ['stage_based_catfim', 'stage_based_catfim_sites']\n",
    "catfim_types = ['stage_based_catfim_sites']\n",
    "# catfim_types = ['stage_based_catfim']\n",
    "\n",
    "start_dt = datetime.now()\n",
    "\n",
    "for catfim_type in catfim_types:\n",
    "    print(f\"Loading {catfim_type} data\")\n",
    "    load_catfim_table(catfim_type)\n",
    "\n",
    "print(\"\")\n",
    "end_dt = datetime.now()\n",
    "time_duration = end_dt - start_dt\n",
    "print(f\"... duration was  {str(time_duration).split('.')[0]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d1a57d-089c-4042-98d8-6893d7a45acf",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h3>12.c - CatFIM Backup old \"public\" FIM 10 / 30 DBs and prepare new databases</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cf108d-7360-48a9-a4a2-81b23b9e51b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This covers ONLY Catfim public FIM 10/30 for both flow based and stage based\n",
    "'''\n",
    "\n",
    "''' DONE for 4.4.0.0.  (4.5.2.11)'''\n",
    "\n",
    "# db_name_appendix = f\"{OLD_FIM_TAG}_fim_10\"\n",
    "\n",
    "# print(\"Starting Data Backups and table drops for stage and flow based PUBLIC catfim\")\n",
    "# # db_names = [\"stage_based_catfim_public\", \"stage_based_catfim_sites_public\",\n",
    "# #              \"flow_based_catfim_public\", \"flow_based_catfim_sites_public\"]\n",
    "\n",
    "# # stage_based_catfim_sites_public didn't exist for fim 10 but should have in TI (does in other enviros likely)\n",
    "# db_names = [\"stage_based_catfim_public\", \n",
    "#              \"flow_based_catfim_public\", \"flow_based_catfim_sites_public\"]\n",
    "\n",
    "# for db_name in db_names:\n",
    "#     new_table_name = f\"reference.{db_name}_{db_name_appendix}\"\n",
    "#     sql = f\"CREATE TABLE IF NOT EXISTS {new_table_name} AS TABLE reference.{db_name}\"\n",
    "#     sf.execute_sql(sql, db_type='egis')\n",
    "#     print(f\"{db_name} copied to {new_table_name} if it does not already exist\")\n",
    "\n",
    "    \n",
    "# # Aug 2024: Now we can drop the tables as we don't have any indexes on them at this time other than the gist geom index.\n",
    "# # By dropping them, we can auto adjust the tables schema. (don't truncate)\n",
    "\n",
    "# for db_name in db_names:\n",
    "#     sf.execute_sql(f\"DROP TABLE IF EXISTS reference.{db_name};\", db_type='egis')\n",
    "#     print(f\"reference.{db_name} table dropped if it existed\")\n",
    "\n",
    "# print(\"Data Backups of flow based catfim are complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb73b36-056f-47b2-90cd-38b26171723a",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h3>12.d - Load CatFIM \"public\" FIM 30 DBs</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd38c00d-22ad-476e-ab05-cf293e5bbc15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"Loading CatFIM Public datasets (FIM 30)\")\n",
    "\n",
    "catfim_types = [\"stage_based_catfim\", \"stage_based_catfim_sites\",\n",
    "                \"flow_based_catfim\", \"flow_based_catfim_sites\"]\n",
    "\n",
    "__public_fim_release = \"fim_30\"  # The new fim public release being loaded (ie. fim_10, fim_30, fim_60..)\n",
    "\n",
    "start_dt = datetime.now()\n",
    "\n",
    "for catfim_type in catfim_types:\n",
    "    print(\"\")\n",
    "    sql = f'''\n",
    "    DROP TABLE IF EXISTS reference.{catfim_type}_public;\n",
    "\n",
    "    SELECT\n",
    "        catfim.*,\n",
    "        '{__public_fim_release}' as public_fim_release\n",
    "    INTO reference.{catfim_type}_public\n",
    "    FROM reference.{catfim_type} as catfim\n",
    "    JOIN reference.public_fim_domain as fim_domain ON ST_Intersects(catfim.geom, fim_domain.geom)\n",
    "    '''\n",
    "    print(sf.execute_sql(sql, db_type='egis'))\n",
    "    print(f\"public {__public_fim_release} data load for {catfim_type} is complete\")\n",
    "\n",
    "# what about indexes again?\n",
    "\n",
    "# for db_name in db_names:\n",
    "#     new_table_name = f\"reference.{db_name}_{db_name_appendix}\"\n",
    "#     sql = f\"CREATE TABLE IF NOT EXISTS {new_table_name} AS TABLE reference.{db_name}\"\n",
    "#     sf.execute_sql(sql, db_type='egis')\n",
    "#     print(f\"{db_name} copied to {new_table_name} if it does not already exist\")\n",
    "\n",
    "print(\"\")\n",
    "end_dt = datetime.now()\n",
    "time_duration = end_dt - start_dt\n",
    "print(f\"... duration was  {str(time_duration).split('.')[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb36b537-f957-457a-9bfa-64d944f85599",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2>13 - Clear the HAND Cache</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d1ab20-4148-4d1d-bbb6-a2c3983ba145",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "TRUNCATE TABLE fim_cache.hand_hydrotable_cached;\n",
    "TRUNCATE TABLE fim_cache.hand_hydrotable_cached_max;\n",
    "TRUNCATE TABLE fim_cache.hand_hydrotable_cached_geo;\n",
    "TRUNCATE TABLE fim_cache.hand_hydrotable_cached_zero_stage;\n",
    "\"\"\"\n",
    "sf.execute_sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166702e8-60d2-485e-a5bc-23b475841e5a",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2>14 - SAVE TO REPO (AND REDEPLOY TO TI WITH NEW VERSION VARIABLE IN TERRAFORM ??)</h2>\n",
    "\n",
    "Oct 21, 2024: We don''t have a system per-say to update for Terraform, but we now have github hooks\n",
    "built right into JupyterHub. We need to figure out how to work with multiple branches and \"getting latest\"\n",
    "but this gives us source control management now.\n",
    "\n",
    "\n",
    "Note from Rob: While, un-elegant, there so much quick evolution here that I recommend we even keep seperate named load scripts in GIT\n",
    "ie) one for FIM Version 4.4.0.0 and one for 4.5.2.11, etc. So many changes for each edition and very fast script changes WIP may \n",
    "make it smarter to keep each script seperately (ie. 4.4.0.0, 4.5.2.11, etc)\n",
    "\n",
    "<h4>Make sure to Publish the changes to git and add a PR</h4>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
