{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "657203a8-278b-4265-86f9-e23983445fe1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### This notebook was created by Corey Krewson in 2023 to facilitate all the steps needed to update the pipelines / EGIS map services, when a new FIM version is released. These steps entail:\n",
    "1. Updating the FIM crosswalk table in the derived schema of the viz database.\n",
    "2. Creating new AEP FIM schema/tables in the EGIS database.\n",
    "3. Creating new FIM Catchments schema/tables in the EGIS database.\n",
    "4. Update a bunch of other SRC Skill / FIM Performance / CatFIM data (not sure what all this entails - Shawn Crawley may know more).\n",
    "5. Clear the HAND cache on the viz RDS database.\n",
    "\n",
    "Unfortunately, these steps are still very manual, and this notebook is the main source of documentation to making these updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b656259",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from helper_functions.shared_functions import check_if_s3_key_exists, run_sql_in_db, get_db_engine  # no check_if_s3_key_exists function\n",
    "from helper_functions.shared_functions import run_sql_in_db, get_db_engine\n",
    "from helper_functions.viz_classes import database\n",
    "import os\n",
    "import codecs\n",
    "import csv\n",
    "import boto3\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from io import StringIO\n",
    "from sqlalchemy.exc import DataError\n",
    "from sqlalchemy.types import Text\n",
    "\n",
    "# Helps us get to the keys. Note: This was added Oct 16, 2024 and is untested\n",
    "# /hydrovis/Core/Manual_Workflows/\n",
    "sys.path.append(os.path.join(os.path.abspath(''), '../../../../AWS_Secret_keys'))\n",
    "import AWS_Keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e944eff3-6023-48f4-9f57-27304a240447",
   "metadata": {},
   "outputs": [],
   "source": [
    "FIM_VERSION = \"4.4.0.0\"\n",
    "FIM_ROOT_DPATH = f\"fim/fim_{FIM_VERSION.replace('.', '_')}\"\n",
    "HAND_DATASETS_DPATH = f\"{FIM_ROOT_DPATH}/hand_datasets\"\n",
    "QA_DATASETS_DPATH = f\"{FIM_ROOT_DPATH}/qa_datasets\"\n",
    "FIM_BUCKET = \"hydrovis-ti-deployment-us-east-1\"\n",
    "FIM_CROSSWALK_FPATH = os.path.join(HAND_DATASETS_DPATH, \"crosswalk_table.csv\")\n",
    "PIPELINE_ARN = ''\n",
    "\n",
    "# SECRET KEYS AND TOKENS inside a folder in the sandbox folder to avoid checkins\n",
    "\n",
    "S3_CLIENT = boto3.client(\"s3\")\n",
    "STEPFUNCTION_CLIENT = boto3.client('stepfunctions')\n",
    "VIZ_DB_ENGINE = get_db_engine('viz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb715ee",
   "metadata": {},
   "source": [
    "<h2>1 - UPDATE VLAB PRIVATE REPO WITH NEW FIM VERSION</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6875edcd",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2>2 - UPLOAD FIM4 HYDRO ID/FEATURE ID CROSSWALK</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb51763d-ce45-4e48-9bc3-c772590d71ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "February 2024 Update from Tyler: This code will need to be updated to handle a new hand_id unique integer that the fim team (Rob Hannah and Matt Luck) has added to the crosswalk, and is now important to fim runs. They also changed the field names / format to match our schema, so this chunk of code should be able to be simplified significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96a49f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Getting column name from {FIM_CROSSWALK_FPATH}\")\n",
    "data = S3_CLIENT.get_object(Bucket=FIM_BUCKET, Key=FIM_CROSSWALK_FPATH)\n",
    "d_reader = csv.DictReader(codecs.getreader(\"utf-8\")(data[\"Body\"]))\n",
    "headers = d_reader.fieldnames\n",
    "\n",
    "print(headers)\n",
    "raise Exception(\"Forced exit\")\n",
    "\n",
    "header_str = \"(\"\n",
    "for header in headers:\n",
    "    header_str += header\n",
    "    if header in ['HydroID', 'LakeID', 'feature_id']:\n",
    "        header_str += ' integer,'\n",
    "    elif header in ['BranchID']:\n",
    "        header_str += ' bigint,'\n",
    "    else:\n",
    "        header_str += ' TEXT,'\n",
    "header_str = header_str[:-1] + \")\"\n",
    "\n",
    "\n",
    "db = database(db_type=\"viz\")\n",
    "with db.get_db_connection() as conn, conn.cursor() as cur:\n",
    "    print(f\"Deleting/Creating derived.fim4_featureid_crosswalk using columns {header_str}\")\n",
    "    sql = f\"DROP TABLE IF EXISTS derived.fim4_featureid_crosswalk; CREATE TABLE derived.fim4_featureid_crosswalk {header_str};\"\n",
    "    cur.execute(sql)\n",
    "    conn.commit()\n",
    "\n",
    "    print(f\"Importing {FIM_CROSSWALK_FPATH} to derived.fim4_featureid_crosswalk\")\n",
    "    sql = f\"\"\"\n",
    "        SELECT aws_s3.table_import_from_s3(\n",
    "           'derived.fim4_featureid_crosswalk',\n",
    "           '', \n",
    "           '(format csv, HEADER true)',\n",
    "           (SELECT aws_commons.create_s3_uri(\n",
    "               '{FIM_BUCKET}',\n",
    "               '{FIM_CROSSWALK_FPATH}',\n",
    "               'us-east-1'\n",
    "                ) AS s3_uri\n",
    "            ),\n",
    "            aws_commons.create_aws_credentials('{TI_ACCESS_KEY}', '{TI_SECRET_KEY}', '{TI_TOKEN}')\n",
    "           );\n",
    "        \"\"\"\n",
    "    cur.execute(sql)\n",
    "    conn.commit()\n",
    "\n",
    "    print(f\"Adding fim_version column to derived.fim4_featureid_crosswalk\")\n",
    "    sql = f\"ALTER TABLE derived.fim4_featureid_crosswalk ADD COLUMN fim_version text DEFAULT '{FIM_VERSION}';\"\n",
    "    cur.execute(sql)\n",
    "    conn.commit()\n",
    "\n",
    "    print(f\"Renaming columns in derived.fim4_featureid_crosswalk\")\n",
    "    sql = f\"\"\"\n",
    "        ALTER TABLE derived.fim4_featureid_crosswalk RENAME COLUMN HydroID TO hydro_id;\n",
    "        ALTER TABLE derived.fim4_featureid_crosswalk RENAME COLUMN LakeID TO lake_id;\n",
    "        ALTER TABLE derived.fim4_featureid_crosswalk RENAME COLUMN BranchID  TO branch_id;\n",
    "    \"\"\"\n",
    "    cur.execute(sql)\n",
    "    conn.commit()\n",
    "\n",
    "    print(f\"Adding feature id index to derived.fim4_featureid_crosswalk\")\n",
    "    sql = f\"CREATE INDEX fim4_crosswalk_feature_id ON derived.fim4_featureid_crosswalk USING btree (feature_id)\"\n",
    "    cur.execute(sql)\n",
    "    conn.commit()\n",
    "\n",
    "    print(f\"Adding hydro id index to derived.fim4_featureid_crosswalk\")\n",
    "    sql = f\"CREATE INDEX fim4_crosswalk_hydro_id ON derived.fim4_featureid_crosswalk USING btree (hydro_id)\"\n",
    "    cur.execute(sql)\n",
    "    conn.commit()\n",
    "\n",
    "print(f\"Successully updated derived.fim4_featureid_crosswalk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0b9ca7",
   "metadata": {},
   "source": [
    "<h2>3 - UPDATE FIM HAND PROCESSING LAMBDA ENV VARIABLE WITH NEW FIM VERSION</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641831e4",
   "metadata": {},
   "source": [
    "<h2>4 - UPDATE FIM DATA PREP LAMBDA ENV VARIABLE WITH NEW FIM VERSION</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec58bcb-1800-45dc-a440-d4adab1f1b71",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2>5 - Run AEP FIM Pipelines.</h2>\n",
    "Updated Documentation from Tyler Early 2024: This can be done in a couple of diferent ways. One option is to use the pipeline_input code created below by Corey to start the AEP pipelines directly from this notebook. However, those pipeline_input dictionaries may very well be be out of date, pending more recent updates to the pipelines. The other option, which I prefer, is to setup a manual test event in the initialize_pipeline lambda function to trigger an AEP pipeline like this:\n",
    "\n",
    "{\n",
    "  \"configuration\": \"reference\",\n",
    "  \"products_to_run\": \"static_nwm_aep_inundation_extent_library\",\n",
    "  \"invoke_step_function\": False\n",
    "}\n",
    "\n",
    "Using this test event will produce the pipeline instructions, printing any errors that come up, and you can simply change the invoke_step_function flag to True when you're ready to actually invoke a pipeline run (which you can monitor/manage in the step function gui). You will need to manually update the static_nwm_aep_inundation_extent_library.yml product config file to only run 1 aep configuration at a time, and work through the configs as the pipelines finish (takes about an hour each). I've also found that the fim_data_prep lambda function needs to be temporarilly increased to ~4,500mb of memory to run these pipelines. It's also worth noting that these are very resource intesive pipelines, as FIM is calculated for every reach in the nation. AWS costs can amount to hundreds or even thousands of dollars by running these pipelines, so use responsibly.\n",
    "\n",
    "A couple other important notes:\n",
    "- These AEP configurations write data directly to the aep_fim schema in the egis RDS database, instead of the viz database.\n",
    "- You'll need to dump the aep_fim schema after that is complete for backup / deployment into other environments.\n",
    "- This process has not been tested with new NWM 3.0 Recurrence Flows, and a good thorough audit / QC check of output data is warranted, given those changes and the recent updates to the pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d6ee69",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_input = {\n",
    "  \"configuration\": \"reference\",\n",
    "  \"job_type\": \"auto\",\n",
    "  \"data_type\": \"channel\",\n",
    "  \"keep_raw\": False,\n",
    "  \"reference_time\": datetime.now().strftime('%Y-%m-%d 00:00:00'),\n",
    "  \"configuration_data_flow\": {\n",
    "    \"db_max_flows\": [],\n",
    "    \"db_ingest_groups\": [],\n",
    "    \"lambda_max_flows\": []\n",
    "  },\n",
    "  \"pipeline_products\": [\n",
    "    {\n",
    "      \"product\": \"static_nwm_aep_inundation_extent_library\",\n",
    "      \"configuration\": \"reference\",\n",
    "      \"product_type\": \"fim\",\n",
    "      \"run\": True,\n",
    "      \"fim_configs\": [\n",
    "        {\n",
    "          \"name\": \"rf_2_inundation\",\n",
    "          \"target_table\": \"aep_fim.rf_2_inundation\",\n",
    "          \"fim_type\": \"hand\",\n",
    "          \"sql_file\": \"rf_2_inundation\"\n",
    "        }\n",
    "      ],\n",
    "      \"services\": [\n",
    "        \"static_nwm_aep_inundation_extent_library_noaa\"\n",
    "      ],\n",
    "      \"raster_outputs\": {\n",
    "        \"output_bucket\": \"\",\n",
    "        \"output_raster_workspaces\": []\n",
    "      },\n",
    "      \"postprocess_sql\": [],\n",
    "      \"product_summaries\": [],\n",
    "      \"lambda_max_flow_dependent\": False\n",
    "    }\n",
    "  ],\n",
    "  \"sql_rename_dict\": {}\n",
    "}\n",
    "\n",
    "STEPFUNCTION_CLIENT.start_execution(\n",
    "    stateMachineArn = PIPELINE_ARN,\n",
    "    name = f\"sagemaker_aep_2_{datetime.now().strftime('%Y%m%dT%H%M')}\",\n",
    "    input= json.dumps(pipeline_input)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f89d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_input = {\n",
    "  \"configuration\": \"reference\",\n",
    "  \"job_type\": \"auto\",\n",
    "  \"data_type\": \"channel\",\n",
    "  \"keep_raw\": False,\n",
    "  \"reference_time\": datetime.now().strftime('%Y-%m-%d 00:00:00'),\n",
    "  \"configuration_data_flow\": {\n",
    "    \"db_max_flows\": [],\n",
    "    \"db_ingest_groups\": [],\n",
    "    \"lambda_max_flows\": []\n",
    "  },\n",
    "  \"pipeline_products\": [\n",
    "    {\n",
    "      \"product\": \"static_nwm_aep_inundation_extent_library\",\n",
    "      \"configuration\": \"reference\",\n",
    "      \"product_type\": \"fim\",\n",
    "      \"run\": True,\n",
    "      \"fim_configs\": [\n",
    "        {\n",
    "          \"name\": \"rf_5_inundation\",\n",
    "          \"target_table\": \"aep_fim.rf_5_inundation\",\n",
    "          \"fim_type\": \"hand\",\n",
    "          \"sql_file\": \"rf_5_inundation\"\n",
    "        }\n",
    "      ],\n",
    "      \"services\": [\n",
    "        \"static_nwm_aep_inundation_extent_library_noaa\"\n",
    "      ],\n",
    "      \"raster_outputs\": {\n",
    "        \"output_bucket\": \"\",\n",
    "        \"output_raster_workspaces\": []\n",
    "      },\n",
    "      \"postprocess_sql\": [],\n",
    "      \"product_summaries\": [],\n",
    "      \"lambda_max_flow_dependent\": False\n",
    "    }\n",
    "  ],\n",
    "  \"sql_rename_dict\": {}\n",
    "}\n",
    "\n",
    "STEPFUNCTION_CLIENT.start_execution(\n",
    "    stateMachineArn = PIPELINE_ARN,\n",
    "    name = f\"sagemaker_aep_5_{datetime.now().strftime('%Y%m%dT%H%M')}\",\n",
    "    input= json.dumps(pipeline_input)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791d1a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_input = {\n",
    "  \"configuration\": \"reference\",\n",
    "  \"job_type\": \"auto\",\n",
    "  \"data_type\": \"channel\",\n",
    "  \"keep_raw\": False,\n",
    "  \"reference_time\": datetime.now().strftime('%Y-%m-%d 00:00:00'),\n",
    "  \"configuration_data_flow\": {\n",
    "    \"db_max_flows\": [],\n",
    "    \"db_ingest_groups\": [],\n",
    "    \"lambda_max_flows\": []\n",
    "  },\n",
    "  \"pipeline_products\": [\n",
    "    {\n",
    "      \"product\": \"static_nwm_aep_inundation_extent_library\",\n",
    "      \"configuration\": \"reference\",\n",
    "      \"product_type\": \"fim\",\n",
    "      \"run\": True,\n",
    "      \"fim_configs\": [\n",
    "        {\n",
    "          \"name\": \"rf_10_inundation\",\n",
    "          \"target_table\": \"aep_fim.rf_10_inundation\",\n",
    "          \"fim_type\": \"hand\",\n",
    "          \"sql_file\": \"rf_10_inundation\"\n",
    "        }\n",
    "      ],\n",
    "      \"services\": [\n",
    "        \"static_nwm_aep_inundation_extent_library_noaa\"\n",
    "      ],\n",
    "      \"raster_outputs\": {\n",
    "        \"output_bucket\": \"\",\n",
    "        \"output_raster_workspaces\": []\n",
    "      },\n",
    "      \"postprocess_sql\": [],\n",
    "      \"product_summaries\": [],\n",
    "      \"lambda_max_flow_dependent\": False\n",
    "    }\n",
    "  ],\n",
    "  \"sql_rename_dict\": {}\n",
    "}\n",
    "\n",
    "STEPFUNCTION_CLIENT.start_execution(\n",
    "    stateMachineArn = PIPELINE_ARN,\n",
    "    name = f\"sagemaker_aep_10_{datetime.now().strftime('%Y%m%dT%H%M')}\",\n",
    "    input= json.dumps(pipeline_input)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb87128",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_input = {\n",
    "  \"configuration\": \"reference\",\n",
    "  \"job_type\": \"auto\",\n",
    "  \"data_type\": \"channel\",\n",
    "  \"keep_raw\": False,\n",
    "  \"reference_time\": datetime.now().strftime('%Y-%m-%d 00:00:00'),\n",
    "  \"configuration_data_flow\": {\n",
    "    \"db_max_flows\": [],\n",
    "    \"db_ingest_groups\": [],\n",
    "    \"lambda_max_flows\": []\n",
    "  },\n",
    "  \"pipeline_products\": [\n",
    "    {\n",
    "      \"product\": \"static_nwm_aep_inundation_extent_library\",\n",
    "      \"configuration\": \"reference\",\n",
    "      \"product_type\": \"fim\",\n",
    "      \"run\": True,\n",
    "      \"fim_configs\": [\n",
    "        {\n",
    "          \"name\": \"rf_25_inundation\",\n",
    "          \"target_table\": \"aep_fim.rf_25_inundation\",\n",
    "          \"fim_type\": \"hand\",\n",
    "          \"sql_file\": \"rf_25_inundation\"\n",
    "        }\n",
    "      ],\n",
    "      \"services\": [\n",
    "        \"static_nwm_aep_inundation_extent_library_noaa\"\n",
    "      ],\n",
    "      \"raster_outputs\": {\n",
    "        \"output_bucket\": \"\",\n",
    "        \"output_raster_workspaces\": []\n",
    "      },\n",
    "      \"postprocess_sql\": [],\n",
    "      \"product_summaries\": [],\n",
    "      \"lambda_max_flow_dependent\": False\n",
    "    }\n",
    "  ],\n",
    "  \"sql_rename_dict\": {}\n",
    "}\n",
    "\n",
    "STEPFUNCTION_CLIENT.start_execution(\n",
    "    stateMachineArn = PIPELINE_ARN,\n",
    "    name = f\"sagemaker_aep_25_{datetime.now().strftime('%Y%m%dT%H%M')}\",\n",
    "    input= json.dumps(pipeline_input)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4832e4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_input = {\n",
    "  \"configuration\": \"reference\",\n",
    "  \"job_type\": \"auto\",\n",
    "  \"data_type\": \"channel\",\n",
    "  \"keep_raw\": False,\n",
    "  \"reference_time\": datetime.now().strftime('%Y-%m-%d 00:00:00'),\n",
    "  \"configuration_data_flow\": {\n",
    "    \"db_max_flows\": [],\n",
    "    \"db_ingest_groups\": [],\n",
    "    \"lambda_max_flows\": []\n",
    "  },\n",
    "  \"pipeline_products\": [\n",
    "    {\n",
    "      \"product\": \"static_nwm_aep_inundation_extent_library\",\n",
    "      \"configuration\": \"reference\",\n",
    "      \"product_type\": \"fim\",\n",
    "      \"run\": True,\n",
    "      \"fim_configs\": [\n",
    "        {\n",
    "          \"name\": \"rf_50_inundation\",\n",
    "          \"target_table\": \"aep_fim.rf_50_inundation\",\n",
    "          \"fim_type\": \"hand\",\n",
    "          \"sql_file\": \"rf_50_inundation\"\n",
    "        }\n",
    "      ],\n",
    "      \"services\": [\n",
    "        \"static_nwm_aep_inundation_extent_library_noaa\"\n",
    "      ],\n",
    "      \"raster_outputs\": {\n",
    "        \"output_bucket\": \"\",\n",
    "        \"output_raster_workspaces\": []\n",
    "      },\n",
    "      \"postprocess_sql\": [],\n",
    "      \"product_summaries\": [],\n",
    "      \"lambda_max_flow_dependent\": False\n",
    "    }\n",
    "  ],\n",
    "  \"sql_rename_dict\": {}\n",
    "}\n",
    "\n",
    "STEPFUNCTION_CLIENT.start_execution(\n",
    "    stateMachineArn = PIPELINE_ARN,\n",
    "    name = f\"sagemaker_aep_50_{datetime.now().strftime('%Y%m%dT%H%M')}\",\n",
    "    input= json.dumps(pipeline_input)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a698067",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_input = {\n",
    "  \"configuration\": \"reference\",\n",
    "  \"job_type\": \"auto\",\n",
    "  \"data_type\": \"channel\",\n",
    "  \"keep_raw\": False,\n",
    "  \"reference_time\": datetime.now().strftime('%Y-%m-%d 00:00:00'),\n",
    "  \"configuration_data_flow\": {\n",
    "    \"db_max_flows\": [],\n",
    "    \"db_ingest_groups\": [],\n",
    "    \"lambda_max_flows\": []\n",
    "  },\n",
    "  \"pipeline_products\": [\n",
    "    {\n",
    "      \"product\": \"static_nwm_aep_inundation_extent_library\",\n",
    "      \"configuration\": \"reference\",\n",
    "      \"product_type\": \"fim\",\n",
    "      \"run\": True,\n",
    "      \"fim_configs\": [\n",
    "        {\n",
    "          \"name\": \"rf_high_water_inundation\",\n",
    "          \"target_table\": \"aep_fim.rf_high_water_inundation\",\n",
    "          \"fim_type\": \"hand\",\n",
    "          \"sql_file\": \"rf_high_water_inundation\"\n",
    "        }\n",
    "      ],\n",
    "      \"services\": [\n",
    "        \"static_nwm_aep_inundation_extent_library_noaa\"\n",
    "      ],\n",
    "      \"raster_outputs\": {\n",
    "        \"output_bucket\": \"\",\n",
    "        \"output_raster_workspaces\": []\n",
    "      },\n",
    "      \"postprocess_sql\": [],\n",
    "      \"product_summaries\": [],\n",
    "      \"lambda_max_flow_dependent\": False\n",
    "    }\n",
    "  ],\n",
    "  \"sql_rename_dict\": {}\n",
    "}\n",
    "\n",
    "STEPFUNCTION_CLIENT.start_execution(\n",
    "    stateMachineArn = PIPELINE_ARN,\n",
    "    name = f\"sagemaker_aep_hw_{datetime.now().strftime('%Y%m%dT%H%M')}\",\n",
    "    input= json.dumps(pipeline_input)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1124d0d4",
   "metadata": {},
   "source": [
    "<h2>6 - RUN CATCHMENT WORKFLOWS 2 CONFIGS AT A TIME. CHECK FOR STEP FUNCTION FINISHING BEFORE STARTING NEW ONE</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7495759",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_input = {\n",
    "  \"configuration\": \"reference\",\n",
    "  \"job_type\": \"auto\",\n",
    "  \"data_type\": \"channel\",\n",
    "  \"keep_raw\": False,\n",
    "  \"reference_time\": datetime.now().strftime('%Y-%m-%d 00:00:00'),\n",
    "  \"configuration_data_flow\": {\n",
    "    \"db_max_flows\": [],\n",
    "    \"db_ingest_groups\": [],\n",
    "    \"lambda_max_flows\": []\n",
    "  },\n",
    "  \"pipeline_products\": [\n",
    "    {\n",
    "      \"product\": \"static_hand_catchments_0_branches\",\n",
    "      \"configuration\": \"reference\",\n",
    "      \"product_type\": \"fim\",\n",
    "      \"run\": True,\n",
    "      \"fim_configs\": [\n",
    "        {\n",
    "          \"name\": \"branch_0_catchments\",\n",
    "          \"target_table\": \"fim_catchments.branch_0_catchments\",\n",
    "          \"fim_type\": \"hand\",\n",
    "          \"sql_file\": \"branch_0_catchments\"\n",
    "        }\n",
    "      ],\n",
    "      \"services\": [\n",
    "        \"static_hand_catchments_0_branches_noaa\"\n",
    "      ],\n",
    "      \"raster_outputs\": {\n",
    "        \"output_bucket\": \"\",\n",
    "        \"output_raster_workspaces\": []\n",
    "      },\n",
    "      \"postprocess_sql\": [],\n",
    "      \"product_summaries\": [],\n",
    "      \"lambda_max_flow_dependent\": False\n",
    "    },\n",
    "    {\n",
    "      \"product\": \"static_hand_catchments_0_branches_hi\",\n",
    "      \"configuration\": \"reference\",\n",
    "      \"product_type\": \"fim\",\n",
    "      \"run\": True,\n",
    "      \"fim_configs\": [\n",
    "        {\n",
    "          \"name\": \"branch_0_catchments_hi\",\n",
    "          \"target_table\": \"fim_catchments.branch_0_catchments_hi\",\n",
    "          \"fim_type\": \"hand\",\n",
    "          \"sql_file\": \"branch_0_catchments_hi\"\n",
    "        }\n",
    "      ],\n",
    "      \"services\": [\n",
    "        \"static_hand_catchments_0_branches_hi_noaa\"\n",
    "      ],\n",
    "      \"raster_outputs\": {\n",
    "        \"output_bucket\": \"\",\n",
    "        \"output_raster_workspaces\": []\n",
    "      },\n",
    "      \"postprocess_sql\": [],\n",
    "      \"product_summaries\": [],\n",
    "      \"lambda_max_flow_dependent\": False\n",
    "    },\n",
    "    {\n",
    "      \"product\": \"static_hand_catchments_0_branches_prvi\",\n",
    "      \"configuration\": \"reference\",\n",
    "      \"product_type\": \"fim\",\n",
    "      \"run\": True,\n",
    "      \"fim_configs\": [\n",
    "        {\n",
    "          \"name\": \"branch_0_catchments_prvi\",\n",
    "          \"target_table\": \"fim_catchments.branch_0_catchments_prvi\",\n",
    "          \"fim_type\": \"hand\",\n",
    "          \"sql_file\": \"branch_0_catchments_prvi\"\n",
    "        }\n",
    "      ],\n",
    "      \"services\": [\n",
    "        \"static_hand_catchments_0_branches_prvi_noaa\"\n",
    "      ],\n",
    "      \"raster_outputs\": {\n",
    "        \"output_bucket\": \"\",\n",
    "        \"output_raster_workspaces\": []\n",
    "      },\n",
    "      \"postprocess_sql\": [],\n",
    "      \"product_summaries\": [],\n",
    "      \"lambda_max_flow_dependent\": False\n",
    "    }\n",
    "  ],\n",
    "  \"sql_rename_dict\": {}\n",
    "}\n",
    "\n",
    "STEPFUNCTION_CLIENT.start_execution(\n",
    "    stateMachineArn = PIPELINE_ARN,\n",
    "    name = f\"sagemaker_0_catchments_{datetime.now().strftime('%Y%m%dT%H%M')}\",\n",
    "    input= json.dumps(pipeline_input)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae8917f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_input = {\n",
    "  \"configuration\": \"reference\",\n",
    "  \"job_type\": \"auto\",\n",
    "  \"data_type\": \"channel\",\n",
    "  \"keep_raw\": False,\n",
    "  \"reference_time\": datetime.now().strftime('%Y-%m-%d 00:00:00'),\n",
    "  \"configuration_data_flow\": {\n",
    "    \"db_max_flows\": [],\n",
    "    \"db_ingest_groups\": [],\n",
    "    \"lambda_max_flows\": []\n",
    "  },\n",
    "  \"pipeline_products\": [\n",
    "    {\n",
    "      \"product\": \"static_hand_catchments_gms_branches\",\n",
    "      \"configuration\": \"reference\",\n",
    "      \"product_type\": \"fim\",\n",
    "      \"run\": True,\n",
    "      \"fim_configs\": [\n",
    "        {\n",
    "          \"name\": \"branch_gms_catchments\",\n",
    "          \"target_table\": \"fim_catchments.branch_gms_catchments\",\n",
    "          \"fim_type\": \"hand\",\n",
    "          \"sql_file\": \"branch_gms_catchments\"\n",
    "        }\n",
    "      ],\n",
    "      \"services\": [\n",
    "        \"static_hand_catchments_gms_branches_noaa\"\n",
    "      ],\n",
    "      \"raster_outputs\": {\n",
    "        \"output_bucket\": \"\",\n",
    "        \"output_raster_workspaces\": []\n",
    "      },\n",
    "      \"postprocess_sql\": [],\n",
    "      \"product_summaries\": [],\n",
    "      \"lambda_max_flow_dependent\": False\n",
    "    },\n",
    "    {\n",
    "      \"product\": \"static_hand_catchments_gms_branches_hi\",\n",
    "      \"configuration\": \"reference\",\n",
    "      \"product_type\": \"fim\",\n",
    "      \"run\": True,\n",
    "      \"fim_configs\": [\n",
    "        {\n",
    "          \"name\": \"branch_gms_catchments_hi\",\n",
    "          \"target_table\": \"fim_catchments.branch_gms_catchments_hi\",\n",
    "          \"fim_type\": \"hand\",\n",
    "          \"sql_file\": \"branch_gms_catchments_hi\"\n",
    "        }\n",
    "      ],\n",
    "      \"services\": [\n",
    "        \"static_hand_catchments_gms_branches_hi_noaa\"\n",
    "      ],\n",
    "      \"raster_outputs\": {\n",
    "        \"output_bucket\": \"\",\n",
    "        \"output_raster_workspaces\": []\n",
    "      },\n",
    "      \"postprocess_sql\": [],\n",
    "      \"product_summaries\": [],\n",
    "      \"lambda_max_flow_dependent\": False\n",
    "    },\n",
    "    {\n",
    "      \"product\": \"static_hand_catchments_gms_branches_prvi\",\n",
    "      \"configuration\": \"reference\",\n",
    "      \"product_type\": \"fim\",\n",
    "      \"run\": True,\n",
    "      \"fim_configs\": [\n",
    "        {\n",
    "          \"name\": \"branch_gms_catchments_prvi\",\n",
    "          \"target_table\": \"fim_catchments.branch_gms_catchments_prvi\",\n",
    "          \"fim_type\": \"hand\",\n",
    "          \"sql_file\": \"branch_gms_catchments_prvi\"\n",
    "        }\n",
    "      ],\n",
    "      \"services\": [\n",
    "        \"static_hand_catchments_gms_branches_prvi_noaa\"\n",
    "      ],\n",
    "      \"raster_outputs\": {\n",
    "        \"output_bucket\": \"\",\n",
    "        \"output_raster_workspaces\": []\n",
    "      },\n",
    "      \"postprocess_sql\": [],\n",
    "      \"product_summaries\": [],\n",
    "      \"lambda_max_flow_dependent\": False\n",
    "    }\n",
    "  ],\n",
    "  \"sql_rename_dict\": {}\n",
    "}\n",
    "\n",
    "STEPFUNCTION_CLIENT.start_execution(\n",
    "    stateMachineArn = PIPELINE_ARN,\n",
    "    name = f\"sagemaker_gms_catchments_{datetime.now().strftime('%Y%m%dT%H%M')}\",\n",
    "    input= json.dumps(pipeline_input)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75d51cc",
   "metadata": {},
   "source": [
    "<h2>6 - UPDATE RATING CURVES IN DB</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcc72f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sql = '''\n",
    "DROP TABLE IF EXISTS derived.hydrotable;\n",
    "DROP TABLE IF EXISTS derived.usgs_elev_table;\n",
    "\n",
    "SELECT distinct LPAD(huc8::text, 8, '0') as huc8 FROM derived.featureid_huc_crosswalk WHERE huc8 is not null;\n",
    "'''\n",
    "df = run_sql_in_db(sql)\n",
    "\n",
    "col_names = ['HydroID', 'feature_id', 'NextDownID', 'order_', 'Number of Cells',\n",
    "       'SurfaceArea (m2)', 'BedArea (m2)', 'TopWidth (m)', 'LENGTHKM',\n",
    "       'AREASQKM', 'WettedPerimeter (m)', 'HydraulicRadius (m)',\n",
    "       'WetArea (m2)', 'Volume (m3)', 'SLOPE', 'ManningN', 'stage',\n",
    "       'default_discharge_cms', 'default_Volume (m3)', 'default_WetArea (m2)',\n",
    "       'default_HydraulicRadius (m)', 'default_ManningN',\n",
    "       'precalb_discharge_cms', 'calb_coef_spatial', 'HUC', 'LakeID',\n",
    "       'subdiv_applied', 'channel_n', 'overbank_n', 'subdiv_discharge_cms',\n",
    "       'last_updated', 'submitter', 'calb_coef_usgs', 'obs_source',\n",
    "       'calb_coef_final', 'calb_applied', 'discharge_cms']\n",
    "\n",
    "usecols = ['HydroID', 'feature_id', 'stage', 'discharge_cms']\n",
    "\n",
    "paginator = S3_CLIENT.get_paginator('list_objects')\n",
    "operation_parameters = {'Bucket': FIM_BUCKET,\n",
    "                        'Prefix': f'{HAND_DATASETS_DPATH}/',\n",
    "                        'Delimiter': '/'}\n",
    "page_iterator = paginator.paginate(**operation_parameters)\n",
    "page_count = 0\n",
    "for page in page_iterator:\n",
    "    page_count += 1\n",
    "    prefix_objects = page['CommonPrefixes']\n",
    "    for i, prefix_obj in enumerate(prefix_objects):\n",
    "        print(f\"Processing {i+1} of {len(prefix_objects)} on page {page_count}\")\n",
    "        branch_prefix = f'{prefix_obj.get(\"Prefix\")}branches/0/'\n",
    "#         ## UNCOMMENT FOR ALL BRANCHES - NOT JUST 0\n",
    "#         huc_branches_prefix = f'{prefix_obj.get(\"Prefix\")}branches/'\n",
    "#         branches_result = S3_CLIENT.list_objects(Bucket=FIM_BUCKET, Prefix=huc_branches_prefix, Delimiter='/')\n",
    "#         branch_prefix_objects = branches_result.get('CommonPrefixes')\n",
    "#         for i, branch_prefix_obj in enumerate(branch_prefix_objects):\n",
    "#             branch_prefix = branch_prefix_obj['Prefix']\n",
    "#         ## END UNCOMMENT\n",
    "        ## [UN]INDENT FROM HERE TO THE END IF [COMMENTED]UNCOMMENTED\n",
    "        branch_files_result = S3_CLIENT.list_objects(Bucket=FIM_BUCKET, Prefix=branch_prefix, Delimiter='/')\n",
    "        hydro_table_key = None\n",
    "        usgs_elev_table_key = None\n",
    "        for content_obj in branch_files_result.get('Contents'):\n",
    "            branch_file_prefix = content_obj['Key']\n",
    "            if 'hydroTable' in branch_file_prefix:\n",
    "                hydro_table_key = branch_file_prefix\n",
    "            elif 'usgs_elev_table.csv' in branch_file_prefix:\n",
    "                usgs_elev_table_key = branch_file_prefix\n",
    "\n",
    "        if hydro_table_key and usgs_elev_table_key:\n",
    "#             print(f\"Found usgs_elev_table and hydroTable in {branch_prefix}\")\n",
    "            try:\n",
    "#                 print(\"...Fetching csvs...\")\n",
    "                uet = S3_CLIENT.get_object(Bucket=FIM_BUCKET, Key=usgs_elev_table_key)['Body']\n",
    "                ht = S3_CLIENT.get_object(Bucket=FIM_BUCKET, Key=hydro_table_key)['Body']\n",
    "#                 print(\"...Reading with pandas...\")\n",
    "                uet_df = pd.read_csv(uet)\n",
    "                ht_df = pd.read_csv(ht, header=0, names=col_names, usecols=usecols)\n",
    "#                 print('...Writing to db...')\n",
    "                uet_df['fim_version'] = FIM_VERSION\n",
    "                try:\n",
    "                    drop_indicies = []\n",
    "                    for index, row in uet_df.iterrows():\n",
    "                        if 'HydroID' in row and row['HydroID']:\n",
    "                            iter_hydro_id = row['HydroID']\n",
    "#                             print(f\"Subsetting hydrotable where HydroID == {iter_hydro_id}\")\n",
    "                            ht_df_iter = ht_df[ht_df['HydroID']==iter_hydro_id]\n",
    "                            if ht_df_iter.empty:\n",
    "                                drop_indicies.append(index)\n",
    "                                continue\n",
    "                            ht_df_iter['fim_version'] = FIM_VERSION\n",
    "                            ht_df_iter.to_sql(con=VIZ_DB_ENGINE, schema='derived', name='hydrotable', index=False, if_exists='append')\n",
    "                        else:\n",
    "                            drop_indicies.append(index)\n",
    "                    mod_uet_df = uet_df.drop(drop_indicies)\n",
    "                    mod_uet_df.to_sql(con=VIZ_DB_ENGINE, dtype={\"location_id\": Text(), \"nws_data_huc\": Text()}, schema='derived', name='usgs_elev_table', index=False, if_exists='append')\n",
    "                except Exception as e:\n",
    "                    print('******************************************')\n",
    "                    print(f'Error encountered on {branch_file_prefix}')\n",
    "                    print(e)\n",
    "                    print('******************************************')       \n",
    "            except Exception as e:\n",
    "                raise e\n",
    "                print(f'Fetch failed: {e}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a162bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "DROP TABLE IF EXISTS derived.hydrotable_staggered;\n",
    "SELECT\n",
    "    et.location_id,\n",
    "    ht.feature_id,\n",
    "    (stage + et.dem_adj_elevation) * 3.28084 as elevation_ft,\n",
    "    LEAD((stage + et.dem_adj_elevation) * 3.28084) OVER (PARTITION BY ht.feature_id ORDER BY ht.feature_id, stage) as next_elevation_ft,\n",
    "    discharge_cms * 35.3147 as discharge_cfs,\n",
    "    LEAD(discharge_cms * 35.3147) OVER (PARTITION BY ht.feature_id ORDER BY ht.feature_id, stage) as next_discharge_cfs\n",
    "INTO derived.hydrotable_staggered\n",
    "FROM derived.hydrotable AS ht\n",
    "JOIN derived.usgs_elev_table AS et ON ht.\"HydroID\" = et.\"HydroID\" AND et.location_id IS NOT NULL;\n",
    "'''\n",
    "\n",
    "run_sql_in_db(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d23f83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "usgs_curves_key = f'{QA_DATASETS_DPATH}/usgs_rating_curves.csv'\n",
    "obj_body = S3_CLIENT.get_object(Bucket=FIM_BUCKET, Key=usgs_curves_key)['Body']\n",
    "df = pd.read_csv(obj_body)\n",
    "df.to_sql(con=VIZ_DB_ENGINE, schema='derived', name='usgs_rating_curves', index=False, if_exists='replace', chunksize=150000, method='multi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673b1aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "DROP TABLE IF EXISTS derived.usgs_rating_curves_staggered;\n",
    "SELECT \n",
    "    location_id,\n",
    "    flow as discharge_cfs, \n",
    "    LEAD(flow) OVER (PARTITION BY location_id ORDER BY location_id, stage) as next_discharge_cfs,\n",
    "    stage,\n",
    "    navd88_datum,\n",
    "    elevation_navd88 as elevation_ft,\n",
    "    LEAD(elevation_navd88) OVER (PARTITION BY location_id ORDER BY location_id, stage) as next_elevation_ft\n",
    "INTO derived.usgs_rating_curves_staggered\n",
    "FROM derived.usgs_rating_curves;\n",
    "'''\n",
    "\n",
    "run_sql_in_db(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd95566",
   "metadata": {},
   "source": [
    "<h2>7 - UPDATE SRC SKILL METRICS IN DB</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645e99af",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_stats_key = f'{QA_DATASETS_DPATH}/agg_nwm_recurr_flow_elev_stats_location_id.csv'\n",
    "obj_body = S3_CLIENT.get_object(Bucket=FIM_BUCKET, Key=src_stats_key)['Body']\n",
    "df = pd.read_csv(obj_body)\n",
    "# df['fim_version'] = FIM_VERSION\n",
    "print(df.to_dict('records'))\n",
    "# df.to_sql(con=VIZ_DB_ENGINE, schema='derived', name='src_skill', index=False, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116df1bb-5c63-4377-8113-1d850f10d934",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from helper_functions.shared_funcs import execute_sql"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5632191",
   "metadata": {},
   "source": [
    "<h2>8 - UPDATE FIM PERFORMANCE METRICS IN DB</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ca5581",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from helper_functions.shared_functions import *\n",
    "import boto3\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "from geoalchemy2 import Geometry\n",
    "from geopandas import GeoDataFrame\n",
    "\n",
    "# os.environ['EGIS_DB_HOST'] =''  #TI DB\n",
    "\n",
    "db_type = \"egis\"\n",
    "db_engine = get_db_engine(db_type)\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Define bucket and parent directories.\n",
    "bucket = \"hydrovis-ti-deployment-us-east-1\"\n",
    "parent_directory = \"qc_fim_data\"\n",
    "local_download_parent_directory = f'brad_data/qc_fim_data'\n",
    "\n",
    "#file_handles = ['fim_performance_points.csv']\n",
    "\n",
    "file_handles = ['fim_performance_points.csv', 'fim_performance_polys.csv', 'fim_performance_catchments_dissolved.csv']\n",
    "\n",
    "#file_handles = ['fim_performance_catchments_dissolved.csv']\n",
    "\n",
    "for file_handle in file_handles:\n",
    "    # Define path to file to download and its local download path, the download.\n",
    "    filename = f\"{QA_DATASETS_DPATH}/{file_handle}\"\n",
    "    print(filename)\n",
    "    local_download_path = os.path.join(local_download_parent_directory, f'{file_handle}')\n",
    "    print(f\"--> Downloading {FIM_BUCKET}/{filename} to {local_download_path}\")\n",
    "    #s3.download_file(FIM_BUCKET, filename, local_download_path)\n",
    "    \n",
    "    if file_handle == 'fim_performance_catchments_dissolved.csv':\n",
    "        file_handle = 'fim_performance_catchments.csv'\n",
    "    \n",
    "    #  -- Open file and reformat -- #\n",
    "    print(\"Reading file...\")\n",
    "    df = pd.read_csv(local_download_path)\n",
    "    print(\"File read.\")\n",
    "    # Rename headers.\n",
    "    if file_handle == 'fim_performance_points.csv':\n",
    "        df = df.rename(columns={'Unnamed: 0': 'oid', 'geometry': 'geom'})\n",
    "    else:\n",
    "        df = df.rename(columns={'Unnamed: 0': 'oid', 'geometry': 'geom', 'huc':'huc8'})\n",
    "    \n",
    "    print(df.dtypes)\n",
    "    # Convert all field names to lowercase (needed for ArcGIS Pro).\n",
    "    df.columns= df.columns.str.lower()\n",
    "\n",
    "    # Enforce data types on df before loading in DB (TODO: need to create special cases for each layer).\n",
    "    if file_handle == 'fim_performance_points.csv':\n",
    "        df = df.astype({'huc': 'str'})\n",
    "    else:\n",
    "        df = df.astype({'huc8': 'str'})\n",
    "    df = df.fillna(0)\n",
    "    try:\n",
    "        df = df.astype({'feature_id': 'int'})\n",
    "        df = df.astype({'feature_id': 'str'})\n",
    "        df = df.astype({'oid': 'int'})\n",
    "    except KeyError:  # If there is no feature_id field\n",
    "        pass\n",
    "    try:\n",
    "        df = df.astype({'nwm_seg': 'int'})\n",
    "        df = df.astype({'nwm_seg': 'str'})\n",
    "    except KeyError:  # If there is no nwm_seg field\n",
    "        pass\n",
    "    try:\n",
    "        df = df.astype({'usgs_gage': 'int'})\n",
    "        df = df.astype({'usgs_gage': 'str'})\n",
    "    except KeyError:  # If there is no usgs_gage field\n",
    "        pass\n",
    "        \n",
    "    # zfill HUC8 field.\n",
    "    if file_handle == 'fim_performance_points.csv':\n",
    "        df['huc'] = df['huc'].apply(lambda x: x.zfill(8))\n",
    "    else:\n",
    "        df['huc8'] = df['huc8'].apply(lambda x: x.zfill(8))\n",
    " \n",
    "    # Upload df to database.\n",
    "    stripped_layer_name = file_handle.replace(\".csv\", \"\")\n",
    "    table_name = \"reference.\" + stripped_layer_name\n",
    "    print(\"Loading data into DB...\")\n",
    "    \n",
    "    # Chunk load data into DB\n",
    "    if file_handle in ['fim_performance_catchments.csv']:\n",
    "        df['version'] = FIM_VERSION\n",
    "        print(\"Chunk loading...\")\n",
    "        # Create list of df chunks\n",
    "        n = 10000  #chunk row size\n",
    "        list_df = [df[i:i+n] for i in range(0,df.shape[0],n)]\n",
    "        #geometry = 'MULTIPOLYGON'\n",
    "        # Load the first chunk into the DB as a new table\n",
    "        first_chunk_df = list_df[0]\n",
    "        print(first_chunk_df.shape[0])\n",
    "        \n",
    "        first_chunk_df.to_sql(\n",
    "            name=stripped_layer_name, \n",
    "            con=db_engine, \n",
    "            schema='reference',\n",
    "            if_exists='replace', \n",
    "            index=False,\n",
    "            dtype={'oid': sqlalchemy.types.Integer(),\n",
    "                   'version': sqlalchemy.types.String(), \n",
    "                   'geom': Geometry('MULTIPOLYGON',srid=3857)\n",
    "                  }\n",
    "        )\n",
    "        # Load remaining chunks into newly created table\n",
    "        \n",
    "        for remaining_chunk_df in list_df[1:]:\n",
    "            print(remaining_chunk_df.shape[0])\n",
    "            remaining_chunk_df.to_sql(\n",
    "                name=stripped_layer_name, \n",
    "                con=db_engine, \n",
    "                schema='reference',\n",
    "                if_exists='append', \n",
    "                index=False,\n",
    "                dtype={'oid': sqlalchemy.types.Integer(),\n",
    "                       'version': sqlalchemy.types.String(), \n",
    "                       'geom': Geometry('MULTIPOLYGON',srid=3857)\n",
    "                      }\n",
    "            )\n",
    "    else:\n",
    "        if 'points' in stripped_layer_name: geometry = 'POINT'\n",
    "        if 'polys' in stripped_layer_name: geometry = 'POLYGON'\n",
    "        print(\"GEOMETRY\")\n",
    "        print(geometry)\n",
    "        df.to_sql(\n",
    "            name=stripped_layer_name, \n",
    "            con=db_engine, \n",
    "            schema='reference',\n",
    "            if_exists='replace', \n",
    "            index=False,\n",
    "            dtype={'oid': sqlalchemy.types.Integer(),\n",
    "                   'version': sqlalchemy.types.String(), \n",
    "                   'geom': Geometry(geometry,srid=3857)\n",
    "                  }\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ca031d",
   "metadata": {},
   "source": [
    "<h2>9 - UPDATE STAGE-BASED CATFIM DATA IN DB</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6210f1-f3c4-489a-aa1f-53646d7d0f59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os \n",
    "from helper_functions.shared_functions import * \n",
    "import boto3 \n",
    "import geopandas as gpd \n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "from geoalchemy2 import Geometry\n",
    "\n",
    "db_type = \"egis\" \n",
    "db_engine = get_db_engine(db_type)\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Define bucket and parent directories.\n",
    "bucket = \"hydrovis-ti-deployment-us-east-1\"\n",
    "parent_directory = \"qc_fim_data\"\n",
    "local_download_parent_directory = f'brad_data/qc_fim_data'\n",
    "\n",
    "#file_handles = ['stage_based_catfim_sites.csv', 'stage_based_catfim.csv']\n",
    "\n",
    "#file_handles = ['stage_based_catfim_sites.csv', 'catfim_library_dissolved.csv']\n",
    "\n",
    "file_handles = ['catfim_library_dissolved.csv']\n",
    "\n",
    "for file_handle in file_handles: \n",
    "    \n",
    "    # Define path to file to download and its local download path, then download. \n",
    "    filename = f\"{QA_DATASETS_DPATH}/{file_handle}\" \n",
    "    print(filename) \n",
    "    local_download_path = os.path.join(local_download_parent_directory, f'{file_handle}') \n",
    "    print(f\"--> Downloading {FIM_BUCKET}/{filename} to {local_download_path}\") \n",
    "    #s3.download_file(FIM_BUCKET, filename, local_download_path)\n",
    "\n",
    "    #  -- Open file and reformat -- #\n",
    "    print(\"Reading file...\")\n",
    "    df = pd.read_csv(local_download_path)\n",
    "    print(\"File read.\")\n",
    "\n",
    "    if file_handle == 'catfim_library_dissolved.csv':\n",
    "        file_handle = 'stage_based_catfim.csv'\n",
    "    # Rename headers.\n",
    "    df = df.rename(columns={'Unnamed: 0': 'oid', 'geometry': 'geom', 'huc':'huc8'})\n",
    "\n",
    "    # Convert all field names to lowercase (needed for ArcGIS Pro).\n",
    "    df.columns= df.columns.str.lower()\n",
    "\n",
    "    # Remove sites that are in derived.ahps_restricted_sites\n",
    "    restricted_sites_df = get_db_values(\"derived.ahps_restricted_sites\", [\"*\"])\n",
    "    restricted_dict = restricted_sites_df.to_dict('records')\n",
    "\n",
    "    # Change 'mapped' to 'no' if sites are present in restricted_sites_df\n",
    "    for site in restricted_dict:\n",
    "        nws_lid = site['nws_lid'].lower()\n",
    "        if \"sites\" in file_handle:\n",
    "            df.loc[df.ahps_lid==nws_lid, 'mapped'] = 'no'\n",
    "            df.loc[df.ahps_lid==nws_lid, 'status'] = site['restricted_reason']\n",
    "        else:\n",
    "            df.loc[df.ahps_lid==nws_lid, 'viz'] = 'no'\n",
    "            df = df[df['viz']=='yes']  # Subset df to only sites desired for mapping\n",
    "\n",
    "    for sea_level_site in ['qutg1', 'augg1', 'baxg1', 'lamf1', 'adlg1', 'hrag1', 'stng1']:\n",
    "        if \"sites\" in file_handle:\n",
    "            df.loc[df.ahps_lid==sea_level_site, 'mapped'] = 'no'\n",
    "            df.loc[df.ahps_lid==sea_level_site, 'status'] = 'Stage thresholds seem to be based on sea level and not channel thalweg'\n",
    "        else:\n",
    "            df.loc[df.ahps_lid==sea_level_site, 'viz'] = 'no'\n",
    "            df = df[df['viz']=='yes']  # Subset df to only sites desired for mapping\n",
    "            \n",
    "    # Enforce data types on df before loading in DB (TODO: need to create special cases for each layer).\n",
    "    df = df.astype({'huc8': 'str'})\n",
    "    df = df.fillna(0)\n",
    "\n",
    "    if file_handle == 'stage_based_catfim.csv':\n",
    "        df['fim_version'] = FIM_VERSION\n",
    "\n",
    "    try:\n",
    "        df = df.astype({'feature_id': 'int'})\n",
    "        df = df.astype({'feature_id': 'str'})\n",
    "    except KeyError:  # If there is no feature_id field\n",
    "        pass\n",
    "    try:\n",
    "        df = df.astype({'nwm_seg': 'int'})\n",
    "        df = df.astype({'nwm_seg': 'str'})\n",
    "    except KeyError:  # If there is no nwm_seg field\n",
    "        pass\n",
    "    try:\n",
    "        df = df.astype({'usgs_gage': 'int'})\n",
    "        df = df.astype({'usgs_gage': 'str'})\n",
    "    except KeyError:  # If there is no usgs_gage field\n",
    "        pass\n",
    "\n",
    "    # zfill HUC8 field.\n",
    "    df['huc8'] = df['huc8'].apply(lambda x: x.zfill(8))\n",
    "\n",
    "    if file_handle in ['stage_based_catfim_sites.csv']:\n",
    "        df = df.astype({'nws_data_rfc_forecast_point': 'str'})\n",
    "        df = df.astype({'nws_data_rfc_defined_fcst_point': 'str'})\n",
    "        df = df.astype({'nws_data_riverpoint': 'str'})\n",
    "\n",
    "    # Upload df to database.\n",
    "    stripped_layer_name = file_handle.replace(\".csv\", \"\")\n",
    "    table_name = \"reference.\" + stripped_layer_name\n",
    "    print(\"Loading data into DB...\")\n",
    "\n",
    "    print(\"Dataframe shape\")\n",
    "    print(df.shape[0])\n",
    "\n",
    "    # Chunk load data into DB\n",
    "    if file_handle in ['stage_based_catfim.csv']:\n",
    "        print(\"Chunk loading...\")\n",
    "        # Create list of df chunks\n",
    "        n = 1000  #chunk row size\n",
    "        list_df = [df[i:i+n] for i in range(0,df.shape[0],n)]\n",
    "\n",
    "        # Load the first chunk into the DB as a new table\n",
    "        first_chunk_df = list_df[0]\n",
    "        print(first_chunk_df.shape[0])\n",
    "        #geometry = 'MULTIPOLYGON'\n",
    "        first_chunk_df.to_sql(\n",
    "            name=stripped_layer_name, \n",
    "            con=db_engine, \n",
    "            schema='reference',\n",
    "            if_exists='replace', \n",
    "            index=False,\n",
    "            dtype={'oid': sqlalchemy.types.Integer(), \n",
    "                   'geom': Geometry('MULTIPOLYGON',srid=3857)\n",
    "                  }\n",
    "        )\n",
    "        \n",
    "        # Load remaining chunks into newly created table\n",
    "        \n",
    "        for remaining_chunk in list_df[1:]:\n",
    "            print(remaining_chunk.shape[0])\n",
    "            remaining_chunk.to_sql(\n",
    "                name=stripped_layer_name, \n",
    "                con=db_engine, \n",
    "                schema='reference',\n",
    "                if_exists='append', \n",
    "                index=False,\n",
    "                dtype={'oid': sqlalchemy.types.Integer(),\n",
    "                       'geom': Geometry('MULTIPOLYGON',srid=3857)\n",
    "                      }\n",
    "            )\n",
    "        \n",
    "        \n",
    "        \n",
    "    else:\n",
    "        geometry = 'POINT'\n",
    "        df.to_sql(\n",
    "            name=stripped_layer_name, \n",
    "            con=db_engine, \n",
    "            schema='reference',\n",
    "            if_exists='replace', \n",
    "            index=False,\n",
    "            dtype={'oid': sqlalchemy.types.Integer(),\n",
    "                   'geom': Geometry(geometry,srid=3857)\n",
    "                  }\n",
    "        )\n",
    "        \n",
    "        \n",
    "    print(\"Creating index...\")\n",
    "    with db_engine.connect() as conn:\n",
    "            result = conn.execute(text(f'CREATE INDEX ON reference.{stripped_layer_name} USING GIST (geom);'))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422ec875-d590-481f-bf6c-f4453b76414b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "db_type = \"egis\"\n",
    "db_engine = get_db_engine(db_type)\n",
    "print(db_engine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28048dba-494c-45bf-b334-f8deca65d68e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "local_download_parent_directory = f'brad_data/qc_fim_data'\n",
    "local_download_path = os.path.join(local_download_parent_directory, f'{file_handle}')\n",
    "filename = f\"/temp/catfim_library_exploded.gpkg\"\n",
    "s3.download_file(FIM_BUCKET, filename, local_download_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601192df",
   "metadata": {},
   "source": [
    "<h2>10 - UPDATE FLOW-BASED CATFIM DATA IN DB</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c55cd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from helper_functions.shared_functions import *\n",
    "import boto3\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "# os.environ['EGIS_DB_HOST'] = ''  #TI DB\n",
    "\n",
    "db_type = \"egis\"\n",
    "db_engine = get_db_engine(db_type)\n",
    "print(db_engine)\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Define bucket and parent directories.\n",
    "bucket = \"hydrovis-ti-deployment-us-east-1\"\n",
    "parent_directory = \"qc_fim_data\"\n",
    "local_download_parent_directory = f'brad_data/qc_fim_data'\n",
    "\n",
    "file_handles = ['flow_based_catfim_sites.csv', 'catfim_library_dissolved_flow_based.csv']\n",
    "\n",
    "for file_handle in file_handles:\n",
    "    # Define path to file to download and its local download path, the download.\n",
    "    filename = f\"{FIM_ROOT_DPATH}/qa_datasets/{file_handle}\"\n",
    "    print(filename)\n",
    "    local_download_path = os.path.join(local_download_parent_directory, f'{file_handle}')\n",
    "    print(f\"--> Downloading {FIM_BUCKET}/{filename} to {local_download_path}\")\n",
    "    #s3.download_file(FIM_BUCKET, filename, local_download_path)\n",
    "    \n",
    "    #  -- Open file and reformat -- #\n",
    "    print(\"Reading file...\")\n",
    "    df = pd.read_csv(local_download_path)\n",
    "    print(\"File read.\")\n",
    "    # Rename headers.\n",
    "    df = df.rename(columns={'Unnamed: 0': 'oid', 'geometry': 'geom', 'huc':'huc8'})\n",
    "        \n",
    "    if file_handle == 'catfim_library_dissolved_flow_based.csv':\n",
    "        file_handle = 'flow_based_catfim.csv'\n",
    "        \n",
    "    # Convert all field names to lowercase (needed for ArcGIS Pro).\n",
    "    df.columns= df.columns.str.lower()\n",
    "\n",
    "    # Remove sites that are in derived.ahps_restricted_sites\n",
    "    restricted_sites_df = get_db_values(\"derived.ahps_restricted_sites\", [\"*\"])\n",
    "    restricted_dict = restricted_sites_df.to_dict('records')\n",
    "\n",
    "    # Change 'mapped' to 'no' if sites are present in restricted_sites_df\n",
    "    for site in restricted_dict:\n",
    "        nws_lid = site['nws_lid'].lower()\n",
    "        print(nws_lid)\n",
    "        if \"sites\" in file_handle:\n",
    "            #print(True)\n",
    "            #print(nws_lid)\n",
    "            df.loc[df.ahps_lid==nws_lid, 'mapped'] = 'no'\n",
    "            df.loc[df.ahps_lid==nws_lid, 'status'] = site['restricted_reason']\n",
    "            #print(df.loc[df.ahps_lid==nws_lid]['status'])\n",
    "        else:\n",
    "            df.loc[df.ahps_lid==nws_lid, 'viz'] = 'no'\n",
    "            df = df[df['viz']=='yes']\n",
    "    \n",
    "    # Enforce data types on df before loading in DB (TODO: need to create special cases for each layer).\n",
    "    df = df.astype({'huc8': 'str'})\n",
    "    df = df.fillna(0)\n",
    "    try:\n",
    "        df = df.astype({'feature_id': 'int'})\n",
    "        df = df.astype({'feature_id': 'str'})\n",
    "    except KeyError:  # If there is no feature_id field\n",
    "        pass\n",
    "    try:\n",
    "        df = df.astype({'nwm_seg': 'int'})\n",
    "        df = df.astype({'nwm_seg': 'str'})\n",
    "    except KeyError:  # If there is no nwm_seg field\n",
    "        pass\n",
    "    try:\n",
    "        df = df.astype({'usgs_gage': 'int'})\n",
    "        df = df.astype({'usgs_gage': 'str'})\n",
    "    except KeyError:  # If there is no usgs_gage field\n",
    "        pass\n",
    "        \n",
    "    # zfill HUC8 field.\n",
    "    df['huc8'] = df['huc8'].apply(lambda x: x.zfill(8))\n",
    "    \n",
    "    if file_handle in ['flow_based_catfim_sites.csv']:\n",
    "        df = df.astype({'nws_data_rfc_forecast_point': 'str'})\n",
    "        df = df.astype({'nws_data_rfc_defined_fcst_point': 'str'})\n",
    "        df = df.astype({'nws_data_riverpoint': 'str'})\n",
    "        \n",
    "    # Upload df to database.\n",
    "    stripped_layer_name = file_handle.replace(\".csv\", \"\")\n",
    "    table_name = \"reference.\" + stripped_layer_name\n",
    "    print(\"Loading data into DB...\")\n",
    "    \n",
    "    print(\"Dataframe shape\")\n",
    "    print(df.shape[0])\n",
    "    \n",
    "    # Chunk load data into DB\n",
    "    if file_handle in ['flow_based_catfim.csv']:\n",
    "        print(\"Chunk loading...\")\n",
    "        # Create list of df chunks\n",
    "        n = 1000  #chunk row size\n",
    "        list_df = [df[i:i+n] for i in range(0,df.shape[0],n)]\n",
    "        \n",
    "        # Load the first chunk into the DB as a new table\n",
    "        first_chunk_df = list_df[0]\n",
    "        print(first_chunk_df.shape[0])\n",
    "        #geometry = 'POLYGON'\n",
    "        \n",
    "        df.to_sql(\n",
    "            name=stripped_layer_name, \n",
    "            con=db_engine, \n",
    "            schema='reference',\n",
    "            if_exists='replace', \n",
    "            index=False,\n",
    "            dtype={'oid': sqlalchemy.types.Integer(),\n",
    "                   'geom': Geometry('MULTIPOLYGON',srid=3857)\n",
    "                  }\n",
    "        )\n",
    "        \n",
    "        # Load remaining chunks into newly created table\n",
    "        for remaining_chunk in list_df[1:]:\n",
    "            print(remaining_chunk.shape[0])\n",
    "            \n",
    "            remaining_chunk.to_sql(\n",
    "                        name=stripped_layer_name, \n",
    "                        con=db_engine, \n",
    "                        schema='reference',\n",
    "                        if_exists='append', \n",
    "                        index=False,\n",
    "                        dtype={'oid': sqlalchemy.types.Integer(),\n",
    "                               'geom': Geometry('MULTIPOLYGON',srid=3857)\n",
    "                              }\n",
    "                    )\n",
    "            \n",
    "        \n",
    "    else:\n",
    "        geometry = 'POINT'\n",
    "        df.to_sql(\n",
    "            name=stripped_layer_name, \n",
    "            con=db_engine, \n",
    "            schema='reference',\n",
    "            if_exists='replace', \n",
    "            index=False,\n",
    "            dtype={'oid': sqlalchemy.types.Integer(),\n",
    "                   'geom': Geometry(geometry,srid=3857)\n",
    "                  }\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4c0634-0878-421e-88bd-c63cd0acbace",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2>11 - UPDATE RAS2FIM DATA IN DB</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c42c0f-7e25-491c-9353-0e74a4089036",
   "metadata": {},
   "source": [
    "Update from Tyler in early 2024: This process will need to be revisited, as Rob Hannah was working on updates to the Ras2FIM data model to sync up with our database. Brad and Corey were on point for this, so proper attention / planning will need to happen to mitigate the knowledge transfer loss / properly test any new updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9be5017-b22e-43d5-96ff-73aa7313d9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "ALTER TABLE ras2fim.geocurves ADD COLUMN previous_stage_ft double precision;\n",
    "ALTER TABLE ras2fim.geocurves ADD COLUMN previous_stage_m double precision;\n",
    "ALTER TABLE ras2fim.geocurves ADD COLUMN previous_discharge_cfs double precision;\n",
    "ALTER TABLE ras2fim.geocurves ADD COLUMN previous_discharge_cms double precision\n",
    "\"\"\"\n",
    "\n",
    "sql = \"\"\"\n",
    "WITH lagged as (SELECT \n",
    "    feature_id,\n",
    "    (lag(stage_m, 1) OVER (PARTITION BY feature_id ORDER by stage_m)) as previous_stage_m,\n",
    "    (lag(stage_ft, 1) OVER (PARTITION BY feature_id ORDER by stage_ft)) as previous_stage_ft,\n",
    "    (lag(discharge_cfs, 1) OVER (PARTITION BY feature_id ORDER by discharge_cfs)) as previous_discharge_cfs,\n",
    "    (lag(discharge_cms, 1) OVER (PARTITION BY feature_id ORDER by discharge_cms)) as previous_discharge_cms\n",
    "FROM ras2fim.geocurves)\n",
    "\n",
    "UPDATE ras2fim.geocurves gc\n",
    "SET previous_stage_ft = lagged.previous_stage_ft,\n",
    "    previous_stage_m = lagged.previous_stage_m,\n",
    "    previous_discharge_cfs = lagged.previous_discharge_cfs,\n",
    "    previous_discharge_cms = lagged.previous_discharge_cms\n",
    "FROM lagged\n",
    "WHERE gc.feature_id = lagged.feature_id and gc.stage_ft = lagged.stage_ft;\n",
    "\"\"\"\n",
    "\n",
    "sql = \"\"\"\n",
    "SELECT\n",
    "    feature_id,\n",
    "    max(discharge_cfs) as max_rc_discharge_cfs,\n",
    "    max(stage_ft) as max_rc_stage_ft,\n",
    "    max(discharge_cms) as max_rc_discharge_cms,\n",
    "    max(stage_m) as max_rc_stage_m\n",
    "INTO ras2fim.max_geocurves\n",
    "FROM ras2fim.geocurves\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fae3f0b-7669-4aaf-b50a-bf82aea06a00",
   "metadata": {},
   "source": [
    "<h2>12 - Clear the HAND Cache</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cf4236-7320-4780-a6f5-b83258073bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "TRUNCATE TABLE fim_cache.hand_hydrotable_cached;\n",
    "TRUNCATE TABLE fim_cache.hand_hydrotable_cached_max;\n",
    "TRUNCATE TABLE fim_cache.hand_hydrotable_cached_geo;\n",
    "TRUNCATE TABLE fim_cache.hand_hydrotable_cached_zero_stage;\n",
    "\"\"\"\n",
    "run_sql_in_db(sql)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
